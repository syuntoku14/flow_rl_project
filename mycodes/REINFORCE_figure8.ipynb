{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.controllers import RLController, IDMController, StaticLaneChanger, ContinuousRouter\n",
    "from flow.core.experiment import Experiment\n",
    "from flow.core.params import SumoParams, EnvParams, NetParams, \\\n",
    "    SumoCarFollowingParams\n",
    "from flow.core.params import VehicleParams, InitialConfig\n",
    "from flow.envs.loop.loop_accel import AccelEnv, ADDITIONAL_ENV_PARAMS\n",
    "from flow.scenarios.figure_eight import Figure8Scenario, ADDITIONAL_NET_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SumoParams\n",
    "sim_params = SumoParams(sim_step=0.1, render=False)\n",
    "\n",
    "\n",
    "# Vehicles Setting\n",
    "vehicles = VehicleParams()\n",
    "\n",
    "vehicles.add(\n",
    "    veh_id=\"rl\",\n",
    "    acceleration_controller=(RLController, {}),\n",
    "    routing_controller=(ContinuousRouter, {}),\n",
    "    car_following_params=SumoCarFollowingParams(\n",
    "        speed_mode=\"obey_safe_speed\",\n",
    "    ),\n",
    "    num_vehicles=1)\n",
    "\n",
    "vehicles.add(\n",
    "    veh_id=\"idm\",\n",
    "    acceleration_controller=(IDMController, {}),\n",
    "    lane_change_controller=(StaticLaneChanger, {}),\n",
    "    routing_controller=(ContinuousRouter, {}),\n",
    "    car_following_params=SumoCarFollowingParams(\n",
    "        speed_mode=\"obey_safe_speed\",\n",
    "    ),\n",
    "    initial_speed=0,\n",
    "    num_vehicles=14)\n",
    "\n",
    "\n",
    "# Additional Env params\n",
    "HORIZON = 1500\n",
    "\n",
    "additional_env_params = {\n",
    "    \"target_velocity\": 20,\n",
    "    \"max_accel\": 3,\n",
    "    \"max_decel\": 3,\n",
    "    \"sort_vehicles\": False\n",
    "}\n",
    "env_params = EnvParams(\n",
    "    horizon=HORIZON, additional_params=additional_env_params)\n",
    "\n",
    "\n",
    "# Additional Net params\n",
    "additional_net_params = {\n",
    "    \"radius_ring\": 30,\n",
    "    \"lanes\": 1,\n",
    "    \"speed_limit\": 30,\n",
    "    \"resolution\": 40\n",
    "}\n",
    "net_params = NetParams(\n",
    "    no_internal_links=False, additional_params=additional_net_params)\n",
    "\n",
    "\n",
    "## Initial config\n",
    "initial_config = InitialConfig(spacing=\"uniform\")\n",
    "\n",
    "\n",
    "## Scenario\n",
    "exp_tag = \"figure-eight-control\"\n",
    "\n",
    "scenario = Figure8Scenario(\n",
    "    exp_tag,\n",
    "    vehicles,\n",
    "    net_params,\n",
    "    initial_config=initial_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccelEnv_torch(AccelEnv):\n",
    "    def reset(self):\n",
    "        obs = super().reset()\n",
    "        return torch.tensor(obs).float().cuda()\n",
    "        \n",
    "    def step(self, action):\n",
    "        new_obs, reward, done, info = super().step(action)\n",
    "        new_obs = torch.tensor(new_obs).float().cuda()\n",
    "        return new_obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 6.9524e-04, 9.0580e-02, 1.5725e-01,\n",
       "        2.2391e-01, 2.9058e-01, 3.5725e-01, 4.2391e-01, 4.9058e-01, 5.5725e-01,\n",
       "        6.2391e-01, 6.9058e-01, 7.5725e-01, 8.2391e-01, 8.9058e-01, 9.5725e-01],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = AccelEnv_torch(env_params, sim_params, scenario)\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# States, Actions, Rewards, Terminates\n",
    "\n",
    "* States: The state consists of the velocities and absolute position of all vehicles in the network. This assumes a constant number of vehicles.\n",
    "\n",
    "(speed x num_vehicles), (position x num_vehicles)\n",
    "\n",
    "* Actions: Actions are a list of acceleration for each rl vehicles, bounded by the maximum accelerations and decelerations specified in EnvParams.\n",
    "\n",
    "accel x num_rl_vehicles\n",
    "\n",
    "* Rewards: The reward function is the two-norm of the distance of the speed of the vehicles in the network from the \"target_velocity\" term. For a description of the reward, see: flow.core.rewards.desired_speed\n",
    "\n",
    "* Termination: A rollout is terminated if the time horizon is reached or if two vehicles collide into one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = Variable(torch.FloatTensor([math.pi])).cuda()\n",
    "\n",
    "def normal(x, mu, sigma_sq):\n",
    "    a = (-1*(Variable(x)-mu).pow(2)/(2*sigma_sq)).exp()\n",
    "    b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, num_inputs, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space.shape[0]\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, 128)\n",
    "        self.dropout = nn.dropout(p=0.6)\n",
    "        self.linear2_mu = nn.Linear(128, num_outputs)\n",
    "        self.linear2_sigma = nn.Linear(128, num_outputs)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu = self.linear2_mu(x)\n",
    "        sigma_sq = self.linear2_sigma(x)\n",
    "        sigma_sq = F.softplus(sigma_sq)  # make it positive\n",
    "        \n",
    "        return mu, sigma_sq\n",
    "\n",
    "class REINFORCE:\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        self.action_space = action_space\n",
    "        self.model = Policy(hidden_size, num_inputs, action_space)\n",
    "        self.model = self.model.cuda()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        mu, sigma_sq = self.model(Variable(state).cuda())\n",
    "        eps = torch.randn(mu.size())\n",
    "        action = (mu + sigma_sq.sqrt()*Variable(eps).cuda()).data\n",
    "        prob = normal(action, mu, sigma_sq)\n",
    "        entropy = -0.5*((2*pi.expand_as(sigma_sq)+sigma_sq).log()+1)\n",
    "        \n",
    "        log_prob = prob.log()\n",
    "        return action, log_prob, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(_agent, t_max=1000):\n",
    "    s_0 = env.reset()\n",
    "    states, actions, rewards, log_probs, entropies = [], [], [], [], []\n",
    "    done = False\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # Get actions and convert to numpy array\n",
    "        action, log_prob, entropy = _agent.select_action(s_0)\n",
    "        s_1, r, done, _ = env.step(action.cpu())\n",
    "\n",
    "        states.append(s_0)\n",
    "        rewards.append(r)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        entropies.append(entropy)\n",
    "        s_0 = s_1\n",
    "           \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    states = torch.stack(states)\n",
    "    actions = torch.cat(actions)\n",
    "    log_probs = torch.cat(log_probs)\n",
    "    entropies = torch.cat(entropies)\n",
    "    \n",
    "    return states, actions, rewards, log_probs, entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discouted_rewards(rewards, gamma=0.99):\n",
    "    r = np.array([gamma**i * rewards[i] \n",
    "                  for i in range(len(rewards))])\n",
    "    # Reverse the array direction for cumsum and then\n",
    "    # revert back to the original order\n",
    "    r = r.cumsum()[::-1].copy()\n",
    "    return torch.tensor(r).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_session(_agent, states, actions, rewards, \n",
    "                     log_probs, entropies, gamma=0.99):\n",
    "    agent.optimizer.zero_grad()\n",
    "    returns = discouted_rewards(rewards)\n",
    "    loss = (-returns * log_probs - 0.0001 * entropies).mean()\n",
    "    loss.backward() #retain_graph=True)\n",
    "    agent.optimizer.step()\n",
    "    \n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "action_space = env.action_space\n",
    "\n",
    "agent = REINFORCE(16, num_inputs, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 1/30 [01:16<36:57, 76.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:15.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 2/30 [02:17<33:31, 71.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:15.048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 3/30 [03:05<29:08, 64.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:17.508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 4/30 [03:44<24:44, 57.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:12.386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█▋        | 5/30 [04:37<23:16, 55.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:14.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 6/30 [05:55<24:59, 62.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:15.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 23%|██▎       | 7/30 [06:53<23:26, 61.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:13.971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██▋       | 8/30 [07:39<20:45, 56.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:15.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|███       | 9/30 [08:19<18:04, 51.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:15.070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 10/30 [09:08<16:52, 50.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:17.160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 37%|███▋      | 11/30 [09:46<14:52, 46.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:14.421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 12/30 [10:39<14:37, 48.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:18.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 43%|████▎     | 13/30 [11:26<13:41, 48.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:14.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|████▋     | 14/30 [12:18<13:07, 49.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:16.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 15/30 [13:09<12:25, 49.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:16.417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|█████▎    | 16/30 [13:48<10:52, 46.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:14.065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|█████▋    | 17/30 [14:50<11:07, 51.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:20.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████    | 18/30 [15:38<10:03, 50.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:17.018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 63%|██████▎   | 19/30 [16:39<09:47, 53.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:19.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 20/30 [17:26<08:34, 51.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:15.580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|███████   | 21/30 [18:11<07:25, 49.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:15.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 73%|███████▎  | 22/30 [18:54<06:20, 47.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:15.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 77%|███████▋  | 23/30 [19:31<05:11, 44.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:14.337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████  | 24/30 [20:14<04:24, 44.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:16.332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|████████▎ | 25/30 [20:57<03:38, 43.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:16.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 87%|████████▋ | 26/30 [22:03<03:21, 50.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:20.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|█████████ | 27/30 [22:55<02:32, 50.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:14.330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 93%|█████████▎| 28/30 [23:46<01:41, 50.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:16.277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 97%|█████████▋| 29/30 [24:36<00:50, 50.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:17.523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [25:27<00:00, 50.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:16.109\n",
      "CPU times: user 20min 10s, sys: 1min 18s, total: 21min 28s\n",
      "Wall time: 25min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in tqdm(range(30)):\n",
    "    rewards = [train_on_session(agent, *generate_session(agent, t_max=1000))\n",
    "        for _ in range(100)]  # generate new sessions\n",
    "    \n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 6.9524e-04, 9.0580e-02, 1.5725e-01,\n",
       "        2.2391e-01, 2.9058e-01, 3.5725e-01, 4.2391e-01, 4.9058e-01, 5.5725e-01,\n",
       "        6.2391e-01, 6.9058e-01, 7.5725e-01, 8.2391e-01, 8.9058e-01, 9.5725e-01],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_params = SumoParams(sim_step=0.1, render=True)\n",
    "env = AccelEnv_torch(env_params, sim_params, scenario)\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.3592, 0.3547, 0.3545, 0.3547, 0.3527, 0.3398, 0.2742, 0.3227, 0.3619,\n",
       "         0.3546, 0.3551, 0.3550, 0.3529, 0.3408, 0.3299, 0.1683, 0.2703, 0.3369,\n",
       "         0.4032, 0.4698, 0.5361, 0.5981, 0.6497, 0.7319, 0.8035, 0.8699, 0.9366,\n",
       "         1.0033, 0.0361, 0.0913], device='cuda:0'),\n",
       " 0.515206968561324,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(_agent, t_max=1000):\n",
    "    s_0 = env.reset()\n",
    "    states, actions, rewards, log_probs, entropies = [], [], [], [], []\n",
    "    done = False\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # Get actions and convert to numpy array\n",
    "        action, log_prob, entropy = _agent.select_action(s_0)\n",
    "        s_1, r, done, _ = env.step(action.cpu())\n",
    "\n",
    "        s_0 = s_1\n",
    "           \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
