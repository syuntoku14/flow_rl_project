{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.controllers import RLController, IDMController, StaticLaneChanger, ContinuousRouter\n",
    "from flow.core.experiment import Experiment\n",
    "from flow.core.params import SumoParams, EnvParams, NetParams, \\\n",
    "    SumoCarFollowingParams\n",
    "from flow.core.params import VehicleParams, InitialConfig\n",
    "from flow.envs.loop.loop_accel import AccelEnv, ADDITIONAL_ENV_PARAMS\n",
    "from flow.scenarios.figure_eight import Figure8Scenario, ADDITIONAL_NET_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SumoParams\n",
    "sim_params = SumoParams(render=True)\n",
    "\n",
    "\n",
    "# Vehicles Setting\n",
    "vehicles = VehicleParams()\n",
    "\n",
    "vehicles.add(\n",
    "    veh_id=\"rl\",\n",
    "    acceleration_controller=(RLController, {}),\n",
    "    routing_controller=(ContinuousRouter, {}),\n",
    "    car_following_params=SumoCarFollowingParams(\n",
    "        speed_mode=\"obey_safe_speed\",\n",
    "    ),\n",
    "    num_vehicles=1)\n",
    "\n",
    "vehicles.add(\n",
    "    veh_id=\"idm\",\n",
    "    acceleration_controller=(IDMController, {}),\n",
    "    lane_change_controller=(StaticLaneChanger, {}),\n",
    "    routing_controller=(ContinuousRouter, {}),\n",
    "    car_following_params=SumoCarFollowingParams(\n",
    "        speed_mode=\"obey_safe_speed\",\n",
    "    ),\n",
    "    initial_speed=0,\n",
    "    num_vehicles=14)\n",
    "\n",
    "\n",
    "# Additional Env params\n",
    "HORIZON = 1500\n",
    "\n",
    "additional_env_params = {\n",
    "    \"target_velocity\": 20,\n",
    "    \"max_accel\": 3,\n",
    "    \"max_decel\": 3,\n",
    "    \"sort_vehicles\": False\n",
    "}\n",
    "env_params = EnvParams(\n",
    "    horizon=HORIZON, additional_params=additional_env_params)\n",
    "\n",
    "\n",
    "# Additional Net params\n",
    "additional_net_params = {\n",
    "    \"radius_ring\": 30,\n",
    "    \"lanes\": 1,\n",
    "    \"speed_limit\": 30,\n",
    "    \"resolution\": 40\n",
    "}\n",
    "net_params = NetParams(\n",
    "    no_internal_links=False, additional_params=additional_net_params)\n",
    "\n",
    "\n",
    "## Initial config\n",
    "initial_config = InitialConfig(spacing=\"uniform\")\n",
    "\n",
    "\n",
    "## Scenario\n",
    "exp_tag = \"figure-eight-control\"\n",
    "\n",
    "scenario = Figure8Scenario(\n",
    "    exp_tag,\n",
    "    vehicles,\n",
    "    net_params,\n",
    "    initial_config=initial_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AccelEnv(env_params, sim_params, scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# States, Actions, Rewards, Terminates\n",
    "\n",
    "* States: The state consists of the velocities and absolute position of all vehicles in the network. This assumes a constant number of vehicles.\n",
    "\n",
    "(speed x num_vehicles), (position x num_vehicles)\n",
    "\n",
    "* Actions: Actions are a list of acceleration for each rl vehicles, bounded by the maximum accelerations and decelerations specified in EnvParams.\n",
    "\n",
    "accel x num_rl_vehicles\n",
    "\n",
    "* Rewards: The reward function is the two-norm of the distance of the speed of the vehicles in the network from the \"target_velocity\" term. For a description of the reward, see: flow.core.rewards.desired_speed\n",
    "\n",
    "* Termination: A rollout is terminated if the time horizon is reached or if two vehicles collide into one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(1,)\n",
      "30\n",
      "['rl_0', 'idm_0', 'idm_1', 'idm_2', 'idm_3', 'idm_4', 'idm_5', 'idm_6', 'idm_7', 'idm_8', 'idm_9', 'idm_10', 'idm_11', 'idm_12', 'idm_13']\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space.shape[0])\n",
    "print(env.sorted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 6.95237622e-04\n",
      " 9.05795302e-02 1.57246197e-01 2.23912864e-01 2.90579530e-01\n",
      " 3.57246197e-01 4.23912864e-01 4.90579530e-01 5.57246197e-01\n",
      " 6.23912864e-01 6.90579530e-01 7.57246197e-01 8.23912864e-01\n",
      " 8.90579530e-01 9.57246197e-01]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = Variable(torch.FloatTensor([math.pi])).cuda()\n",
    "\n",
    "def normal(x, mu, sigma_sq):\n",
    "    a = (-1*(Variable(x)-mu).pow(2)/(2*sigma_sq)).exp()\n",
    "    b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space.shape[0]\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2_mu = nn.Linear(hidden_size, num_outputs)\n",
    "        self.linear2_sigma = nn.Linear(hidden_size, num_outputs)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu = self.linear2_mu(x)\n",
    "        sigma_sq = self.linear2_sigma(x)\n",
    "        sigma_sq = F.softplus(sigma_sq)  # make it positive\n",
    "        \n",
    "        return mu, sigma_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        self.action_space = action_space\n",
    "        self.model = Policy(hidden_size, num_inputs, action_space)\n",
    "        self.model = self.model.cuda()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        mu, sigma_sq = self.model(Variable(state).cuda())\n",
    "        eps = torch.randn(mu.size())\n",
    "        action = (mu + sigma_sq.sqrt()*Variable(eps).cuda()).data\n",
    "        prob = normal(action, mu, sigma_sq)\n",
    "        entropy = -0.5*((2*pi.expand_as(sigma_sq)*sigma_eq).log()+1)\n",
    "        \n",
    "        log_prob = prob.log()\n",
    "        return action, log_prob, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "action_space = env.action_space\n",
    "\n",
    "agent = REINFORCE(16, num_inputs, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4686], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([-0.2907], device='cuda:0', grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model(torch.tensor(state).float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(_agent, t_max=1000):\n",
    "    s_0 = env.reset()\n",
    "    states, actions, rewards = [], [], []\n",
    "    done = False\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # Get actions and convert to numpy array\n",
    "        action = _agent.select_action(s_0)\n",
    "        s_1, r, done, _ = env.step(action)\n",
    "\n",
    "        states.append(s_0)\n",
    "        rewards.append(r)\n",
    "        actions.append(action)\n",
    "        s_0 = s_1\n",
    "           \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Variable data has to be a tensor, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-dcd349f36b8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-301af4612668>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(_agent, t_max)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Get actions and convert to numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0ms_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-6edf1bba24e6>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0msigma_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma_sq\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# make it positive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Variable data has to be a tensor, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "generate_session(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discouted_rewards(rewards, gamma=0.99):\n",
    "    r = np.array([gamma**i * rewards[i] \n",
    "                  for i in range(len(rewards))])\n",
    "    # Reverse the array direction for cumsum and then\n",
    "    # revert back to the original order\n",
    "    r = r.cumsum()[::-1].copy()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_session(_agent, _optimizer, states, actions, rewards, gamma=0.99):\n",
    "    _optimizer.zero_grad()\n",
    "    state_tensor = torch.FloatTensor(states)\n",
    "    reward_tensor = torch.FloatTensor(discount_rewards(rewards))\n",
    "    # Actions are used as indices, must be LongTensor\n",
    "    action_tensor = torch.LongTensor(actions)\n",
    "\n",
    "    # Calculate loss\n",
    "    prob = _agent.predict(state_tensor)\n",
    "    logprob = torch.log(\n",
    "        _agent.predict(state_tensor))\n",
    "    selected_logprobs = logprob[np.arange(len(action_tensor)), action_tensor]\n",
    "    selected_probs = prob[np.arange(len(action_tensor)), action_tensor]\n",
    "    entropy = - torch.sum(selected_probs * selected_logprobs)\n",
    "    loss = -(reward_tensor * selected_logprobs).mean() - 0.001*entropy\n",
    "\n",
    "    # Calculate gradients\n",
    "    loss.backward()\n",
    "    # Apply gradients\n",
    "    _optimizer.step()\n",
    "    \n",
    "    return np.sum(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
