\section{Discussion}\label{sec:discussion}

This research solves the problems of the previous work\cite{Kreidieh2018}. The two problems of the previous work is, the limitation of the number of the RL cars and the vulnerability of the master node. Compared to \cite{Kreidieh2018}, multi-agent doesn't require one master node to control all the vehicles, and each RL cars works independently. Therefore, the multi-agent one is safer than the one master node and more flexible. However, there are still many space to improve. The future works are as follows.

\subsection{Model generalization}
The agent is trained only on the merge network, so it is not guaranteed that it works on the other environments(e.g. figure eight, grid, bottleneck (see Fig. \ref{fig:envs})). A future work is to train a more general agent which works in a variety of environments. As for the environments, there might be some problems on the original flow benchmarks. As mentioned in \ref{sec:experiments}, agents can lean how to reduce the outflow rate in the merge network. If the agent stop at the start point on the real traffic, congestion can happen at the start point. To measure the performance of the agent more accurately, better benchmark which prevents this phenomenon is necessary.

\subsection{For the real world application}
The results of the GAIL shows the potential to reproduce an expert performance from trajectories. IL can solve the reward function problem, and it is useful especially in the real world tasks. In the real world, it is easy to obtain the trajectories using some sensors. The obtained trajectories can be used as an expert, and from which it is possible to reconstruct an agent imitating the behavior. The problem of GAIL is its sample inefficiency. GAIL consumes about 1 million samples to train an agent, and it requires 20 CPUs and 150 iteration of simulation. To find more sample efficient algorithms is a future work.

\subsection{Better algorithms}
Another problem of GAIL for this task is the biased trajectory from the expert. The expert agent keeps a certain length of bumper-to-bumper distance and accelerates when there is a enough distance. After a few seconds from time $t=0$, all the RL cars have enough distance and they converge to a stable traffic where they can drive with constant speed(free-flow). The problem is that the most of the part of the expert trajectory is from the free-flow traffic. Therefore, the number of samples from congested traffic is less than that of the free-flow traffic, so the expert sample is biased. It might make the training slower. DQN\cite{Mnih2013} has a similar problem, but it is solved by prioritized experience replay\cite{Schaul2015}. The idea is to use the sample which have huge effects on the training. This idea can be used in the IL work and solve the problem. To apply such less biased algorithm to the traffic control is another future work.

One solution for sample efficient algorithm is model-based RL. \cite{Nagabandi2017}, \cite{Nagabandi} apply Model Predictive Control(MPC) to the RL training part, and successfully reduce the number of samples to train an agent controlling the real world robot. Although the model-predictive control is powerful as a model-based method, some changes are required if we want to apply it for the mixed-autonomy task. The problem is due to the fact that the reward function\ref{eq:newR} cannot be calculated from the state space, since it contains the mean velocity of the all the cars. Most of the model-based RL predicts the next state $s_{t+1}$ from the current state $s_t$ and action $a_t$ pair, and calculate the reward $r_{t+1}$ from the pair. Some predictions of the reward function or the other approach are necessary since the $r_{t+1}$ cannot be calculated from the pair in this task. 

A promising approach is using inverse RL\cite{Ng2000}. Inverse RL reconstructs the reward function from the trajectory of a policy. Since the generated reward function by inverse RL is estimated from the state and action pair, it is possible to use the reward to the MPC. In short, the algorithm proposal consists of two steps, 1. reconstruct the reward function $r_t(s_t, a_t)$ from an expert, 2. use MPC and do model-based RL to train the agent.