\section{Background\label{background}}

\subsection{Reinforcement Learning}

RL is an optimization algorithm in Markov decision process(MDP), defined by $\left(\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \rho_{0}, \gamma, T\right)$. Here, $\mathcal{S}$ is a state space, $\mathcal{A}$ is an action space, $\mathcal{P}$ is a state transition probability function, $r$ is a reward function, $\rho_{0}$ is initial state distribution, $\gamma$ is a discount factor, and $T$ is a time horizon.

In RL algorithms, agents interact the environment and try to learn a parameterized policy $\pi_\theta$ which maximize the sum of the discounted reward $\sum_{i=0}^{T} \gamma^{i} r_{i}$, i.e. $\pi_\theta = \argmax_{\pi_\theta} \sum_{i=0}^{T} \gamma^{i} r_{i}$. The rewards are sampled from the trajectory $\tau=\left(s_{0}, a_{0}, \ldots, a_{T-1}, s_{T}\right)$. The initial states, actions and the next states are sampled as $s_{0} \sim \rho_{0}\left(s_{0}\right)$, $a_{t} \sim \pi_{\theta}\left(a_{t}\right)$, and $s_{t+1} \sim \mathcal{P}\left(s_{t+1} | s_{t}, a_{t}, a_{t}\right)$

The plain policy gradient(PG) method updates the parameter using following gradient.

\begin{equation} \label{eq:PG}
\nabla J\left(\pi_{\theta}\right)=E\left[\nabla_{\theta} \log \pi_{\theta}(a | s) Q_{\pi}(s, a)\right]
\end{equation}

$Q_{\pi}$ is an action value function. The problem of plain PG is its instability. Intuitively, the instability is from the term $\nabla_{\theta} \log \pi_{\theta}(a | s)$. As the training goes, the policy is getting deterministic, and the gradient of the policy becomes large around its mean value. PPO solves the problem by restricting the update in parameter space, and achieves stabler RL training.

\subsection{Imitation Learning}

Designing reward function is difficult problem in RL. Instead, IL trains the agents from the trajectory generated by an expert. \cite{Ho} shows that the IL problem is same as a problem finding a policy whose occupancy measure $\rho_\pi$ matches the one of the expert. Here, occupancy measure is the distribution of state and action pair that the agent encounters during an episode. GAIL, which used in this research, exploits this feature. The structure of GAIL is similar to the structure of generative adversarial network(GAN)\cite{Goodfellow2014}. GAIL has a discriminator which tries to tell whether the trajectory comes from the expert or the agent trained by GAIL. As the training goes, the trajectory generated by the GAIL is getting similar to the one from the expert.