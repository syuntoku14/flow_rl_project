{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-28 20:56:42,910\tINFO node.py:469 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-28_20-56-42_11088/logs.\n",
      "2019-05-28 20:56:43,026\tINFO services.py:407 -- Waiting for redis server at 127.0.0.1:26818 to respond...\n",
      "2019-05-28 20:56:43,165\tINFO services.py:407 -- Waiting for redis server at 127.0.0.1:51969 to respond...\n",
      "2019-05-28 20:56:43,169\tINFO services.py:804 -- Starting Redis shard with 6.68 GB max memory.\n",
      "2019-05-28 20:56:43,217\tINFO node.py:483 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-28_20-56-42_11088/logs.\n",
      "2019-05-28 20:56:43,221\tINFO services.py:1427 -- Starting the Plasma object store with 10.02 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '169.237.32.118',\n",
       " 'object_store_address': '/tmp/ray/session_2019-05-28_20-56-42_11088/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-05-28_20-56-42_11088/sockets/raylet',\n",
       " 'redis_address': '169.237.32.118:26818',\n",
       " 'webui_url': None}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym, pickle, argparse, json, logging\n",
    "from copy import deepcopy\n",
    "import ray\n",
    "from gail.gail import GAILTrainer\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import PPOPolicyGraph\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "from ray.rllib.agents import Trainer\n",
    "from ray.rllib.evaluation import PolicyEvaluator, SampleBatch, MultiAgentSampleBatchBuilder\n",
    "from ray.rllib.offline.json_writer import JsonWriter\n",
    "from ray.rllib.offline.json_reader import JsonReader\n",
    "from ray.rllib.evaluation.sample_batch import DEFAULT_POLICY_ID\n",
    "from ray.rllib.evaluation.metrics import collect_metrics\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder, get_flow_params\n",
    "logger = logging.getLogger(__name__)\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpus = 3\n",
    "num_rollouts = 3\n",
    "horizon = 750\n",
    "gae_lambda = 0.97\n",
    "step_size = 5e-4\n",
    "num_iter = 10\n",
    "benchmark_name = \"multi_merge\"\n",
    "exp_name = \"test_ir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = deepcopy(DEFAULT_CONFIG)\n",
    "config[\"num_workers\"] = min(num_cpus, num_rollouts)\n",
    "config[\"train_batch_size\"] = horizon * num_rollouts\n",
    "config[\"sample_batch_size\"] = horizon / 2\n",
    "config[\"use_gae\"] = True\n",
    "config[\"horizon\"] = horizon\n",
    "config[\"lambda\"] = gae_lambda\n",
    "config[\"lr\"] = step_size\n",
    "config[\"vf_clip_param\"] = 1e6\n",
    "config[\"num_sgd_iter\"] = 10\n",
    "config['clip_actions'] = False  # FIXME(ev) temporary ray bug\n",
    "config[\"model\"][\"fcnet_hiddens\"] = [128, 64, 32]\n",
    "config[\"observation_filter\"] = \"NoFilter\"\n",
    "config[\"entropy_coeff\"] = 0.0\n",
    "config[\"expert_path\"] = '/headless/rl_project/flow_codes/ModelBased/expert_sample'\n",
    "\n",
    "benchmark = __import__(\n",
    "            \"flow.benchmarks.%s\" % benchmark_name, fromlist=[\"flow_params\"])\n",
    "flow_params = benchmark.flow_params\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(\n",
    "    flow_params, cls=FlowParamsEncoder, sort_keys=True, indent=4)\n",
    "config['env_config']['flow_params'] = flow_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_env, env_name = make_create_env(params=flow_params, version=0)\n",
    "register_env(env_name, create_env)\n",
    "env = create_env()\n",
    "\n",
    "# we don't need this config\n",
    "POLICY_ID = DEFAULT_POLICY_ID \n",
    "default_policy = (PPOPolicyGraph, env.observation_space, env.action_space, {})\n",
    "policy_graph = {POLICY_ID: default_policy}\n",
    "config[\"multiagent\"] = {\n",
    "        'policy_graphs': policy_graph,\n",
    "        'policy_mapping_fn': tune.function(lambda agent_id: POLICY_ID),\n",
    "        'policies_to_train': [POLICY_ID]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-28 20:57:00,129\tWARNING json_reader.py:52 -- Treating input directory as glob pattern: /headless/rl_project/flow_codes/ModelBased/expert_sample/*.json\n",
      "2019-05-28 20:57:00,132\tINFO json_reader.py:65 -- Found 2 input files.\n",
      "2019-05-28 20:57:01,284\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "/opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "2019-05-28 20:57:02,787\tINFO policy_evaluator.py:728 -- Built policy map: {'default_policy': <ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph object at 0x7ff2ad4f8710>}\n",
      "2019-05-28 20:57:02,788\tINFO policy_evaluator.py:729 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7ff2ad4f8358>}\n",
      "2019-05-28 20:57:02,790\tINFO policy_evaluator.py:343 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7ff2ad2aa2b0>}\n",
      "2019-05-28 20:57:02,837\tWARNING worker.py:334 -- WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n"
     ]
    }
   ],
   "source": [
    "agent = GAILTrainer(config, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1buf.wgt', 'rb') as f:\n",
    "    weights = pickle.load(f)\n",
    "    weights[POLICY_ID] = weights.pop('default')\n",
    "agent.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = JsonWriter(\"./expert_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m 2019-05-28 20:58:01,748\tINFO policy_evaluator.py:437 -- Generating sample batch of size 375.0\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m 2019-05-28 20:58:03,241\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'flow_1.0': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.504),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m        'flow_1.1': np.ndarray((12,), dtype=float32, min=0.009, max=0.804, mean=0.19)}}\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m 2019-05-28 20:58:03,241\tINFO sampler.py:309 -- Info return from env: {0: {'flow_1.0': {}, 'flow_1.1': {}}}\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m 2019-05-28 20:58:03,242\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.504)\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m 2019-05-28 20:58:03,242\tINFO sampler.py:411 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.504)\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m 2019-05-28 20:58:03,243\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'flow_1.0',\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                                   'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.504),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                                   'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                       { 'data': { 'agent_id': 'flow_1.1',\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                                   'obs': np.ndarray((12,), dtype=float32, min=0.009, max=0.804, mean=0.19),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                                   'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m 2019-05-28 20:58:03,244\tINFO tf_run_builder.py:89 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m 2019-05-28 20:58:03,282\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m { 'default_policy': ( np.ndarray((2, 1), dtype=float32, min=0.224, max=1.07, mean=0.647),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                       { 'action_prob': np.ndarray((2,), dtype=float32, min=0.396, max=3.087, mean=1.742),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                         'behaviour_logits': np.ndarray((2, 2), dtype=float32, min=-2.181, max=1.012, mean=-0.633),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                         'vf_preds': np.ndarray((2,), dtype=float32, min=-26.16, max=-9.81, mean=-17.985)})}\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m 2019-05-28 20:58:06,067\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m { 'flow_1.0': { 'data': { 'action_prob': np.ndarray((26,), dtype=float32, min=0.123, max=4.055, mean=2.203),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'actions': np.ndarray((26, 1), dtype=float32, min=0.707, max=1.829, mean=1.128),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'advantages': np.ndarray((26,), dtype=float32, min=0.187, max=2.461, mean=1.206),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'agent_index': np.ndarray((26,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'behaviour_logits': np.ndarray((26, 2), dtype=float32, min=-2.323, max=1.556, mean=-0.48),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'dones': np.ndarray((26,), dtype=bool, min=0.0, max=1.0, mean=0.038),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'eps_id': np.ndarray((26,), dtype=int64, min=1883949945.0, max=1883949945.0, mean=1883949945.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'infos': np.ndarray((26,), dtype=object, head={'mean_vel': 18.34238713654684, 'cost1': 0.6602456524033717, 'cost2': 0.0, 'outflow': 445.5445544554455}),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'new_obs': np.ndarray((26, 12), dtype=float32, min=0.0, max=1.0, mean=0.52),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'obs': np.ndarray((26, 12), dtype=float32, min=0.0, max=1.0, mean=0.517),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'prev_actions': np.ndarray((26, 1), dtype=float32, min=0.0, max=1.829, mean=1.074),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'prev_rewards': np.ndarray((26,), dtype=float32, min=-0.317, max=0.0, mean=-0.274),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'rewards': np.ndarray((26,), dtype=float32, min=-0.317, max=-0.261, mean=-0.285),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           't': np.ndarray((26,), dtype=int64, min=0.0, max=25.0, mean=12.5),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'unroll_id': np.ndarray((26,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'value_targets': np.ndarray((26,), dtype=float32, min=-7.349, max=-0.269, mean=-3.779),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'vf_preds': np.ndarray((26,), dtype=float32, min=-9.81, max=-1.25, mean=-4.984)},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m   'flow_1.1': { 'data': { 'action_prob': np.ndarray((74,), dtype=float32, min=0.143, max=12.536, mean=3.246),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'actions': np.ndarray((74, 1), dtype=float32, min=0.224, max=1.856, mean=0.92),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'advantages': np.ndarray((74,), dtype=float32, min=-0.43, max=9.845, mean=3.633),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'agent_index': np.ndarray((74,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'behaviour_logits': np.ndarray((74, 2), dtype=float32, min=-3.65, max=1.564, mean=-0.699),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'dones': np.ndarray((74,), dtype=bool, min=0.0, max=1.0, mean=0.014),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'eps_id': np.ndarray((74,), dtype=int64, min=1883949945.0, max=1883949945.0, mean=1883949945.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'infos': np.ndarray((74,), dtype=object, head={'mean_vel': 18.34238713654684, 'cost1': 0.6602456524033717, 'cost2': 0.0, 'outflow': 445.5445544554455}),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'new_obs': np.ndarray((74, 12), dtype=float32, min=-0.005, max=1.0, mean=0.409),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'obs': np.ndarray((74, 12), dtype=float32, min=-0.005, max=1.0, mean=0.404),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'prev_actions': np.ndarray((74, 1), dtype=float32, min=0.0, max=1.856, mean=0.899),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'prev_rewards': np.ndarray((74,), dtype=float32, min=-0.334, max=0.0, mean=-0.296),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'rewards': np.ndarray((74,), dtype=float32, min=-0.334, max=-0.261, mean=-0.3),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           't': np.ndarray((74,), dtype=int64, min=0.0, max=73.0, mean=36.5),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'unroll_id': np.ndarray((74,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'value_targets': np.ndarray((74,), dtype=float32, min=-21.669, max=-0.312, mean=-11.656),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'vf_preds': np.ndarray((74,), dtype=float32, min=-26.16, max=-1.317, mean=-15.29)},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m   'flow_1.2': { 'data': { 'action_prob': np.ndarray((119,), dtype=float32, min=0.008, max=6.883, mean=2.377),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'actions': np.ndarray((119, 1), dtype=float32, min=-1.543, max=2.081, mean=0.795),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'advantages': np.ndarray((119,), dtype=float32, min=-0.463, max=7.462, mean=2.19),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'agent_index': np.ndarray((119,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'behaviour_logits': np.ndarray((119, 2), dtype=float32, min=-3.051, max=1.562, mean=-0.621),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'dones': np.ndarray((119,), dtype=bool, min=0.0, max=1.0, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'eps_id': np.ndarray((119,), dtype=int64, min=1883949945.0, max=1883949945.0, mean=1883949945.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'infos': np.ndarray((119,), dtype=object, head={'mean_vel': 19.216030176357854, 'cost1': 0.6943007103500446, 'cost2': 0.0, 'outflow': 465.5172413793103}),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'new_obs': np.ndarray((119, 12), dtype=float32, min=-0.143, max=1.0, mean=0.383),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'obs': np.ndarray((119, 12), dtype=float32, min=-0.143, max=1.0, mean=0.38),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'prev_actions': np.ndarray((119, 1), dtype=float32, min=-1.543, max=2.081, mean=0.78),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'prev_rewards': np.ndarray((119,), dtype=float32, min=-0.354, max=-0.261, mean=-0.319),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'rewards': np.ndarray((119,), dtype=float32, min=-0.354, max=-0.261, mean=-0.319),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           't': np.ndarray((119,), dtype=int64, min=15.0, max=133.0, mean=74.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'unroll_id': np.ndarray((119,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'value_targets': np.ndarray((119,), dtype=float32, min=-27.071, max=-0.324, mean=-15.744),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'vf_preds': np.ndarray((119,), dtype=float32, min=-32.864, max=-1.31, mean=-17.935)},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m   'flow_1.3': { 'data': { 'action_prob': np.ndarray((120,), dtype=float32, min=0.022, max=9.036, mean=2.695),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'actions': np.ndarray((120, 1), dtype=float32, min=-1.552, max=1.654, mean=0.693),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'advantages': np.ndarray((120,), dtype=float32, min=0.13, max=2.821, mean=1.456),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'agent_index': np.ndarray((120,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'behaviour_logits': np.ndarray((120, 2), dtype=float32, min=-3.347, max=1.562, mean=-0.716),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'dones': np.ndarray((120,), dtype=bool, min=0.0, max=1.0, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'eps_id': np.ndarray((120,), dtype=int64, min=1883949945.0, max=1883949945.0, mean=1883949945.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'infos': np.ndarray((120,), dtype=object, head={'mean_vel': 17.534220388196392, 'cost1': 0.6271964848064259, 'cost2': 0.0, 'outflow': 977.1428571428571}),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'new_obs': np.ndarray((120, 12), dtype=float32, min=-0.143, max=1.0, mean=0.4),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'obs': np.ndarray((120, 12), dtype=float32, min=-0.143, max=1.0, mean=0.398),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'prev_actions': np.ndarray((120, 1), dtype=float32, min=-1.552, max=1.654, mean=0.684),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'prev_rewards': np.ndarray((120,), dtype=float32, min=-0.372, max=-0.307, mean=-0.336),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'rewards': np.ndarray((120,), dtype=float32, min=-0.372, max=-0.307, mean=-0.336),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           't': np.ndarray((120,), dtype=int64, min=74.0, max=193.0, mean=133.5),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'unroll_id': np.ndarray((120,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'value_targets': np.ndarray((120,), dtype=float32, min=-26.406, max=-0.33, mean=-16.135),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'vf_preds': np.ndarray((120,), dtype=float32, min=-27.52, max=-1.064, mean=-17.591)},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m   'flow_1.4': { 'data': { 'action_prob': np.ndarray((147,), dtype=float32, min=0.058, max=13.646, mean=3.543),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'actions': np.ndarray((147, 1), dtype=float32, min=-1.509, max=1.995, mean=0.717),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'advantages': np.ndarray((147,), dtype=float32, min=-7.897, max=5.245, mean=-0.49),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'agent_index': np.ndarray((147,), dtype=int64, min=4.0, max=4.0, mean=4.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'behaviour_logits': np.ndarray((147, 2), dtype=float32, min=-3.54, max=1.646, mean=-0.829),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'dones': np.ndarray((147,), dtype=bool, min=0.0, max=1.0, mean=0.007),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'eps_id': np.ndarray((147,), dtype=int64, min=1883949945.0, max=1883949945.0, mean=1883949945.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'infos': np.ndarray((147,), dtype=object, head={'mean_vel': 17.33731452103985, 'cost1': 0.6091767825768515, 'cost2': 0.0, 'outflow': 1162.5}),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'new_obs': np.ndarray((147, 12), dtype=float32, min=-0.431, max=1.0, mean=0.401),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'obs': np.ndarray((147, 12), dtype=float32, min=-0.431, max=1.0, mean=0.399),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'prev_actions': np.ndarray((147, 1), dtype=float32, min=-1.509, max=1.995, mean=0.705),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'prev_rewards': np.ndarray((147,), dtype=float32, min=-0.47, max=-0.305, mean=-0.384),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'rewards': np.ndarray((147,), dtype=float32, min=-0.47, max=-0.305, mean=-0.384),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           't': np.ndarray((147,), dtype=int64, min=139.0, max=285.0, mean=212.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'unroll_id': np.ndarray((147,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'value_targets': np.ndarray((147,), dtype=float32, min=-26.747, max=-0.395, mean=-18.9),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'vf_preds': np.ndarray((147,), dtype=float32, min=-28.167, max=-1.274, mean=-18.41)},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m   'flow_1.5': { 'data': { 'action_prob': np.ndarray((122,), dtype=float32, min=0.033, max=15.001, mean=3.451),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'actions': np.ndarray((122, 1), dtype=float32, min=-1.404, max=3.258, mean=0.546),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'advantages': np.ndarray((122,), dtype=float32, min=-10.719, max=18.778, mean=0.139),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'agent_index': np.ndarray((122,), dtype=int64, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'behaviour_logits': np.ndarray((122, 2), dtype=float32, min=-3.662, max=1.595, mean=-0.788),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'dones': np.ndarray((122,), dtype=bool, min=0.0, max=1.0, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'eps_id': np.ndarray((122,), dtype=int64, min=1883949945.0, max=1883949945.0, mean=1883949945.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'infos': np.ndarray((122,), dtype=object, head={'mean_vel': 17.242525886210004, 'cost1': 0.6197518503652758, 'cost2': 0.0, 'outflow': 1404.0}),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'new_obs': np.ndarray((122, 12), dtype=float32, min=-0.572, max=1.0, mean=0.272),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'obs': np.ndarray((122, 12), dtype=float32, min=-0.572, max=1.0, mean=0.271),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'prev_actions': np.ndarray((122, 1), dtype=float32, min=-1.404, max=3.258, mean=0.541),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'prev_rewards': np.ndarray((122,), dtype=float32, min=-0.492, max=-0.305, mean=-0.428),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'rewards': np.ndarray((122,), dtype=float32, min=-0.492, max=-0.305, mean=-0.43),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           't': np.ndarray((122,), dtype=int64, min=203.0, max=324.0, mean=263.5),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'unroll_id': np.ndarray((122,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'value_targets': np.ndarray((122,), dtype=float32, min=-29.473, max=-0.49, mean=-22.014),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'vf_preds': np.ndarray((122,), dtype=float32, min=-29.692, max=-11.381, mean=-22.153)},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m   'flow_1.6': { 'data': { 'action_prob': np.ndarray((58,), dtype=float32, min=0.144, max=4.156, mean=1.266),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'actions': np.ndarray((58, 1), dtype=float32, min=-1.421, max=2.536, mean=0.009),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'advantages': np.ndarray((58,), dtype=float32, min=-7.02, max=27.468, mean=8.373),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'agent_index': np.ndarray((58,), dtype=int64, min=6.0, max=6.0, mean=6.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'behaviour_logits': np.ndarray((58, 2), dtype=float32, min=-2.535, max=1.586, mean=-0.633),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'dones': np.ndarray((58,), dtype=bool, min=0.0, max=1.0, mean=0.017),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'eps_id': np.ndarray((58,), dtype=int64, min=1883949945.0, max=1883949945.0, mean=1883949945.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'infos': np.ndarray((58,), dtype=object, head={'mean_vel': 13.256772176966825, 'cost1': 0.5041590054983139, 'cost2': 0.0, 'outflow': 1476.0}),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'new_obs': np.ndarray((58, 12), dtype=float32, min=-0.572, max=1.0, mean=0.181),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'obs': np.ndarray((58, 12), dtype=float32, min=-0.572, max=1.0, mean=0.184),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'prev_actions': np.ndarray((58, 1), dtype=float32, min=-1.421, max=2.536, mean=-0.013),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'prev_rewards': np.ndarray((58,), dtype=float32, min=-0.492, max=-0.381, mean=-0.432),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'rewards': np.ndarray((58,), dtype=float32, min=-0.492, max=-0.381, mean=-0.433),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           't': np.ndarray((58,), dtype=int64, min=267.0, max=324.0, mean=295.5),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'unroll_id': np.ndarray((58,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'value_targets': np.ndarray((58,), dtype=float32, min=-28.615, max=-0.49, mean=-19.328),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                           'vf_preds': np.ndarray((58,), dtype=float32, min=-32.049, max=-10.042, mean=-27.701)},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m                 'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-28 20:58:08,222\tINFO json_writer.py:97 -- Writing to new output file <_io.TextIOWrapper name='/headless/rl_project/flow_codes/ModelBased/expert_sample/output-2019-05-28_20-58-08_worker-0_0.json' mode='w' encoding='UTF-8'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m 2019-05-28 20:58:08,053\tINFO policy_evaluator.py:474 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m { 'data': { 'action_prob': np.ndarray((774,), dtype=float32, min=0.008, max=15.001, mean=2.86),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             'actions': np.ndarray((774, 1), dtype=float32, min=-1.552, max=3.258, mean=0.658),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             'advantages': np.ndarray((774,), dtype=float32, min=-10.719, max=27.468, mean=1.353),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             'agent_index': np.ndarray((774,), dtype=int64, min=0.0, max=6.0, mean=3.021),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             'behaviour_logits': np.ndarray((774, 2), dtype=float32, min=-3.662, max=1.646, mean=-0.72),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             'dones': np.ndarray((774,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             'eps_id': np.ndarray((774,), dtype=int64, min=697840902.0, max=1883949945.0, mean=1718446357.605),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             'infos': np.ndarray((774,), dtype=object, head={'mean_vel': 18.34238713654684, 'cost1': 0.6602456524033717, 'cost2': 0.0, 'outflow': 445.5445544554455}),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             'new_obs': np.ndarray((774, 12), dtype=float32, min=-0.572, max=1.0, mean=0.359),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             'obs': np.ndarray((774, 12), dtype=float32, min=-0.572, max=1.0, mean=0.357),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             'prev_actions': np.ndarray((774, 1), dtype=float32, min=-1.552, max=3.258, mean=0.642),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             'prev_rewards': np.ndarray((774,), dtype=float32, min=-0.492, max=0.0, mean=-0.357),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             'rewards': np.ndarray((774,), dtype=float32, min=-0.492, max=-0.258, mean=-0.36),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             't': np.ndarray((774,), dtype=int64, min=0.0, max=324.0, mean=143.282),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             'unroll_id': np.ndarray((774,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             'value_targets': np.ndarray((774,), dtype=float32, min=-36.991, max=-0.269, mean=-18.095),\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m             'vf_preds': np.ndarray((774,), dtype=float32, min=-36.863, max=-1.064, mean=-19.448)},\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Warning: Teleporting vehicle 'flow_0.2'; collision with vehicle 'flow_2.1', lane=':center_1_0', gap=-1.00, time=51.00 stage=move.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    samples = agent.sample(agent.train_batch_size)\n",
    "    samples.count\n",
    "\n",
    "    writer.write(sample_batch=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-28 20:59:09,311\tWARNING json_reader.py:52 -- Treating input directory as glob pattern: /headless/rl_project/flow_codes/ModelBased/expert_sample/*.json\n",
      "2019-05-28 20:59:09,314\tINFO json_reader.py:65 -- Found 1 input files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2364"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = JsonReader(\"./expert_sample\")\n",
    "sample = reader.next()\n",
    "sample.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4461"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.next().count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11122)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11120)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=11124)\u001b[0m Loading configuration... done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-28 21:00:29,951\tINFO policy_evaluator.py:564 -- Training on concatenated sample batches:\n",
      "\n",
      "{ 'data': { 'action_prob': np.ndarray((2379,), dtype=float32, min=0.001, max=15.399, mean=2.565),\n",
      "            'actions': np.ndarray((2379, 1), dtype=float32, min=-5.937, max=5.656, mean=0.704),\n",
      "            'advantages': np.ndarray((2379,), dtype=float32, min=-13.093, max=24.699, mean=2.184),\n",
      "            'agent_index': np.ndarray((2379,), dtype=int64, min=0.0, max=6.0, mean=3.275),\n",
      "            'behaviour_logits': np.ndarray((2379, 2), dtype=float32, min=-3.678, max=2.013, mean=-0.566),\n",
      "            'dones': np.ndarray((2379,), dtype=bool, min=0.0, max=1.0, mean=0.012),\n",
      "            'eps_id': np.ndarray((2379,), dtype=int64, min=165897188.0, max=1160255321.0, mean=672477232.312),\n",
      "            'infos': np.ndarray((2379,), dtype=object, head={'mean_vel': 12.803015796527372, 'cost1': 0.49515484300469237, 'cost2': 0.0, 'outflow': 1512.0}),\n",
      "            'new_obs': np.ndarray((2379, 12), dtype=float32, min=-0.765, max=1.0, mean=0.349),\n",
      "            'obs': np.ndarray((2379, 12), dtype=float32, min=-0.765, max=1.0, mean=0.347),\n",
      "            'prev_actions': np.ndarray((2379, 1), dtype=float32, min=-5.937, max=5.656, mean=0.69),\n",
      "            'prev_rewards': np.ndarray((2379,), dtype=float32, min=-0.574, max=0.0, mean=-0.405),\n",
      "            'rewards': np.ndarray((2379,), dtype=float32, min=-0.574, max=-0.26, mean=-0.406),\n",
      "            't': np.ndarray((2379,), dtype=int64, min=0.0, max=324.0, mean=173.19),\n",
      "            'unroll_id': np.ndarray((2379,), dtype=int64, min=12.0, max=12.0, mean=12.0),\n",
      "            'value_targets': np.ndarray((2379,), dtype=float32, min=-42.004, max=-0.269, mean=-19.477),\n",
      "            'vf_preds': np.ndarray((2379,), dtype=float32, min=-43.599, max=-0.711, mean=-21.661)},\n",
      "  'type': 'SampleBatch'}\n",
      "\n",
      "2019-05-28 21:00:29,952\tINFO tf_run_builder.py:89 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "2019-05-28 21:00:30,334\tINFO policy_evaluator.py:586 -- Training output:\n",
      "\n",
      "{ 'learner_stats': { 'cur_kl_coeff': 0.32036132,\n",
      "                     'cur_lr': 0.0005000000237487257,\n",
      "                     'entropy': 0.50045687,\n",
      "                     'kl': -1.8916144e-09,\n",
      "                     'model': {},\n",
      "                     'policy_loss': -2.183929,\n",
      "                     'total_loss': 18.50116,\n",
      "                     'vf_explained_var': 0.8779771,\n",
      "                     'vf_loss': 20.685093}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'config': {'batch_mode': 'truncate_episodes',\n",
       "  'callbacks': {'on_episode_end': None,\n",
       "   'on_episode_start': None,\n",
       "   'on_episode_step': None,\n",
       "   'on_postprocess_traj': None,\n",
       "   'on_sample_end': None,\n",
       "   'on_train_result': None},\n",
       "  'clip_actions': False,\n",
       "  'clip_param': 0.3,\n",
       "  'clip_rewards': None,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'compress_observations': False,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'entropy_coeff': 0.0,\n",
       "  'env': 'MultiWaveAttenuationMergePOEnv-v0',\n",
       "  'env_config': {'flow_params': '{\\n    \"env\": {\\n        \"additional_params\": {\\n            \"FLOW_RATE\": 2000,\\n            \"FLOW_RATE_MERGE\": 100,\\n            \"RL_PENETRATION\": 0.1,\\n            \"buf_length\": 1,\\n            \"eta1\": 1.0,\\n            \"eta2\": 0.2,\\n            \"eta3\": 0.1,\\n            \"max_accel\": 3,\\n            \"max_decel\": 3,\\n            \"reward_scale\": 1.0,\\n            \"t_min\": 1.0,\\n            \"target_velocity\": 25\\n        },\\n        \"evaluate\": false,\\n        \"horizon\": 750,\\n        \"sims_per_step\": 2,\\n        \"warmup_steps\": 100\\n    },\\n    \"env_name\": \"MultiWaveAttenuationMergePOEnv\",\\n    \"exp_tag\": \"multi_merge\",\\n    \"initial\": {\\n        \"additional_params\": {},\\n        \"bunching\": 0,\\n        \"edges_distribution\": \"all\",\\n        \"lanes_distribution\": Infinity,\\n        \"min_gap\": 0,\\n        \"perturbation\": 0.0,\\n        \"shuffle\": false,\\n        \"spacing\": \"uniform\",\\n        \"x0\": 0\\n    },\\n    \"net\": {\\n        \"additional_params\": {\\n            \"highway_lanes\": 1,\\n            \"merge_lanes\": 1,\\n            \"merge_length\": 100,\\n            \"post_merge_length\": 100,\\n            \"pre_merge_length\": 600,\\n            \"speed_limit\": 30\\n        },\\n        \"inflows\": {\\n            \"_InFlows__flows\": [\\n                {\\n                    \"begin\": 1,\\n                    \"departLane\": \"free\",\\n                    \"departSpeed\": 10,\\n                    \"edge\": \"inflow_highway\",\\n                    \"end\": 2000000.0,\\n                    \"name\": \"flow_0\",\\n                    \"vehsPerHour\": 1800.0,\\n                    \"vtype\": \"human\"\\n                },\\n                {\\n                    \"begin\": 1,\\n                    \"departLane\": \"free\",\\n                    \"departSpeed\": 10,\\n                    \"edge\": \"inflow_highway\",\\n                    \"end\": 2000000.0,\\n                    \"name\": \"flow_1\",\\n                    \"vehsPerHour\": 200.0,\\n                    \"vtype\": \"rl\"\\n                },\\n                {\\n                    \"begin\": 1,\\n                    \"departLane\": \"free\",\\n                    \"departSpeed\": 7.5,\\n                    \"edge\": \"inflow_merge\",\\n                    \"end\": 2000000.0,\\n                    \"name\": \"flow_2\",\\n                    \"vehsPerHour\": 100,\\n                    \"vtype\": \"human\"\\n                }\\n            ],\\n            \"num_flows\": 3\\n        },\\n        \"netfile\": null,\\n        \"no_internal_links\": false,\\n        \"osm_path\": null\\n    },\\n    \"scenario\": \"MergeScenario\",\\n    \"sim\": {\\n        \"emission_path\": null,\\n        \"lateral_resolution\": null,\\n        \"no_step_log\": true,\\n        \"num_clients\": 1,\\n        \"overtake_right\": false,\\n        \"port\": null,\\n        \"print_warnings\": true,\\n        \"pxpm\": 2,\\n        \"render\": false,\\n        \"restart_instance\": true,\\n        \"save_render\": false,\\n        \"seed\": null,\\n        \"show_radius\": false,\\n        \"sight_radius\": 25,\\n        \"sim_step\": 0.2,\\n        \"teleport_time\": -1\\n    },\\n    \"simulator\": \"traci\",\\n    \"veh\": [\\n        {\\n            \"acceleration_controller\": [\\n                \"SimCarFollowingController\",\\n                {}\\n            ],\\n            \"car_following_params\": {\\n                \"controller_params\": {\\n                    \"accel\": 1.0,\\n                    \"carFollowModel\": \"IDM\",\\n                    \"decel\": 1.5,\\n                    \"impatience\": 0.5,\\n                    \"maxSpeed\": 30,\\n                    \"minGap\": 2.5,\\n                    \"sigma\": 0.5,\\n                    \"speedDev\": 0.1,\\n                    \"speedFactor\": 1.0,\\n                    \"tau\": 1.0\\n                },\\n                \"speed_mode\": 1\\n            },\\n            \"initial_speed\": 0,\\n            \"lane_change_controller\": [\\n                \"SimLaneChangeController\",\\n                {}\\n            ],\\n            \"lane_change_params\": {\\n                \"controller_params\": {\\n                    \"laneChangeModel\": \"LC2013\",\\n                    \"lcCooperative\": \"1.0\",\\n                    \"lcKeepRight\": \"1.0\",\\n                    \"lcSpeedGain\": \"1.0\",\\n                    \"lcStrategic\": \"1.0\"\\n                },\\n                \"lane_change_mode\": 512\\n            },\\n            \"num_vehicles\": 5,\\n            \"routing_controller\": null,\\n            \"veh_id\": \"human\"\\n        },\\n        {\\n            \"acceleration_controller\": [\\n                \"RLController\",\\n                {}\\n            ],\\n            \"car_following_params\": {\\n                \"controller_params\": {\\n                    \"accel\": 1.0,\\n                    \"carFollowModel\": \"IDM\",\\n                    \"decel\": 1.5,\\n                    \"impatience\": 0.5,\\n                    \"maxSpeed\": 30,\\n                    \"minGap\": 2.5,\\n                    \"sigma\": 0.5,\\n                    \"speedDev\": 0.1,\\n                    \"speedFactor\": 1.0,\\n                    \"tau\": 1.0\\n                },\\n                \"speed_mode\": 1\\n            },\\n            \"initial_speed\": 0,\\n            \"lane_change_controller\": [\\n                \"SimLaneChangeController\",\\n                {}\\n            ],\\n            \"lane_change_params\": {\\n                \"controller_params\": {\\n                    \"laneChangeModel\": \"LC2013\",\\n                    \"lcCooperative\": \"1.0\",\\n                    \"lcKeepRight\": \"1.0\",\\n                    \"lcSpeedGain\": \"1.0\",\\n                    \"lcStrategic\": \"1.0\"\\n                },\\n                \"lane_change_mode\": 512\\n            },\\n            \"num_vehicles\": 0,\\n            \"routing_controller\": null,\\n            \"veh_id\": \"rl\"\\n        }\\n    ]\\n}'},\n",
       "  'expert_path': '/headless/rl_project/flow_codes/ModelBased/expert_sample',\n",
       "  'gamma': 0.99,\n",
       "  'grad_clip': None,\n",
       "  'horizon': 750,\n",
       "  'ignore_worker_failures': False,\n",
       "  'input': 'sampler',\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'kl_coeff': 0.2,\n",
       "  'kl_target': 0.01,\n",
       "  'lambda': 0.97,\n",
       "  'local_evaluator_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
       "   'intra_op_parallelism_threads': 8},\n",
       "  'log_level': 'INFO',\n",
       "  'lr': 0.0005,\n",
       "  'lr_schedule': None,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'model': {'conv_activation': 'relu',\n",
       "   'conv_filters': None,\n",
       "   'custom_model': None,\n",
       "   'custom_options': {},\n",
       "   'custom_preprocessor': None,\n",
       "   'dim': 84,\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_hiddens': [128, 64, 32],\n",
       "   'framestack': True,\n",
       "   'free_log_std': False,\n",
       "   'grayscale': False,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action_reward': False,\n",
       "   'max_seq_len': 20,\n",
       "   'squash_to_range': False,\n",
       "   'use_lstm': False,\n",
       "   'zero_mean': True},\n",
       "  'monitor': False,\n",
       "  'multiagent': {'policies_to_train': ['default_policy'],\n",
       "   'policy_graphs': {'default_policy': (ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph,\n",
       "     Box(12,),\n",
       "     Box(1,),\n",
       "     {})},\n",
       "   'policy_mapping_fn': tune.function(<function <lambda> at 0x7ff2af850488>)},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'num_gpus': 0,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'num_sgd_iter': 10,\n",
       "  'num_workers': 3,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'optimizer': {},\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'postprocess_inputs': False,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'remote_worker_envs': False,\n",
       "  'sample_async': False,\n",
       "  'sample_batch_size': 375.0,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'simple_optimizer': False,\n",
       "  'soft_horizon': False,\n",
       "  'straggler_mitigation': False,\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'allow_soft_placement': True,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'intra_op_parallelism_threads': 2,\n",
       "   'log_device_placement': False},\n",
       "  'train_batch_size': 2250,\n",
       "  'use_gae': True,\n",
       "  'vf_clip_param': 1000000.0,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'vf_share_layers': False},\n",
       " 'custom_metrics': {},\n",
       " 'date': '2019-05-28_21-00-30',\n",
       " 'done': False,\n",
       " 'episode_len_mean': 318.3777777777778,\n",
       " 'episode_reward_max': -18.391516895552353,\n",
       " 'episode_reward_mean': -251.17443368764046,\n",
       " 'episode_reward_min': -349.60020069043605,\n",
       " 'episodes_this_iter': 45,\n",
       " 'episodes_total': 45,\n",
       " 'experiment_id': 'fb82036d740e4d729608117532bbcdbd',\n",
       " 'hostname': 'kronos',\n",
       " 'iterations_since_restore': 1,\n",
       " 'node_ip': '169.237.32.118',\n",
       " 'num_metric_batches_dropped': 0,\n",
       " 'off_policy_estimator': {},\n",
       " 'pid': 11088,\n",
       " 'policy_reward_mean': {},\n",
       " 'sampler_perf': {'mean_env_wait_ms': 7.5071083576848086,\n",
       "  'mean_inference_ms': 1.0356266771733258,\n",
       "  'mean_processing_ms': 4.971569618436853},\n",
       " 'time_since_restore': 6.921435594558716,\n",
       " 'time_this_iter_s': 6.921435594558716,\n",
       " 'time_total_s': 6.921435594558716,\n",
       " 'timestamp': 1559077230,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'timesteps_total': None,\n",
       " 'training_iteration': 1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
