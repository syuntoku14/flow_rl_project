{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* find the varid horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, pickle, argparse, json, logging\n",
    "from gym import ObservationWrapper\n",
    "from copy import deepcopy\n",
    "import ray\n",
    "\n",
    "from meir import MEIRTrainer\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import PPOPolicyGraph\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "from ray.rllib.agents import Trainer\n",
    "from ray.rllib.evaluation import PolicyEvaluator, MultiAgentBatch\n",
    "from ray.rllib.evaluation.metrics import collect_metrics\n",
    "from ray.rllib.offline.json_reader import JsonReader\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.utils import merge_dicts\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.evaluation.postprocessing import discount\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder, get_flow_params\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_cpus = 3\n",
    "num_rollouts = 3\n",
    "horizon = 750\n",
    "gae_lambda = 0.97\n",
    "step_size = 5e-4\n",
    "num_iter = 10\n",
    "benchmark_name = \"multi_merge\"\n",
    "exp_name = \"test_ir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '169.237.32.118',\n",
       " 'object_store_address': '/tmp/ray/session_2019-05-27_21-31-03_8222/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-05-27_21-31-03_8222/sockets/raylet',\n",
       " 'redis_address': '169.237.32.118:27703',\n",
       " 'webui_url': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=num_cpus, logging_level=40, ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = deepcopy(DEFAULT_CONFIG)\n",
    "config[\"num_workers\"] = min(num_cpus, num_rollouts)\n",
    "config[\"train_batch_size\"] = horizon * num_rollouts\n",
    "config[\"sample_batch_size\"] = horizon / 2\n",
    "config[\"use_gae\"] = True\n",
    "config[\"horizon\"] = horizon\n",
    "config[\"lambda\"] = gae_lambda\n",
    "config[\"lr\"] = step_size\n",
    "config[\"vf_clip_param\"] = 1e6\n",
    "config[\"num_sgd_iter\"] = 10\n",
    "config['clip_actions'] = False  # FIXME(ev) temporary ray bug\n",
    "config[\"model\"][\"fcnet_hiddens\"] = [128, 64, 32]\n",
    "config[\"observation_filter\"] = \"NoFilter\"\n",
    "config[\"entropy_coeff\"] = 0.0\n",
    "config[\"num_train\"] = 2\n",
    "config[\"expert_path\"] = './expert_sample'\n",
    "config[\"theta_lr\"] = 0.1\n",
    "\n",
    "benchmark = __import__(\n",
    "            \"flow.benchmarks.%s\" % benchmark_name, fromlist=[\"flow_params\"])\n",
    "flow_params = benchmark.custom_rew_flow_params\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(\n",
    "    flow_params, cls=FlowParamsEncoder, sort_keys=True, indent=4)\n",
    "config['env_config']['flow_params'] = flow_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_env, env_name = make_create_env(params=flow_params, version=0)\n",
    "register_env(env_name, create_env)\n",
    "env = create_env()\n",
    "\n",
    "POLICY_ID = 'rl'\n",
    "default_policy = (PPOPolicyGraph, env.observation_space, env.action_space, {})\n",
    "policy_graph = {POLICY_ID: default_policy}\n",
    "config[\"multiagent\"] = {\n",
    "        'policy_graphs': policy_graph,\n",
    "        'policy_mapping_fn': tune.function(lambda agent_id: POLICY_ID),\n",
    "        'policies_to_train': [POLICY_ID]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnvPolicyEvaluator(PolicyEvaluator):\n",
    "    def set_theta_to_env(self, theta):\n",
    "        self.env.set_theta(theta)\n",
    "        \n",
    "    def get_theta_from_env(self):\n",
    "        return self.env.get_theta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEIRTrainer(Trainer):\n",
    "    _allow_unknown_configs = True\n",
    "    _name = \"MEIR\"\n",
    "    _default_config = DEFAULT_CONFIG\n",
    "    _policy_graph = PPOPolicyGraph\n",
    "    \n",
    "    @override(Trainer)\n",
    "    def _init(self, config, env_name):\n",
    "        self._policy_graph = self.config[\"multiagent\"][\"policy_graphs\"]\n",
    "        \n",
    "        self.local_evaluator = self.make_local_evaluator(\n",
    "             env_name, self._policy_graph, self.config)        \n",
    "        self.remote_evaluators = self.make_remote_evaluators(\n",
    "            env_name, self._policy_graph, self.config[\"num_workers\"])\n",
    "        \n",
    "        self.train_batch_size = self.config[\"train_batch_size\"]\n",
    "        self.num_sgd_iter = self.config[\"num_sgd_iter\"]\n",
    "        self.num_train = self.config[\"num_train\"]\n",
    "        self.expert_path = self.config[\"expert_path\"]\n",
    "        self.theta_lr = self.config[\"theta_lr\"]\n",
    "        \n",
    "        expert_reader = JsonReader(self.expert_path)\n",
    "        self.expert_samples = expert_reader.next()\n",
    "        self.expert_features = self.calculate_expected_feature(self.expert_samples)\n",
    "        self.theta = np.random.uniform(size=self.expert_features.shape)\n",
    "        \n",
    "    def make_local_evaluator(self,\n",
    "                             env_creator,\n",
    "                             policy_graph,\n",
    "                             extra_config=None):\n",
    "        \"\"\"Convenience method to return configured local evaluator.\"\"\"\n",
    "\n",
    "        return self._make_evaluator(\n",
    "            CustomEnvPolicyEvaluator,\n",
    "            env_creator,\n",
    "            policy_graph,\n",
    "            0,\n",
    "            merge_dicts(\n",
    "                # important: allow local tf to use more CPUs for optimization\n",
    "                merge_dicts(\n",
    "                    self.config, {\n",
    "                        \"tf_session_args\": self.\n",
    "                        config[\"local_evaluator_tf_session_args\"]\n",
    "                    }),\n",
    "                extra_config or {}))        \n",
    "    \n",
    "    def make_remote_evaluators(self, env_creator, policy_graph, count):\n",
    "        \"\"\"Convenience method to return a number of remote evaluators.\"\"\"\n",
    "\n",
    "        remote_args = {\n",
    "            \"num_cpus\": self.config[\"num_cpus_per_worker\"],\n",
    "            \"num_gpus\": self.config[\"num_gpus_per_worker\"],\n",
    "            \"resources\": self.config[\"custom_resources_per_worker\"],\n",
    "        }\n",
    "\n",
    "        cls = CustomEnvPolicyEvaluator.as_remote(**remote_args).remote\n",
    "\n",
    "        return [\n",
    "            self._make_evaluator(cls, env_creator, policy_graph, i + 1,\n",
    "                                 self.config) for i in range(count)\n",
    "        ]\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        self.set_theta_to_evaluators()\n",
    "        \n",
    "        # set local weights to remote\n",
    "        weights = ray.put(self.local_evaluator.get_weights())\n",
    "        for e in self.remote_evaluators:\n",
    "            e.set_weights.remote(weights)\n",
    "            \n",
    "        samples = []\n",
    "        while sum(s.count for s in samples) < sample_size:\n",
    "            samples.extend(\n",
    "                ray.get([\n",
    "                    e.sample.remote() for e in self.remote_evaluators\n",
    "                ]))\n",
    "        samples = MultiAgentBatch.concat_samples(samples)\n",
    "        return samples\n",
    "    \n",
    "    def calculate_expected_feature(self, samples):\n",
    "        features = np.mean(samples[\"obs\"], axis=0)\n",
    "        return features\n",
    "    \n",
    "    def train_policy_by_samples(self, samples):\n",
    "        # train policy by given samples\n",
    "        for i in range(self.num_sgd_iter):\n",
    "            fetches = self.local_evaluator.learn_on_batch(samples)\n",
    "            \n",
    "        def update(pi, pi_id):\n",
    "            if pi_id in fetches:\n",
    "                pi.update_kl(fetches[pi_id]['learner_stats'][\"kl\"])\n",
    "            else:\n",
    "                logger.debug(\n",
    "                    \"No data for {}, not updating kl\".format(pi_id))\n",
    "        self.local_evaluator.foreach_trainable_policy(update)       \n",
    "        \n",
    "    def set_theta_to_evaluators(self):\n",
    "        self.local_evaluator.set_theta_to_env(self.theta)\n",
    "        for e in self.remote_evaluators:\n",
    "            e.set_theta_to_env.remote(self.theta)\n",
    "    \n",
    "    def update_theta(self, samples, learning_rate=0.01):\n",
    "        # update and return the difference norm\n",
    "        features = self.calculate_expected_feature(samples)\n",
    "        update = self.expert_features - features\n",
    "        self.theta += learning_rate * update\n",
    "        return np.linalg.norm(self.expert_features - features)\n",
    "    \n",
    "    @override(Trainer)    \n",
    "    def _train(self):\n",
    "        self.set_theta_to_evaluators()\n",
    "        \n",
    "        # optimize policy under estimated reward\n",
    "        for train_iter in range(self.num_train):\n",
    "            # collect samples with new reward fnc\n",
    "            samples = self.sample(self.train_batch_size)\n",
    "\n",
    "            # train local based on samples\n",
    "            self.train_policy_by_samples(samples)\n",
    "            res = collect_metrics(self.local_evaluator, self.remote_evaluators)\n",
    "            pretty_print(res)\n",
    "        \n",
    "        samples = self.sample(self.train_batch_size) \n",
    "        norm = self.update_theta(samples, self.theta_lr)\n",
    "        \n",
    "        res[\"custom_metrics\"][\"theta_norm\"] = norm\n",
    "        return res\n",
    "\n",
    "    @override(Trainer)\n",
    "    def __getstate__(self):\n",
    "        state = super().__getstate__()\n",
    "        state[\"theta\"] = self.theta\n",
    "        return state\n",
    "    \n",
    "    @override(Trainer)\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if \"theta\" in state:\n",
    "            self.theta = state[\"theta\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-27 21:31:27,440\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "/opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "2019-05-27 21:31:28,875\tINFO policy_evaluator.py:728 -- Built policy map: {'rl': <ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph object at 0x7f4b84609c88>}\n",
      "2019-05-27 21:31:28,876\tINFO policy_evaluator.py:729 -- Built preprocessor map: {'rl': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f4b84609940>}\n",
      "2019-05-27 21:31:28,877\tINFO policy_evaluator.py:343 -- Built filter map: {'rl': <ray.rllib.utils.filter.NoFilter object at 0x7f4b8460cdd8>}\n",
      "2019-05-27 21:31:28,961\tWARNING json_reader.py:52 -- Treating input directory as glob pattern: /headless/rl_project/flow_codes/ModelBased/expert_sample/*.json\n",
      "2019-05-27 21:31:28,962\tINFO json_reader.py:65 -- Found 1 input files.\n"
     ]
    }
   ],
   "source": [
    "agent = MEIRTrainer(config, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-062e94f49eb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_policy_by_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "agent.train_policy_by_samples(multi_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ecc99f88a8e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "agent.local_evaluator.learn_on_batch(multi_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m 2019-05-27 21:04:21,855\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m 2019-05-27 21:04:21.857309: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m 2019-05-27 21:04:22,035\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m 2019-05-27 21:04:22.037521: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m 2019-05-27 21:04:22,069\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m 2019-05-27 21:04:22.071130: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m 2019-05-27 21:04:23,426\tINFO policy_evaluator.py:437 -- Generating sample batch of size 375.0\n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m 2019-05-27 21:04:24,913\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'flow_1.0': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.499),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m        'flow_1.1': np.ndarray((12,), dtype=float32, min=0.004, max=0.738, mean=0.184)}}\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m 2019-05-27 21:04:24,913\tINFO sampler.py:309 -- Info return from env: {0: {'flow_1.0': {}, 'flow_1.1': {}}}\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m 2019-05-27 21:04:24,914\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.499)\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m 2019-05-27 21:04:24,914\tINFO sampler.py:411 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.499)\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m 2019-05-27 21:04:24,916\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m { 'rl': [ { 'data': { 'agent_id': 'flow_1.0',\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                       'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                       'info': {},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                       'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.499),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                       'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                       'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                       'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m             'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m           { 'data': { 'agent_id': 'flow_1.1',\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                       'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                       'info': {},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                       'obs': np.ndarray((12,), dtype=float32, min=0.004, max=0.738, mean=0.184),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                       'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                       'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                       'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m             'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m 2019-05-27 21:04:24,916\tINFO tf_run_builder.py:89 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m 2019-05-27 21:04:24,957\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m { 'rl': ( np.ndarray((2, 1), dtype=float32, min=-0.023, max=1.941, mean=0.959),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m           [],\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m           { 'action_prob': np.ndarray((2,), dtype=float32, min=0.062, max=0.398, mean=0.23),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m             'behaviour_logits': np.ndarray((2, 2), dtype=float32, min=-0.003, max=0.005, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m             'vf_preds': np.ndarray((2,), dtype=float32, min=-0.006, max=-0.001, mean=-0.003)})}\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m 2019-05-27 21:04:28,401\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m { 'flow_1.0': { 'data': { 'action_prob': np.ndarray((36,), dtype=float32, min=0.007, max=0.398, mean=0.272),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'actions': np.ndarray((36, 1), dtype=float32, min=-1.859, max=2.857, mean=-0.161),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'advantages': np.ndarray((36,), dtype=float32, min=4.481, max=73.877, mean=49.165),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'agent_index': np.ndarray((36,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'behaviour_logits': np.ndarray((36, 2), dtype=float32, min=0.001, max=0.005, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'dones': np.ndarray((36,), dtype=bool, min=0.0, max=1.0, mean=0.028),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'eps_id': np.ndarray((36,), dtype=int64, min=558694523.0, max=558694523.0, mean=558694523.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'infos': np.ndarray((36,), dtype=object, head={'outflow': 445.5445544554455, 'mean_vel': 17.49239005744129, 'cost1': 0.6492589031746567, 'cost2': 0.0}),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'new_obs': np.ndarray((36, 12), dtype=float32, min=-0.043, max=1.0, mean=0.516),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'obs': np.ndarray((36, 12), dtype=float32, min=-0.043, max=1.0, mean=0.514),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'prev_actions': np.ndarray((36, 1), dtype=float32, min=-1.859, max=2.857, mean=-0.128),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'prev_rewards': np.ndarray((36,), dtype=float32, min=0.0, max=4.508, mean=3.798),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'rewards': np.ndarray((36,), dtype=float32, min=3.686, max=4.508, mean=3.923),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           't': np.ndarray((36,), dtype=int64, min=0.0, max=35.0, mean=17.5),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'unroll_id': np.ndarray((36,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'value_targets': np.ndarray((36,), dtype=float32, min=4.481, max=73.877, mean=49.165),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'vf_preds': np.ndarray((36,), dtype=float32, min=-0.001, max=0.0, mean=-0.0)},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m   'flow_1.1': { 'data': { 'action_prob': np.ndarray((127,), dtype=float32, min=0.0, max=0.399, mean=0.288),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'actions': np.ndarray((127, 1), dtype=float32, min=-2.641, max=4.228, mean=0.012),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'advantages': np.ndarray((127,), dtype=float32, min=4.612, max=89.989, mean=68.007),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'agent_index': np.ndarray((127,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'behaviour_logits': np.ndarray((127, 2), dtype=float32, min=-0.003, max=0.005, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'dones': np.ndarray((127,), dtype=bool, min=0.0, max=1.0, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'eps_id': np.ndarray((127,), dtype=int64, min=558694523.0, max=558694523.0, mean=558694523.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'infos': np.ndarray((127,), dtype=object, head={'outflow': 445.5445544554455, 'mean_vel': 17.49239005744129, 'cost1': 0.6492589031746567, 'cost2': 0.0}),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'new_obs': np.ndarray((127, 12), dtype=float32, min=-0.306, max=1.0, mean=0.405),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'obs': np.ndarray((127, 12), dtype=float32, min=-0.306, max=1.0, mean=0.401),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'prev_actions': np.ndarray((127, 1), dtype=float32, min=-2.641, max=4.228, mean=0.02),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'prev_rewards': np.ndarray((127,), dtype=float32, min=0.0, max=4.612, mean=2.974),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'rewards': np.ndarray((127,), dtype=float32, min=0.332, max=4.612, mean=3.01),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           't': np.ndarray((127,), dtype=int64, min=0.0, max=126.0, mean=63.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'unroll_id': np.ndarray((127,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'value_targets': np.ndarray((127,), dtype=float32, min=4.612, max=89.989, mean=68.006),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'vf_preds': np.ndarray((127,), dtype=float32, min=-0.006, max=0.001, mean=-0.001)},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m   'flow_1.2': { 'data': { 'action_prob': np.ndarray((310,), dtype=float32, min=0.011, max=0.398, mean=0.278),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'actions': np.ndarray((310, 1), dtype=float32, min=-2.681, max=2.475, mean=-0.029),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'advantages': np.ndarray((310,), dtype=float32, min=4.721, max=118.401, mean=78.521),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'agent_index': np.ndarray((310,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'behaviour_logits': np.ndarray((310, 2), dtype=float32, min=-0.003, max=0.006, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'dones': np.ndarray((310,), dtype=bool, min=0.0, max=1.0, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'eps_id': np.ndarray((310,), dtype=int64, min=558694523.0, max=558694523.0, mean=558694523.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'infos': np.ndarray((310,), dtype=object, head={'outflow': 387.93103448275855, 'mean_vel': 17.886126447049264, 'cost1': 0.6729026739077641, 'cost2': 0.0}),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'new_obs': np.ndarray((310, 12), dtype=float32, min=-0.306, max=1.0, mean=0.424),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'obs': np.ndarray((310, 12), dtype=float32, min=-0.306, max=1.0, mean=0.423),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'prev_actions': np.ndarray((310, 1), dtype=float32, min=-2.681, max=2.475, mean=-0.024),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'prev_rewards': np.ndarray((310,), dtype=float32, min=0.727, max=4.796, mean=3.205),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'rewards': np.ndarray((310,), dtype=float32, min=0.727, max=4.796, mean=3.213),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           't': np.ndarray((310,), dtype=int64, min=15.0, max=324.0, mean=169.5),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'unroll_id': np.ndarray((310,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'value_targets': np.ndarray((310,), dtype=float32, min=4.721, max=118.402, mean=78.52),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'vf_preds': np.ndarray((310,), dtype=float32, min=-0.007, max=0.002, mean=-0.001)},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m   'flow_1.3': { 'data': { 'action_prob': np.ndarray((260,), dtype=float32, min=0.013, max=0.398, mean=0.278),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'actions': np.ndarray((260, 1), dtype=float32, min=-2.628, max=2.344, mean=-0.058),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'advantages': np.ndarray((260,), dtype=float32, min=1.493, max=37.207, mean=27.688),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'agent_index': np.ndarray((260,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'behaviour_logits': np.ndarray((260, 2), dtype=float32, min=-0.002, max=0.005, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'dones': np.ndarray((260,), dtype=bool, min=0.0, max=1.0, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'eps_id': np.ndarray((260,), dtype=int64, min=558694523.0, max=558694523.0, mean=558694523.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'infos': np.ndarray((260,), dtype=object, head={'outflow': 542.1686746987951, 'mean_vel': 10.526227063520466, 'cost1': 0.4054938220282822, 'cost2': 0.0}),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'new_obs': np.ndarray((260, 12), dtype=float32, min=-0.088, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'obs': np.ndarray((260, 12), dtype=float32, min=-0.088, max=1.0, mean=0.193),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'prev_actions': np.ndarray((260, 1), dtype=float32, min=-2.628, max=2.344, mean=-0.059),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'prev_rewards': np.ndarray((260,), dtype=float32, min=0.721, max=3.477, mean=1.212),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'rewards': np.ndarray((260,), dtype=float32, min=0.721, max=3.477, mean=1.21),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           't': np.ndarray((260,), dtype=int64, min=65.0, max=324.0, mean=194.5),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'unroll_id': np.ndarray((260,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'value_targets': np.ndarray((260,), dtype=float32, min=1.494, max=37.206, mean=27.686),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'vf_preds': np.ndarray((260,), dtype=float32, min=-0.006, max=0.001, mean=-0.002)},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m   'flow_1.4': { 'data': { 'action_prob': np.ndarray((188,), dtype=float32, min=0.006, max=0.399, mean=0.268),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'actions': np.ndarray((188, 1), dtype=float32, min=-2.087, max=2.896, mean=0.115),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'advantages': np.ndarray((188,), dtype=float32, min=0.64, max=23.876, mean=14.168),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'agent_index': np.ndarray((188,), dtype=int64, min=4.0, max=4.0, mean=4.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'behaviour_logits': np.ndarray((188, 2), dtype=float32, min=-0.004, max=0.002, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'dones': np.ndarray((188,), dtype=bool, min=0.0, max=1.0, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'eps_id': np.ndarray((188,), dtype=int64, min=558694523.0, max=558694523.0, mean=558694523.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'infos': np.ndarray((188,), dtype=object, head={'outflow': 794.1176470588235, 'mean_vel': 4.445526867195334, 'cost1': 0.173787507493746, 'cost2': 0.0}),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'new_obs': np.ndarray((188, 12), dtype=float32, min=-0.149, max=1.0, mean=0.11),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'obs': np.ndarray((188, 12), dtype=float32, min=-0.149, max=1.0, mean=0.11),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'prev_actions': np.ndarray((188, 1), dtype=float32, min=-2.087, max=2.896, mean=0.117),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'prev_rewards': np.ndarray((188,), dtype=float32, min=0.569, max=1.935, mean=0.691),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'rewards': np.ndarray((188,), dtype=float32, min=0.569, max=1.934, mean=0.684),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           't': np.ndarray((188,), dtype=int64, min=137.0, max=324.0, mean=230.5),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'unroll_id': np.ndarray((188,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'value_targets': np.ndarray((188,), dtype=float32, min=0.635, max=23.871, mean=14.162),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                           'vf_preds': np.ndarray((188,), dtype=float32, min=-0.006, max=-0.004, mean=-0.005)},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                 'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m Loading configuration... done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m 2019-05-27 21:04:30,348\tINFO policy_evaluator.py:474 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m { 'count': 375,\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m   'policy_batches': { 'rl': { 'data': { 'action_prob': np.ndarray((1030,), dtype=float32, min=0.0, max=0.399, mean=0.278),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         'actions': np.ndarray((1030, 1), dtype=float32, min=-2.951, max=4.228, mean=-0.026),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         'advantages': np.ndarray((1030,), dtype=float32, min=0.64, max=118.401, mean=47.033),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         'agent_index': np.ndarray((1030,), dtype=int64, min=0.0, max=4.0, mean=2.329),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         'behaviour_logits': np.ndarray((1030, 2), dtype=float32, min=-0.004, max=0.006, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         'dones': np.ndarray((1030,), dtype=bool, min=0.0, max=1.0, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         'eps_id': np.ndarray((1030,), dtype=int64, min=86210395.0, max=558694523.0, mean=508693775.474),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         'infos': np.ndarray((1030,), dtype=object, head={'outflow': 445.5445544554455, 'mean_vel': 17.49239005744129, 'cost1': 0.6492589031746567, 'cost2': 0.0}),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         'new_obs': np.ndarray((1030, 12), dtype=float32, min=-0.306, max=1.0, mean=0.303),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         'obs': np.ndarray((1030, 12), dtype=float32, min=-0.306, max=1.0, mean=0.302),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         'prev_actions': np.ndarray((1030, 1), dtype=float32, min=-2.951, max=4.228, mean=-0.023),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         'prev_rewards': np.ndarray((1030,), dtype=float32, min=0.0, max=4.796, mean=2.164),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         'rewards': np.ndarray((1030,), dtype=float32, min=0.332, max=4.796, mean=2.18),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         't': np.ndarray((1030,), dtype=int64, min=0.0, max=324.0, mean=153.108),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         'unroll_id': np.ndarray((1030,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         'value_targets': np.ndarray((1030,), dtype=float32, min=0.635, max=118.402, mean=47.031),\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                                         'vf_preds': np.ndarray((1030,), dtype=float32, min=-0.007, max=0.002, mean=-0.002)},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m                               'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=6930)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=6931)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=6927)\u001b[0m Loading configuration... done.\n"
     ]
    }
   ],
   "source": [
    "multi_samples = agent.sample(agent.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = multi_samples.policy_batches[\"rl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36196366,  0.6380364 ,  1.        , ...,  1.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.3697982 ,  0.6302018 ,  1.        , ...,  1.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.35862324,  0.64137673,  1.        , ...,  1.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.33383006, -0.05302032,  0.0212647 , ...,  0.00737061,\n",
       "         0.01090145,  0.52218354],\n",
       "       [ 0.33250976, -0.06333038,  0.02053326, ...,  0.01444553,\n",
       "         0.01105476,  0.5197702 ],\n",
       "       [ 0.3248492 , -0.0670458 ,  0.01973958, ..., -0.01763957,\n",
       "         0.010934  ,  0.5175963 ]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"obs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.2152696, 2.1731894, 2.1667137, ..., 1.7010003, 1.6543605,\n",
       "       1.6315465], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"rewards\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 5, 5, 5])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"agent_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{747833057, 988082960}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sample[\"eps_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 -0.6333765 -4.452097 False\n",
      "101 -0.63606596 -3.976601 False\n",
      "102 -0.6352114 -3.4785948 False\n",
      "103 -0.6365751 -2.960939 False\n",
      "104 -0.63839877 -2.420503 False\n",
      "105 -0.6410927 -1.8558179 False\n",
      "106 -0.6455265 -1.2649575 False\n",
      "107 -0.6455265 -0.6450524 True\n",
      "100 -0.6333765 -18.280396 False\n",
      "101 -0.63606596 -18.376608 False\n",
      "102 -0.6352114 -18.473948 False\n",
      "103 -0.6365751 -18.576174 False\n",
      "104 -0.63839877 -18.68123 False\n",
      "105 -0.6410927 -18.788738 False\n",
      "106 -0.6455265 -18.897936 False\n",
      "107 -0.65703934 -19.006937 False\n",
      "108 -0.658573 -19.107588 False\n",
      "109 -0.6617328 -19.211756 False\n",
      "110 -0.664001 -19.316866 False\n",
      "111 -0.6780474 -19.423962 False\n",
      "112 -0.68264073 -19.520853 False\n",
      "113 -0.6847072 -19.61694 False\n",
      "114 -0.6876112 -19.7149 False\n",
      "115 -0.6897008 -19.81383 False\n",
      "116 -0.7259685 -19.914776 False\n",
      "117 -0.7269529 -19.982077 False\n",
      "118 -0.7279562 -20.051197 False\n",
      "119 -0.7274213 -20.122076 False\n",
      "120 -0.74273753 -20.196487 False\n",
      "121 -0.7475499 -20.25798 False\n",
      "122 -0.74694234 -20.316977 False\n",
      "123 -0.74003094 -20.379135 False\n",
      "124 -0.7547501 -20.45101 False\n",
      "125 -0.75616866 -20.51058 False\n",
      "126 -0.7573259 -20.57114 False\n",
      "127 -0.7566428 -20.633003 False\n",
      "128 -0.77220654 -20.698137 False\n",
      "129 -0.7751359 -20.74976 False\n",
      "130 -0.77697456 -20.799168 False\n",
      "131 -0.7763789 -20.849926 False\n",
      "132 -0.79626703 -20.90469 False\n",
      "133 -0.7986873 -20.939697 False\n",
      "134 -0.7991361 -20.973658 False\n",
      "135 -0.7998975 -21.008621 False\n",
      "136 -0.8226587 -21.044203 False\n",
      "137 -0.8225454 -21.057549 False\n",
      "138 -0.8236475 -21.071606 False\n",
      "139 -0.824542 -21.085033 False\n",
      "140 -0.8468969 -21.098083 False\n",
      "141 -0.84540844 -21.088377 False\n",
      "142 -0.84399563 -21.079868 False\n",
      "143 -0.84236383 -21.072464 False\n",
      "144 -0.83898664 -21.066477 False\n",
      "145 -0.86118954 -21.063763 False\n",
      "146 -0.8584884 -21.037807 False\n",
      "147 -0.8576214 -21.01358 False\n",
      "148 -0.8524405 -20.98926 False\n",
      "149 -0.8495736 -20.969343 False\n",
      "150 -0.8483878 -20.951519 False\n",
      "151 -0.84612334 -20.934202 False\n",
      "152 -0.8418719 -20.918503 False\n",
      "153 -0.83650416 -20.906668 False\n",
      "154 -0.8343151 -20.899942 False\n",
      "155 -0.8314994 -20.895182 False\n",
      "156 -0.8291243 -20.893139 False\n",
      "157 -0.82552016 -20.893475 False\n",
      "158 -0.8092051 -20.897642 False\n",
      "159 -0.8111697 -20.91885 False\n",
      "160 -0.80935854 -20.938938 False\n",
      "161 -0.8078313 -20.961786 False\n",
      "162 -0.8081249 -20.987131 False\n",
      "163 -0.80681545 -21.013256 False\n",
      "164 -0.80547595 -21.041851 False\n",
      "165 -0.8063258 -21.07303 False\n",
      "166 -0.80841136 -21.104462 False\n",
      "167 -0.8298056 -21.135075 False\n",
      "168 -0.8321616 -21.143564 False\n",
      "169 -0.8336248 -21.15108 False\n",
      "170 -0.83452904 -21.157383 False\n",
      "171 -0.82452476 -21.16308 False\n",
      "172 -0.8268365 -21.17941 False\n",
      "173 -0.82802105 -21.193968 False\n",
      "174 -0.828893 -21.20787 False\n",
      "175 -0.8307227 -21.221495 False\n",
      "176 -0.8299254 -21.233767 False\n",
      "177 -0.829575 -21.247425 False\n",
      "178 -0.83010644 -21.261997 False\n",
      "179 -0.83011204 -21.276562 False\n",
      "180 -0.83142596 -21.29172 False\n",
      "181 -0.8305436 -21.306128 False\n",
      "182 -0.83308786 -21.322094 False\n",
      "183 -0.83334106 -21.335964 False\n",
      "184 -0.83544326 -21.3502 False\n",
      "185 -0.83569676 -21.362825 False\n",
      "186 -0.83697015 -21.375793 False\n",
      "187 -0.83844787 -21.38796 False\n",
      "188 -0.8379971 -21.399044 False\n",
      "189 -0.8393952 -21.41113 False\n",
      "190 -0.8399906 -21.422195 False\n",
      "191 -0.84238076 -21.43312 False\n",
      "192 -0.8432661 -21.44195 False\n",
      "193 -0.84546214 -21.45031 False\n",
      "194 -0.84837115 -21.45666 False\n",
      "195 -0.83382195 -21.460266 False\n",
      "196 -0.83731246 -21.479218 False\n",
      "197 -0.83927906 -21.495327 False\n",
      "198 -0.8405066 -21.510021 False\n",
      "199 -0.8408216 -21.52406 False\n",
      "200 -0.84568137 -21.538383 False\n",
      "201 -0.84817487 -21.548195 False\n",
      "202 -0.84996945 -21.555769 False\n",
      "203 -0.83862144 -21.56184 False\n",
      "204 -0.8408708 -21.579973 False\n",
      "205 -0.84262216 -21.596493 False\n",
      "206 -0.84682506 -21.613361 False\n",
      "207 -0.8475389 -21.62494 False\n",
      "208 -0.8501069 -21.636406 False\n",
      "209 -0.8507569 -21.645592 False\n",
      "210 -0.8521878 -21.65455 False\n",
      "211 -0.8562631 -21.66239 False\n",
      "212 -0.8578906 -21.66617 False\n",
      "213 -0.8594743 -21.668482 False\n",
      "214 -0.8613165 -21.669264 False\n",
      "215 -0.86126304 -21.66817 False\n",
      "216 -0.8487048 -21.66711 False\n",
      "217 -0.87223077 -21.679052 False\n",
      "218 -0.87338525 -21.665852 False\n",
      "219 -0.87324744 -21.65208 False\n",
      "220 -0.87409276 -21.637886 False\n",
      "221 -0.87551636 -21.622246 False\n",
      "222 -0.874714 -21.604479 False\n",
      "223 -0.87573266 -21.586834 False\n",
      "224 -0.8752637 -21.567402 False\n",
      "225 -0.86032945 -21.54763 False\n",
      "226 -0.8587633 -21.54258 False\n",
      "227 -0.8582422 -21.538977 False\n",
      "228 -0.8604619 -21.535788 False\n",
      "229 -0.8598449 -21.530016 False\n",
      "230 -0.861053 -21.524742 False\n",
      "231 -0.85993516 -21.517984 False\n",
      "232 -0.86026275 -21.512156 False\n",
      "233 -0.85942435 -21.505653 False\n",
      "234 -0.8594488 -21.49984 False\n",
      "235 -0.8484506 -21.493711 False\n",
      "236 -0.84890896 -21.498722 False\n",
      "237 -0.8509388 -21.503494 False\n",
      "238 -0.8511013 -21.506363 False\n",
      "239 -0.85236454 -21.509209 False\n",
      "240 -0.8539581 -21.510817 False\n",
      "241 -0.85709447 -21.510822 False\n",
      "242 -0.85956115 -21.507595 False\n",
      "243 -0.8509739 -21.50168 False\n",
      "244 -0.85590637 -21.504478 False\n",
      "245 -0.8580206 -21.502226 False\n",
      "246 -0.8617463 -21.497717 False\n",
      "247 -0.8635688 -21.489084 False\n",
      "248 -0.86485434 -21.478256 False\n",
      "249 -0.8674602 -21.465662 False\n",
      "250 -0.8702177 -21.449764 False\n",
      "251 -0.87391156 -21.43034 False\n",
      "252 -0.87630224 -21.406265 False\n",
      "253 -0.86638236 -21.378727 False\n",
      "254 -0.86834073 -21.360394 False\n",
      "255 -0.87065876 -21.33929 False\n",
      "256 -0.87270916 -21.314888 False\n",
      "257 -0.87508684 -21.28736 False\n",
      "258 -0.87840265 -21.256222 False\n",
      "259 -0.88430345 -21.220272 False\n",
      "260 -0.88931185 -21.176662 False\n",
      "261 -0.8717333 -21.126053 False\n",
      "262 -0.8755939 -21.091732 False\n",
      "263 -0.8790983 -21.051855 False\n",
      "264 -0.8824948 -21.006737 False\n",
      "265 -0.88781554 -20.95623 False\n",
      "266 -0.89115316 -20.898094 False\n",
      "267 -0.89405036 -20.834112 False\n",
      "268 -0.89883566 -20.76445 False\n",
      "269 -0.90125215 -20.686903 False\n",
      "270 -0.90475315 -20.603687 False\n",
      "271 -0.9070437 -20.513329 False\n",
      "272 -0.9078942 -20.41688 False\n",
      "273 -0.911102 -20.315582 False\n",
      "274 -0.91309875 -20.206766 False\n",
      "275 -0.91549826 -20.091368 False\n",
      "276 -0.9167716 -19.968603 False\n",
      "277 -0.9184903 -19.839472 False\n",
      "278 -0.919115 -19.703278 False\n",
      "279 -0.92076206 -19.560728 False\n",
      "280 -0.9219499 -19.41054 False\n",
      "281 -0.91011596 -19.252954 False\n",
      "282 -0.911341 -19.10122 False\n",
      "283 -0.9118302 -18.941933 False\n",
      "284 -0.9115071 -18.775522 False\n",
      "285 -0.9112032 -18.602573 False\n",
      "286 -0.9106105 -18.422783 False\n",
      "287 -0.9124939 -18.23619 False\n",
      "288 -0.91368794 -18.039831 False\n",
      "289 -0.9138919 -17.834211 False\n",
      "290 -0.9147 -17.619833 False\n",
      "291 -0.9147249 -17.395784 False\n",
      "292 -0.915217 -17.162403 False\n",
      "293 -0.91454935 -16.918852 False\n",
      "294 -0.91546315 -16.66599 False\n",
      "295 -0.9154505 -16.40165 False\n",
      "296 -0.9166361 -16.127949 False\n",
      "297 -0.9157572 -15.840134 False\n",
      "298 -0.916806 -15.541387 False\n",
      "299 -0.9171212 -15.229161 False\n",
      "300 -0.9167309 -14.903763 False\n",
      "301 -0.91566277 -14.565297 False\n",
      "302 -0.91705585 -14.213957 False\n",
      "303 -0.9177964 -13.846516 False\n",
      "304 -0.9172328 -13.463221 False\n",
      "305 -0.9157988 -13.064682 False\n",
      "306 -0.9159114 -12.651183 False\n",
      "307 -0.93414557 -12.220375 False\n",
      "308 -0.9327566 -11.751536 False\n",
      "309 -0.9348891 -11.266112 False\n",
      "310 -0.93464005 -10.758289 False\n",
      "311 -0.9354106 -10.229789 False\n",
      "312 -0.93626124 -9.678665 False\n",
      "313 -0.93614334 -9.103823 False\n",
      "314 -0.93801737 -8.505405 False\n",
      "315 -0.93844473 -7.880276 False\n",
      "316 -0.9378143 -7.2288237 False\n",
      "317 -0.9379817 -6.5511518 False\n",
      "318 -0.9387457 -5.845301 False\n",
      "319 -0.940151 -5.109394 False\n",
      "320 -0.93920875 -4.341656 False\n",
      "321 -0.9392756 -3.5431874 False\n",
      "322 -0.939762 -2.7115986 False\n",
      "323 -0.9419722 -1.8450589 False\n",
      "324 -0.94327205 -0.9404276 True\n",
      "100 -0.6333765 -18.280096 False\n",
      "101 -0.63606596 -18.376144 False\n",
      "102 -0.6352114 -18.473404 False\n",
      "103 -0.6365751 -18.576872 False\n",
      "104 -0.63839877 -18.681868 False\n",
      "105 -0.6410927 -18.789268 False\n",
      "106 -0.6455265 -18.898184 False\n",
      "107 -0.65703934 -19.007019 False\n",
      "108 -0.658573 -19.108572 False\n",
      "109 -0.6617328 -19.212646 False\n",
      "110 -0.664001 -19.317732 False\n",
      "111 -0.6780474 -19.424877 False\n",
      "112 -0.68264073 -19.521774 False\n",
      "113 -0.6847072 -19.617878 False\n",
      "114 -0.6876112 -19.7159 False\n",
      "115 -0.6897008 -19.814976 False\n",
      "116 -0.7259685 -19.915813 False\n",
      "117 -0.7269529 -19.983196 False\n",
      "118 -0.7279562 -20.052177 False\n",
      "119 -0.7274213 -20.123074 False\n",
      "120 -0.74273753 -20.19748 False\n",
      "121 -0.7475499 -20.259008 False\n",
      "122 -0.74694234 -20.318106 False\n",
      "123 -0.74003094 -20.380209 False\n",
      "124 -0.7547501 -20.452003 False\n",
      "125 -0.75616866 -20.511467 False\n",
      "126 -0.7573259 -20.571936 False\n",
      "127 -0.7566428 -20.633728 False\n",
      "128 -0.77220654 -20.698751 False\n",
      "129 -0.7751359 -20.750282 False\n",
      "130 -0.77697456 -20.800978 False\n",
      "131 -0.7763789 -20.851814 False\n",
      "132 -0.79626703 -20.905102 False\n",
      "133 -0.7986873 -20.940155 False\n",
      "134 -0.7991361 -20.974005 False\n",
      "135 -0.7998975 -21.0088 False\n",
      "136 -0.8226587 -21.044308 False\n",
      "137 -0.8225454 -21.057497 False\n",
      "138 -0.8236475 -21.071444 False\n",
      "139 -0.824542 -21.084856 False\n",
      "140 -0.8468969 -21.097897 False\n",
      "141 -0.84540844 -21.088266 False\n",
      "142 -0.84399563 -21.079775 False\n",
      "143 -0.84236383 -21.072397 False\n",
      "144 -0.83898664 -21.066347 False\n",
      "145 -0.86118954 -21.063639 False\n",
      "146 -0.8584884 -21.037664 False\n",
      "147 -0.8576214 -21.013397 False\n",
      "148 -0.8524405 -20.988873 False\n",
      "149 -0.8495736 -20.968954 False\n",
      "150 -0.8483878 -20.951166 False\n",
      "151 -0.84612334 -20.933743 False\n",
      "152 -0.8418719 -20.91808 False\n",
      "153 -0.83650416 -20.906164 False\n",
      "154 -0.8343151 -20.899405 False\n",
      "155 -0.8314994 -20.89452 False\n",
      "156 -0.8291243 -20.89245 False\n",
      "157 -0.82552016 -20.892773 False\n",
      "158 -0.8092051 -20.896843 False\n",
      "159 -0.8111697 -20.918194 False\n",
      "160 -0.80935854 -20.93821 False\n",
      "161 -0.8078313 -20.961008 False\n",
      "162 -0.8081249 -20.986418 False\n",
      "163 -0.80681545 -21.0124 False\n",
      "164 -0.80547595 -21.04089 False\n",
      "165 -0.8063258 -21.071957 False\n",
      "166 -0.80841136 -21.103556 False\n",
      "167 -0.8298056 -21.13413 False\n",
      "168 -0.8321616 -21.143827 False\n",
      "169 -0.8336248 -21.151392 False\n",
      "170 -0.83452904 -21.157806 False\n",
      "171 -0.82452476 -21.163443 False\n",
      "172 -0.8268365 -21.179806 False\n",
      "173 -0.82802105 -21.194359 False\n",
      "174 -0.828893 -21.208397 False\n",
      "175 -0.8307227 -21.222023 False\n",
      "176 -0.8299254 -21.234268 False\n",
      "177 -0.829575 -21.247902 False\n",
      "178 -0.83010644 -21.262457 False\n",
      "179 -0.83011204 -21.2771 False\n",
      "180 -0.83142596 -21.292374 False\n",
      "181 -0.8305436 -21.306831 False\n",
      "182 -0.83308786 -21.322845 False\n",
      "183 -0.83334106 -21.336884 False\n",
      "184 -0.83544326 -21.351267 False\n",
      "185 -0.83569676 -21.36398 False\n",
      "186 -0.83697015 -21.376886 False\n",
      "187 -0.83844787 -21.38898 False\n",
      "188 -0.8379971 -21.400135 False\n",
      "189 -0.8393952 -21.412182 False\n",
      "190 -0.8399906 -21.423304 False\n",
      "191 -0.84238076 -21.434275 False\n",
      "192 -0.8432661 -21.443235 False\n",
      "193 -0.84546214 -21.451551 False\n",
      "194 -0.84837115 -21.458008 False\n",
      "195 -0.83382195 -21.461617 False\n",
      "196 -0.83731246 -21.480602 False\n",
      "197 -0.83927906 -21.49652 False\n",
      "198 -0.8405066 -21.511242 False\n",
      "199 -0.8408216 -21.525322 False\n",
      "200 -0.84568137 -21.53964 False\n",
      "201 -0.84817487 -21.54941 False\n",
      "202 -0.84996945 -21.557127 False\n",
      "203 -0.83862144 -21.563223 False\n",
      "204 -0.8408708 -21.58133 False\n",
      "205 -0.84262216 -21.597921 False\n",
      "206 -0.84682506 -21.613249 False\n",
      "207 -0.8475389 -21.624952 False\n",
      "208 -0.8501069 -21.636415 False\n",
      "209 -0.8507569 -21.64567 False\n",
      "210 -0.8521878 -21.654648 False\n",
      "211 -0.8562631 -21.662416 False\n",
      "212 -0.8578906 -21.666302 False\n",
      "213 -0.8594743 -21.668716 False\n",
      "214 -0.8613165 -21.669521 False\n",
      "215 -0.86126304 -21.668386 False\n",
      "216 -0.8487048 -21.66736 False\n",
      "217 -0.87223077 -21.679306 False\n",
      "218 -0.87338525 -21.667301 False\n",
      "219 -0.87324744 -21.653515 False\n",
      "220 -0.87409276 -21.639395 False\n",
      "221 -0.87551636 -21.623676 False\n",
      "222 -0.874714 -21.60579 False\n",
      "223 -0.87573266 -21.588118 False\n",
      "224 -0.8752637 -21.568638 False\n",
      "225 -0.86032945 -21.548868 False\n",
      "226 -0.8587633 -21.54387 False\n",
      "227 -0.8582422 -21.540285 False\n",
      "228 -0.8604619 -21.53697 False\n",
      "229 -0.8598449 -21.5313 False\n",
      "230 -0.861053 -21.526064 False\n",
      "231 -0.85993516 -21.51937 False\n",
      "232 -0.86026275 -21.5135 False\n",
      "233 -0.85942435 -21.507135 False\n",
      "234 -0.8594488 -21.501278 False\n",
      "235 -0.8484506 -21.49519 False\n",
      "236 -0.84890896 -21.500286 False\n",
      "237 -0.8509388 -21.505167 False\n",
      "238 -0.8511013 -21.507986 False\n",
      "239 -0.85236454 -21.510857 False\n",
      "240 -0.8539581 -21.512548 False\n",
      "241 -0.85709447 -21.512665 False\n",
      "242 -0.85956115 -21.50932 False\n",
      "243 -0.8509739 -21.503323 False\n",
      "244 -0.85590637 -21.505915 False\n",
      "245 -0.8580206 -21.503626 False\n",
      "246 -0.8617463 -21.499084 False\n",
      "247 -0.8635688 -21.49047 False\n",
      "248 -0.86485434 -21.479624 False\n",
      "249 -0.8674602 -21.467003 False\n",
      "250 -0.8702177 -21.451185 False\n",
      "251 -0.87391156 -21.431835 False\n",
      "252 -0.87630224 -21.407763 False\n",
      "253 -0.86638236 -21.380255 False\n",
      "254 -0.86834073 -21.361917 False\n",
      "255 -0.87065876 -21.340752 False\n",
      "256 -0.87270916 -21.3163 False\n",
      "257 -0.87508684 -21.288689 False\n",
      "258 -0.87840265 -21.257462 False\n",
      "259 -0.88430345 -21.221544 False\n",
      "260 -0.88931185 -21.178022 False\n",
      "261 -0.8717333 -21.127386 False\n",
      "262 -0.8755939 -21.092976 False\n",
      "263 -0.8790983 -21.053215 False\n",
      "264 -0.8824948 -21.008177 False\n",
      "265 -0.88781554 -20.95768 False\n",
      "266 -0.89115316 -20.899527 False\n",
      "267 -0.89405036 -20.8355 False\n",
      "268 -0.89883566 -20.765882 False\n",
      "269 -0.90125215 -20.688316 False\n",
      "270 -0.90475315 -20.605042 False\n",
      "271 -0.9070437 -20.514742 False\n",
      "272 -0.9078942 -20.41827 False\n",
      "273 -0.911102 -20.316982 False\n",
      "274 -0.91309875 -20.20812 False\n",
      "275 -0.91549826 -20.092628 False\n",
      "276 -0.9167716 -19.96995 False\n",
      "277 -0.9184903 -19.840843 False\n",
      "278 -0.919115 -19.704592 False\n",
      "279 -0.92076206 -19.56213 False\n",
      "280 -0.9219499 -19.41202 False\n",
      "281 -0.91011596 -19.254463 False\n",
      "282 -0.911341 -19.102692 False\n",
      "283 -0.9118302 -18.943403 False\n",
      "284 -0.9115071 -18.777067 False\n",
      "285 -0.9112032 -18.604193 False\n",
      "286 -0.9106105 -18.42447 False\n",
      "287 -0.9124939 -18.23792 False\n",
      "288 -0.91368794 -18.041689 False\n",
      "289 -0.9138919 -17.836136 False\n",
      "290 -0.9147 -17.621864 False\n",
      "291 -0.9147249 -17.397762 False\n",
      "292 -0.915217 -17.16454 False\n",
      "293 -0.91454935 -16.921124 False\n",
      "294 -0.91546315 -16.668283 False\n",
      "295 -0.9154505 -16.40408 False\n",
      "296 -0.9166361 -16.128822 False\n",
      "297 -0.9157572 -15.841062 False\n",
      "298 -0.916806 -15.542378 False\n",
      "299 -0.9171212 -15.230108 False\n",
      "300 -0.9167309 -14.904557 False\n",
      "301 -0.91566277 -14.566077 False\n",
      "302 -0.91705585 -14.214771 False\n",
      "303 -0.9177964 -13.84743 False\n",
      "304 -0.9172328 -13.464075 False\n",
      "305 -0.9157988 -13.065517 False\n",
      "306 -0.9159114 -12.652034 False\n",
      "307 -0.93414557 -12.221335 False\n",
      "308 -0.9327566 -11.7539425 False\n",
      "309 -0.9348891 -11.268563 False\n",
      "310 -0.93464005 -10.760799 False\n",
      "311 -0.9354106 -10.232448 False\n",
      "312 -0.93626124 -9.681287 False\n",
      "313 -0.93614334 -9.106569 False\n",
      "314 -0.93801737 -8.508162 False\n",
      "315 -0.93844473 -7.8831425 False\n",
      "316 -0.9378143 -7.2318635 False\n",
      "317 -0.9379817 -6.554276 False\n",
      "318 -0.9387457 -5.84842 False\n",
      "319 -0.940151 -5.112654 False\n",
      "320 -0.93920875 -4.3450184 False\n",
      "321 -0.9392756 -3.5466044 False\n",
      "322 -0.939762 -2.7150543 False\n",
      "323 -0.9419722 -1.848776 False\n",
      "324 -0.94327205 -0.9441431 True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 -0.85590637 -21.506037 False\n",
      "245 -0.8580206 -21.503834 False\n",
      "246 -0.8617463 -21.499231 False\n",
      "247 -0.8635688 -21.49061 False\n",
      "248 -0.86485434 -21.479729 False\n",
      "249 -0.8674602 -21.467033 False\n",
      "250 -0.8702177 -21.451092 False\n",
      "251 -0.87391156 -21.431618 False\n",
      "252 -0.87630224 -21.407522 False\n",
      "253 -0.86638236 -21.379911 False\n",
      "254 -0.86834073 -21.362856 False\n",
      "255 -0.87065876 -21.341686 False\n",
      "256 -0.87270916 -21.317234 False\n",
      "257 -0.87508684 -21.289639 False\n",
      "258 -0.87840265 -21.258392 False\n",
      "259 -0.88430345 -21.222452 False\n",
      "260 -0.88931185 -21.178835 False\n",
      "261 -0.8717333 -21.12824 False\n",
      "262 -0.8755939 -21.093897 False\n",
      "263 -0.8790983 -21.0541 False\n",
      "264 -0.8824948 -21.008865 False\n",
      "265 -0.88781554 -20.958344 False\n",
      "266 -0.89115316 -20.900242 False\n",
      "267 -0.89405036 -20.836185 False\n",
      "268 -0.89883566 -20.766449 False\n",
      "269 -0.90125215 -20.688831 False\n",
      "270 -0.90475315 -20.605595 False\n",
      "271 -0.9070437 -20.515165 False\n",
      "272 -0.9078942 -20.418724 False\n",
      "273 -0.911102 -20.317375 False\n",
      "274 -0.91309875 -20.208427 False\n",
      "275 -0.91549826 -20.092907 False\n",
      "276 -0.9167716 -19.970173 False\n",
      "277 -0.9184903 -19.841087 False\n",
      "278 -0.919115 -19.70486 False\n",
      "279 -0.92076206 -19.562332 False\n",
      "280 -0.9219499 -19.412209 False\n",
      "281 -0.91011596 -19.254545 False\n",
      "282 -0.911341 -19.102772 False\n",
      "283 -0.9118302 -18.943472 False\n",
      "284 -0.9115071 -18.77694 False\n",
      "285 -0.9112032 -18.603964 False\n",
      "286 -0.9106105 -18.424206 False\n",
      "287 -0.9124939 -18.237629 False\n",
      "288 -0.91368794 -18.041353 False\n",
      "289 -0.9138919 -17.83569 False\n",
      "290 -0.9147 -17.621342 False\n",
      "291 -0.9147249 -17.39734 False\n",
      "292 -0.915217 -17.164003 False\n",
      "293 -0.91454935 -16.920525 False\n",
      "294 -0.91546315 -16.667728 False\n",
      "295 -0.9154505 -16.403496 False\n",
      "296 -0.9166361 -16.128359 False\n",
      "297 -0.9157572 -15.84053 False\n",
      "298 -0.916806 -15.541799 False\n",
      "299 -0.9171212 -15.229649 False\n",
      "300 -0.9167309 -14.904289 False\n",
      "301 -0.91566277 -14.565843 False\n",
      "302 -0.91705585 -14.214488 False\n",
      "303 -0.9177964 -13.847176 False\n",
      "304 -0.9172328 -13.463844 False\n",
      "305 -0.9157988 -13.065309 False\n",
      "306 -0.9159114 -12.651766 False\n",
      "307 -0.93414557 -12.221024 False\n",
      "308 -0.9327566 -11.753479 False\n",
      "309 -0.9348891 -11.268058 False\n",
      "310 -0.93464005 -10.760405 False\n",
      "311 -0.9354106 -10.231925 False\n",
      "312 -0.93626124 -9.680874 False\n",
      "313 -0.93614334 -9.106127 False\n",
      "314 -0.93801737 -8.507771 False\n",
      "315 -0.93844473 -7.882654 False\n",
      "316 -0.9378143 -7.2312713 False\n",
      "317 -0.9379817 -6.5536513 False\n",
      "318 -0.9387457 -5.8478374 False\n",
      "319 -0.940151 -5.112089 False\n",
      "320 -0.93920875 -4.3444095 False\n",
      "321 -0.9392756 -3.5459836 False\n",
      "322 -0.939762 -2.7145314 False\n",
      "323 -0.9419722 -1.8480633 False\n",
      "324 -0.94327205 -0.9435526 True\n",
      "0 -0.3195464 -4.6767726 False\n",
      "1 -0.3143554 -4.53727 False\n",
      "2 -0.3090544 -4.397398 False\n",
      "3 -0.3025598 -4.257319 False\n",
      "4 -0.29873767 -4.118211 False\n",
      "5 -0.31175473 -3.9772959 False\n",
      "6 -0.30784345 -3.8170362 False\n",
      "7 -0.30226755 -3.6541507 False\n",
      "8 -0.29806593 -3.4903803 False\n",
      "9 -0.31131303 -3.3242462 False\n",
      "10 -0.30722475 -3.1374257 False\n",
      "11 -0.30398127 -2.9471564 False\n",
      "12 -0.3007452 -2.7516782 False\n",
      "13 -0.29606813 -2.5522292 False\n",
      "14 -0.3094606 -2.3494391 False\n",
      "15 -0.3090189 -2.1242256 False\n",
      "16 -0.30747464 -1.8870727 False\n",
      "17 -0.3030724 -1.6456662 False\n",
      "18 -0.29966152 -1.398105 False\n",
      "19 -0.29643458 -1.1437995 False\n",
      "20 -0.2973664 -0.88228613 False\n",
      "21 -0.3100185 -0.60908556 False\n",
      "22 -0.3100185 -0.31145498 True\n",
      "0 -0.3195464 -8.453376 False\n",
      "1 -0.3143554 -8.470017 False\n",
      "2 -0.3090544 -8.492787 False\n",
      "3 -0.3025598 -8.521998 False\n",
      "4 -0.29873767 -8.559289 False\n",
      "5 -0.31175473 -8.601852 False\n",
      "6 -0.30784345 -8.63279 False\n",
      "7 -0.30226755 -8.6690235 False\n",
      "8 -0.29806593 -8.712702 False\n",
      "9 -0.31131303 -8.762408 False\n",
      "10 -0.30722475 -8.800483 False\n",
      "11 -0.30398127 -8.8443575 False\n",
      "12 -0.3007452 -8.8938465 False\n",
      "13 -0.29606813 -8.948207 False\n",
      "14 -0.3094606 -9.009805 False\n",
      "15 -0.3090189 -9.059972 False\n",
      "16 -0.30747464 -9.112245 False\n",
      "17 -0.3030724 -9.168117 False\n",
      "18 -0.29966152 -9.231507 False\n",
      "19 -0.29643458 -9.30113 False\n",
      "20 -0.2973664 -9.376971 False\n",
      "21 -0.3100185 -9.454804 False\n",
      "22 -0.31241366 -9.522806 False\n",
      "23 -0.3108471 -9.591137 False\n",
      "24 -0.30753005 -9.663968 False\n",
      "25 -0.32129496 -9.743221 False\n",
      "26 -0.3303696 -9.8114195 False\n",
      "27 -0.33612186 -9.872875 False\n",
      "28 -0.34009418 -9.930937 False\n",
      "29 -0.34260365 -9.987241 False\n",
      "30 -0.34223947 -10.043255 False\n",
      "31 -0.35864726 -10.102072 False\n",
      "32 -0.3592176 -10.146247 False\n",
      "33 -0.36033463 -10.191623 False\n",
      "34 -0.36061186 -10.23774 False\n",
      "35 -0.35972962 -10.285415 False\n",
      "36 -0.3607951 -10.336017 False\n",
      "37 -0.37151015 -10.387521 False\n",
      "38 -0.37855414 -10.430091 False\n",
      "39 -0.37879202 -10.467069 False\n",
      "40 -0.3785995 -10.505358 False\n",
      "41 -0.37733093 -10.545356 False\n",
      "42 -0.38557202 -10.588387 False\n",
      "43 -0.3920253 -10.624634 False\n",
      "44 -0.39203578 -10.655639 False\n",
      "45 -0.39135423 -10.687881 False\n",
      "46 -0.39104488 -10.722179 False\n",
      "47 -0.39746717 -10.758107 False\n",
      "48 -0.40786806 -10.788829 False\n",
      "49 -0.4080929 -10.810038 False\n",
      "50 -0.40968716 -10.831876 False\n",
      "51 -0.41001013 -10.852903 False\n",
      "52 -0.41594133 -10.874492 False\n",
      "53 -0.42254117 -10.890915 False\n",
      "54 -0.42222357 -10.901116 False\n",
      "55 -0.4220111 -10.91208 False\n",
      "56 -0.42946115 -10.923627 False\n",
      "57 -0.42766237 -10.927892 False\n",
      "58 -0.43281746 -10.934352 False\n",
      "59 -0.43310216 -10.935664 False\n",
      "60 -0.44202393 -10.936668 False\n",
      "61 -0.44238618 -10.928342 False\n",
      "62 -0.44451994 -10.919333 False\n",
      "63 -0.4504421 -10.907797 False\n",
      "64 -0.4503623 -10.889616 False\n",
      "65 -0.45796505 -10.870766 False\n",
      "66 -0.45757702 -10.843298 False\n",
      "67 -0.45659548 -10.815027 False\n",
      "68 -0.45587963 -10.786688 False\n",
      "69 -0.4691975 -10.757829 False\n",
      "70 -0.4682296 -10.711629 False\n",
      "71 -0.46777666 -10.666917 False\n",
      "72 -0.46806294 -10.620836 False\n",
      "73 -0.46712956 -10.572389 False\n",
      "74 -0.4728267 -10.523035 False\n",
      "75 -0.47561777 -10.46564 False\n",
      "76 -0.47616622 -10.402872 False\n",
      "77 -0.4763606 -10.33708 False\n",
      "78 -0.47700337 -10.268347 False\n",
      "79 -0.47947857 -10.19612 False\n",
      "80 -0.47899508 -10.11839 False\n",
      "81 -0.487728 -10.03796 False\n",
      "82 -0.48735243 -9.945025 False\n",
      "83 -0.48666212 -9.848649 False\n",
      "84 -0.48951522 -9.749069 False\n",
      "85 -0.4895412 -9.64242 False\n",
      "86 -0.48923227 -9.531298 False\n",
      "87 -0.49029604 -9.415903 False\n",
      "88 -0.49169558 -9.294586 False\n",
      "89 -0.4953892 -9.16673 False\n",
      "90 -0.49725667 -9.029779 False\n",
      "91 -0.4969456 -8.885246 False\n",
      "92 -0.4971596 -8.735132 False\n",
      "93 -0.49795428 -8.5785475 False\n",
      "94 -0.4991962 -8.414716 False\n",
      "95 -0.5025249 -8.242824 False\n",
      "96 -0.50252295 -8.060323 False\n",
      "97 -0.5020445 -7.8702526 False\n",
      "98 -0.50265676 -7.672899 False\n",
      "99 -0.5018924 -7.4666233 False\n",
      "100 -0.5051327 -7.2527056 False\n",
      "101 -0.50613135 -7.0266232 False\n",
      "102 -0.50668246 -6.789852 False\n",
      "103 -0.5079531 -6.5429316 False\n",
      "104 -0.5099129 -6.2844486 False\n",
      "105 -0.5133087 -6.013245 False\n",
      "106 -0.5146088 -5.7272916 False\n",
      "107 -0.5155978 -5.428192 False\n",
      "108 -0.5166486 -5.1157103 False\n",
      "109 -0.5170045 -4.789183 False\n",
      "110 -0.51818323 -4.4488587 False\n",
      "111 -0.522196 -4.093176 False\n",
      "112 -0.5235066 -3.718656 False\n",
      "113 -0.52425724 -3.3272905 False\n",
      "114 -0.5264126 -2.9189816 False\n",
      "115 -0.52922046 -2.491517 False\n",
      "116 -0.54060996 -2.043482 False\n",
      "117 -0.54181975 -1.56498 False\n",
      "118 -0.54408556 -1.065465 False\n",
      "119 -0.54408556 -0.54284555 True\n",
      "15 -0.3090189 -9.2139435 False\n",
      "16 -0.30747464 -9.27305 False\n",
      "17 -0.3030724 -9.336226 False\n",
      "18 -0.29966152 -9.40663 False\n",
      "19 -0.29643458 -9.483413 False\n",
      "20 -0.2973664 -9.566785 False\n",
      "21 -0.3100185 -9.652571 False\n",
      "22 -0.31241366 -9.730309 False\n",
      "23 -0.3108471 -9.807144 False\n",
      "24 -0.30753005 -9.888779 False\n",
      "25 -0.32129496 -9.977408 False\n",
      "26 -0.3303696 -10.055293 False\n",
      "27 -0.33612186 -10.126977 False\n",
      "28 -0.34009418 -10.195536 False\n",
      "29 -0.34260365 -10.262715 False\n",
      "30 -0.34223947 -10.3302 False\n",
      "31 -0.35864726 -10.400912 False\n",
      "32 -0.3592176 -10.457461 False\n",
      "33 -0.36033463 -10.515673 False\n",
      "34 -0.36061186 -10.575062 False\n",
      "35 -0.35972962 -10.636751 False\n",
      "36 -0.3607951 -10.701938 False\n",
      "37 -0.37151015 -10.768663 False\n",
      "38 -0.37855414 -10.826987 False\n",
      "39 -0.37879202 -10.880286 False\n",
      "40 -0.3785995 -10.935578 False\n",
      "41 -0.37733093 -10.993472 False\n",
      "42 -0.38557202 -11.055075 False\n",
      "43 -0.3920253 -11.110638 False\n",
      "44 -0.39203578 -11.161566 False\n",
      "45 -0.39135423 -11.214745 False\n",
      "46 -0.39104488 -11.270861 False\n",
      "47 -0.39746717 -11.329707 False\n",
      "48 -0.40786806 -11.384258 False\n",
      "49 -0.4080929 -11.430073 False\n",
      "50 -0.40968716 -11.477699 False\n",
      "51 -0.41001013 -11.525599 False\n",
      "52 -0.41594133 -11.575224 False\n",
      "53 -0.42254117 -11.620677 False\n",
      "54 -0.42222357 -11.66115 False\n",
      "55 -0.4220111 -11.703556 False\n",
      "56 -0.42946115 -11.748041 False\n",
      "57 -0.42766237 -11.786607 False\n",
      "58 -0.43281746 -11.8285885 False\n",
      "59 -0.43310216 -11.866903 False\n",
      "60 -0.44202393 -11.906451 False\n",
      "61 -0.44238618 -11.938434 False\n",
      "62 -0.44451994 -11.971363 False\n",
      "63 -0.4504421 -12.003172 False\n",
      "64 -0.4503623 -12.03033 False\n",
      "65 -0.45796505 -12.058761 False\n",
      "66 -0.45757702 -12.08042 False\n",
      "67 -0.45659548 -12.103386 False\n",
      "68 -0.45587963 -12.128246 False\n",
      "69 -0.4691975 -12.154984 False\n",
      "70 -0.4682296 -12.16854 False\n",
      "71 -0.46777666 -12.183999 False\n",
      "72 -0.46806294 -12.200464 False\n",
      "73 -0.46712956 -12.21754 False\n",
      "74 -0.4728267 -12.236177 False\n",
      "75 -0.47561777 -12.249557 False\n",
      "76 -0.47616622 -12.260674 False\n",
      "77 -0.4763606 -12.271702 False\n",
      "78 -0.47700337 -12.283003 False\n",
      "79 -0.47947857 -12.294019 False\n",
      "80 -0.47899508 -12.30298 False\n",
      "81 -0.487728 -12.312742 False\n",
      "82 -0.48735243 -12.313863 False\n",
      "83 -0.48666212 -12.315371 False\n",
      "84 -0.48951522 -12.317581 False\n",
      "85 -0.4895412 -12.316999 False\n",
      "86 -0.48923227 -12.316444 False\n",
      "87 -0.49029604 -12.316092 False\n",
      "88 -0.49169558 -12.314625 False\n",
      "89 -0.4953892 -12.311615 False\n",
      "90 -0.49725667 -12.304714 False\n",
      "91 -0.4969456 -12.29555 False\n",
      "92 -0.4971596 -12.286385 False\n",
      "93 -0.49795428 -12.276541 False\n",
      "94 -0.4991962 -12.265415 False\n",
      "95 -0.5025249 -12.252477 False\n",
      "96 -0.50252295 -12.235588 False\n",
      "97 -0.5020445 -12.218221 False\n",
      "98 -0.50265676 -12.200471 False\n",
      "99 -0.5018924 -12.181453 False\n",
      "100 -0.5051327 -12.162422 False\n",
      "101 -0.50613135 -12.13907 False\n",
      "102 -0.50668246 -12.113809 False\n",
      "103 -0.5079531 -12.086888 False\n",
      "104 -0.5099129 -12.05767 False\n",
      "105 -0.5133087 -12.02499 False\n",
      "106 -0.5146088 -11.987504 False\n",
      "107 -0.5155978 -11.947148 False\n",
      "108 -0.5166486 -11.904048 False\n",
      "109 -0.5170045 -11.858207 False\n",
      "110 -0.51818323 -11.809959 False\n",
      "111 -0.522196 -11.7585335 False\n",
      "112 -0.5235066 -11.700846 False\n",
      "113 -0.52425724 -11.639397 False\n",
      "114 -0.5264126 -11.574591 False\n",
      "115 -0.52922046 -11.504914 False\n",
      "116 -0.54060996 -11.429217 False\n",
      "117 -0.54181975 -11.338751 False\n",
      "118 -0.54408556 -11.243326 False\n",
      "119 -0.5655903 -11.141568 False\n",
      "120 -0.57705915 -11.012078 False\n",
      "121 -0.5825173 -10.866414 False\n",
      "122 -0.58884865 -10.708952 False\n",
      "123 -0.5927184 -10.538367 False\n",
      "124 -0.60051966 -10.356802 False\n",
      "125 -0.60484004 -10.159581 False\n",
      "126 -0.6076511 -9.949741 False\n",
      "127 -0.60792637 -9.728301 False\n",
      "128 -0.616955 -9.497418 False\n",
      "129 -0.62004185 -9.247482 False\n",
      "130 -0.62209046 -8.984136 False\n",
      "131 -0.625537 -8.707727 False\n",
      "132 -0.6325977 -8.416298 False\n",
      "133 -0.63444126 -8.105458 False\n",
      "134 -0.6356436 -7.77989 False\n",
      "135 -0.63403904 -7.4395576 False\n",
      "136 -0.6357047 -7.0868034 False\n",
      "137 -0.6415018 -6.717746 False\n",
      "138 -0.6428693 -6.327469 False\n",
      "139 -0.6445183 -5.9195604 False\n",
      "140 -0.64173496 -5.4930377 False\n",
      "141 -0.6511855 -5.051841 False\n",
      "142 -0.65280527 -4.5825195 False\n",
      "143 -0.6537735 -4.0922055 False\n",
      "144 -0.6551302 -3.5805435 False\n",
      "145 -0.65630996 -3.0463445 False\n",
      "146 -0.6592452 -2.4887493 False\n",
      "147 -0.6588155 -1.9050317 False\n",
      "148 -0.6608212 -1.2976723 False\n",
      "149 -0.6630765 -0.6631501 False\n",
      "70 -0.4682296 -12.166691 False\n",
      "71 -0.46777666 -12.1820545 False\n",
      "72 -0.46806294 -12.19859 False\n",
      "73 -0.46712956 -12.215453 False\n",
      "74 -0.4728267 -12.234005 False\n",
      "75 -0.47561777 -12.248893 False\n",
      "76 -0.47616622 -12.260002 False\n",
      "77 -0.4763606 -12.270924 False\n",
      "78 -0.47700337 -12.282146 False\n",
      "79 -0.47947857 -12.293192 False\n",
      "80 -0.47899508 -12.302082 False\n",
      "81 -0.487728 -12.311838 False\n",
      "82 -0.48735243 -12.312942 False\n",
      "83 -0.48666212 -12.314498 False\n",
      "84 -0.48951522 -12.316856 False\n",
      "85 -0.4895412 -12.316166 False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 -0.48923227 -12.315406 False\n",
      "87 -0.49029604 -12.315093 False\n",
      "88 -0.49169558 -12.313591 False\n",
      "89 -0.4953892 -12.3107195 False\n",
      "90 -0.49725667 -12.303767 False\n",
      "91 -0.4969456 -12.294511 False\n",
      "92 -0.4971596 -12.285252 False\n",
      "93 -0.49795428 -12.275498 False\n",
      "94 -0.4991962 -12.264403 False\n",
      "95 -0.5025249 -12.251591 False\n",
      "96 -0.50252295 -12.234831 False\n",
      "97 -0.5020445 -12.217384 False\n",
      "98 -0.50265676 -12.199721 False\n",
      "99 -0.5018924 -12.180672 False\n",
      "100 -0.5051327 -12.161638 False\n",
      "101 -0.50613135 -12.138301 False\n",
      "102 -0.50668246 -12.113116 False\n",
      "103 -0.5079531 -12.0863285 False\n",
      "104 -0.5099129 -12.056898 False\n",
      "105 -0.5133087 -12.024413 False\n",
      "106 -0.5146088 -11.987015 False\n",
      "107 -0.5155978 -11.946673 False\n",
      "108 -0.5166486 -11.903727 False\n",
      "109 -0.5170045 -11.857773 False\n",
      "110 -0.51818323 -11.809677 False\n",
      "111 -0.522196 -11.758314 False\n",
      "112 -0.5235066 -11.700353 False\n",
      "113 -0.52425724 -11.638831 False\n",
      "114 -0.5264126 -11.57406 False\n",
      "115 -0.52922046 -11.504146 False\n",
      "116 -0.54060996 -11.428601 False\n",
      "117 -0.54181975 -11.338027 False\n",
      "118 -0.54408556 -11.242383 False\n",
      "119 -0.5655903 -11.140517 False\n",
      "120 -0.57705915 -11.01198 False\n",
      "121 -0.5825173 -10.86634 False\n",
      "122 -0.58884865 -10.708883 False\n",
      "123 -0.5927184 -10.538288 False\n",
      "124 -0.60051966 -10.356771 False\n",
      "125 -0.60484004 -10.159537 False\n",
      "126 -0.6076511 -9.949655 False\n",
      "127 -0.60792637 -9.728259 False\n",
      "128 -0.616955 -9.49743 False\n",
      "129 -0.62004185 -9.247424 False\n",
      "130 -0.62209046 -8.984 False\n",
      "131 -0.625537 -8.707568 False\n",
      "132 -0.6325977 -8.416025 False\n",
      "133 -0.63444126 -8.105166 False\n",
      "134 -0.6356436 -7.779543 False\n",
      "135 -0.63403904 -7.4392905 False\n",
      "136 -0.6357047 -7.086676 False\n",
      "137 -0.6415018 -6.717462 False\n",
      "138 -0.6428693 -6.327149 False\n",
      "139 -0.6445183 -5.9192424 False\n",
      "140 -0.64173496 -5.492882 False\n",
      "141 -0.6511855 -5.051728 False\n",
      "142 -0.65280527 -4.582296 False\n",
      "143 -0.6537735 -4.091799 False\n",
      "144 -0.6551302 -3.580223 False\n",
      "145 -0.65630996 -3.0460012 False\n",
      "146 -0.6592452 -2.4885867 False\n",
      "147 -0.6588155 -1.9048594 False\n",
      "148 -0.6608212 -1.297451 False\n",
      "149 -0.6630765 -0.66298693 False\n",
      "122 -0.58884865 -10.708121 False\n",
      "123 -0.5927184 -10.537641 False\n",
      "124 -0.60051966 -10.356013 False\n",
      "125 -0.60484004 -10.158778 False\n",
      "126 -0.6076511 -9.948827 False\n",
      "127 -0.60792637 -9.727288 False\n",
      "128 -0.616955 -9.497883 False\n",
      "129 -0.62004185 -9.248037 False\n",
      "130 -0.62209046 -8.984498 False\n",
      "131 -0.625537 -8.708104 False\n",
      "132 -0.6325977 -8.416567 False\n",
      "133 -0.63444126 -8.1057415 False\n",
      "134 -0.6356436 -7.780049 False\n",
      "135 -0.63403904 -7.439685 False\n",
      "136 -0.6357047 -7.086917 False\n",
      "137 -0.6415018 -6.718017 False\n",
      "138 -0.6428693 -6.327664 False\n",
      "139 -0.6445183 -5.9197483 False\n",
      "140 -0.64173496 -5.493124 False\n",
      "141 -0.6511855 -5.051966 False\n",
      "142 -0.65280527 -4.5826087 False\n",
      "143 -0.6537735 -4.0922427 False\n",
      "144 -0.6551302 -3.5805697 False\n",
      "145 -0.65630996 -3.0463054 False\n",
      "146 -0.6592452 -2.4887629 False\n",
      "147 -0.6588155 -1.9051617 False\n",
      "148 -0.6608212 -1.2978399 False\n",
      "149 -0.6630765 -0.66320384 False\n"
     ]
    }
   ],
   "source": [
    "for t, r, R, d in zip(sample[\"t\"], sample[\"rewards\"], sample[\"advantages\"], sample[\"dones\"]):\n",
    "    print(t, r, R, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:22,497\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:22.499366: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m 2019-05-27 21:18:22,702\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m 2019-05-27 21:18:22.703959: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m 2019-05-27 21:18:22,982\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m 2019-05-27 21:18:22.984504: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:23,838\tINFO policy_evaluator.py:437 -- Generating sample batch of size 375.0\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:25,441\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'flow_1.0': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.513),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m        'flow_1.1': np.ndarray((12,), dtype=float32, min=0.025, max=0.879, mean=0.199)}}\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:25,441\tINFO sampler.py:309 -- Info return from env: {0: {'flow_1.0': {}, 'flow_1.1': {}}}\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:25,442\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.025, max=0.879, mean=0.199)\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:25,442\tINFO sampler.py:411 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.025, max=0.879, mean=0.199)\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:25,443\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m { 'rl': [ { 'data': { 'agent_id': 'flow_1.1',\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'info': {},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'obs': np.ndarray((12,), dtype=float32, min=0.025, max=0.879, mean=0.199),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m             'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m           { 'data': { 'agent_id': 'flow_1.0',\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'info': {},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.513),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m             'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:25,444\tINFO tf_run_builder.py:89 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:25,482\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m { 'rl': ( np.ndarray((2, 1), dtype=float32, min=-0.342, max=-0.205, mean=-0.274),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m           [],\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m           { 'action_prob': np.ndarray((2,), dtype=float32, min=0.377, max=0.392, mean=0.384),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m             'behaviour_logits': np.ndarray((2, 2), dtype=float32, min=-0.004, max=0.005, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m             'vf_preds': np.ndarray((2,), dtype=float32, min=0.001, max=0.004, mean=0.003)})}\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:29,234\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m { 'flow_1.0': { 'data': { 'action_prob': np.ndarray((23,), dtype=float32, min=0.034, max=0.4, mean=0.289),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'actions': np.ndarray((23, 1), dtype=float32, min=-2.066, max=2.213, mean=-0.202),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'advantages': np.ndarray((23,), dtype=float32, min=3.654, max=46.249, mean=29.795),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'agent_index': np.ndarray((23,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'behaviour_logits': np.ndarray((23, 2), dtype=float32, min=-0.005, max=0.006, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'dones': np.ndarray((23,), dtype=bool, min=0.0, max=1.0, mean=0.043),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'eps_id': np.ndarray((23,), dtype=int64, min=414283089.0, max=414283089.0, mean=414283089.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'infos': np.ndarray((23,), dtype=object, head={'outflow': 445.5445544554455, 'mean_vel': 18.949074768428677, 'cost2': 0.0, 'cost1': 0.6584389678280226}),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'new_obs': np.ndarray((23, 12), dtype=float32, min=-0.028, max=1.0, mean=0.541),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'obs': np.ndarray((23, 12), dtype=float32, min=-0.028, max=1.0, mean=0.538),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_actions': np.ndarray((23, 1), dtype=float32, min=-1.92, max=2.213, mean=-0.112),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_rewards': np.ndarray((23,), dtype=float32, min=0.0, max=3.715, mean=2.955),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'rewards': np.ndarray((23,), dtype=float32, min=2.789, max=3.715, mean=3.114),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           't': np.ndarray((23,), dtype=int64, min=0.0, max=22.0, mean=11.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'unroll_id': np.ndarray((23,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'value_targets': np.ndarray((23,), dtype=float32, min=3.656, max=46.254, mean=29.799),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'vf_preds': np.ndarray((23,), dtype=float32, min=0.002, max=0.005, mean=0.004)},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   'flow_1.1': { 'data': { 'action_prob': np.ndarray((134,), dtype=float32, min=0.008, max=0.4, mean=0.282),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'actions': np.ndarray((134, 1), dtype=float32, min=-2.804, max=2.593, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'advantages': np.ndarray((134,), dtype=float32, min=3.691, max=88.268, mean=69.107),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'agent_index': np.ndarray((134,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'behaviour_logits': np.ndarray((134, 2), dtype=float32, min=-0.006, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'dones': np.ndarray((134,), dtype=bool, min=0.0, max=1.0, mean=0.007),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'eps_id': np.ndarray((134,), dtype=int64, min=414283089.0, max=414283089.0, mean=414283089.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'infos': np.ndarray((134,), dtype=object, head={'outflow': 445.5445544554455, 'mean_vel': 18.949074768428677, 'cost2': 0.0, 'cost1': 0.6584389678280226}),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'new_obs': np.ndarray((134, 12), dtype=float32, min=-0.244, max=1.0, mean=0.505),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'obs': np.ndarray((134, 12), dtype=float32, min=-0.244, max=1.0, mean=0.502),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_actions': np.ndarray((134, 1), dtype=float32, min=-2.804, max=2.593, mean=-0.01),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_rewards': np.ndarray((134,), dtype=float32, min=0.0, max=3.815, mean=3.094),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'rewards': np.ndarray((134,), dtype=float32, min=1.179, max=3.815, mean=3.122),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           't': np.ndarray((134,), dtype=int64, min=0.0, max=133.0, mean=66.5),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'unroll_id': np.ndarray((134,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'value_targets': np.ndarray((134,), dtype=float32, min=3.693, max=88.271, mean=69.11),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'vf_preds': np.ndarray((134,), dtype=float32, min=-0.0, max=0.003, mean=0.002)},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   'flow_1.2': { 'data': { 'action_prob': np.ndarray((310,), dtype=float32, min=0.014, max=0.401, mean=0.278),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'actions': np.ndarray((310, 1), dtype=float32, min=-2.579, max=2.542, mean=-0.05),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'advantages': np.ndarray((310,), dtype=float32, min=3.773, max=94.008, mean=62.775),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'agent_index': np.ndarray((310,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'behaviour_logits': np.ndarray((310, 2), dtype=float32, min=-0.005, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'dones': np.ndarray((310,), dtype=bool, min=0.0, max=1.0, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'eps_id': np.ndarray((310,), dtype=int64, min=414283089.0, max=414283089.0, mean=414283089.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'infos': np.ndarray((310,), dtype=object, head={'outflow': 543.103448275862, 'mean_vel': 18.533559329035675, 'cost2': 0.0, 'cost1': 0.6695527472466182}),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'new_obs': np.ndarray((310, 12), dtype=float32, min=-0.244, max=1.0, mean=0.414),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'obs': np.ndarray((310, 12), dtype=float32, min=-0.244, max=1.0, mean=0.413),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_actions': np.ndarray((310, 1), dtype=float32, min=-2.579, max=2.542, mean=-0.052),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_rewards': np.ndarray((310,), dtype=float32, min=1.235, max=3.805, mean=2.601),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'rewards': np.ndarray((310,), dtype=float32, min=1.235, max=3.805, mean=2.607),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           't': np.ndarray((310,), dtype=int64, min=15.0, max=324.0, mean=169.5),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'unroll_id': np.ndarray((310,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'value_targets': np.ndarray((310,), dtype=float32, min=3.775, max=94.012, mean=62.777),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'vf_preds': np.ndarray((310,), dtype=float32, min=-0.0, max=0.004, mean=0.002)},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   'flow_1.3': { 'data': { 'action_prob': np.ndarray((263,), dtype=float32, min=0.01, max=0.4, mean=0.279),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'actions': np.ndarray((263, 1), dtype=float32, min=-2.672, max=2.722, mean=-0.025),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'advantages': np.ndarray((263,), dtype=float32, min=1.461, max=37.636, mean=26.898),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'agent_index': np.ndarray((263,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'behaviour_logits': np.ndarray((263, 2), dtype=float32, min=-0.004, max=0.004, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'dones': np.ndarray((263,), dtype=bool, min=0.0, max=1.0, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'eps_id': np.ndarray((263,), dtype=int64, min=414283089.0, max=414283089.0, mean=414283089.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'infos': np.ndarray((263,), dtype=object, head={'outflow': 883.4355828220858, 'mean_vel': 12.741131067810349, 'cost2': 0.0, 'cost1': 0.48809618741206434}),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'new_obs': np.ndarray((263, 12), dtype=float32, min=-0.146, max=1.0, mean=0.164),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'obs': np.ndarray((263, 12), dtype=float32, min=-0.146, max=1.0, mean=0.165),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_actions': np.ndarray((263, 1), dtype=float32, min=-2.672, max=2.722, mean=-0.026),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_rewards': np.ndarray((263,), dtype=float32, min=0.722, max=1.786, mean=1.191),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'rewards': np.ndarray((263,), dtype=float32, min=0.722, max=1.774, mean=1.189),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           't': np.ndarray((263,), dtype=int64, min=62.0, max=324.0, mean=193.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'unroll_id': np.ndarray((263,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'value_targets': np.ndarray((263,), dtype=float32, min=1.462, max=37.636, mean=26.899),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'vf_preds': np.ndarray((263,), dtype=float32, min=-0.0, max=0.002, mean=0.001)},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   'flow_1.4': { 'data': { 'action_prob': np.ndarray((207,), dtype=float32, min=0.006, max=0.399, mean=0.278),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'actions': np.ndarray((207, 1), dtype=float32, min=-2.117, max=2.912, mean=0.018),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'advantages': np.ndarray((207,), dtype=float32, min=0.839, max=32.481, mean=21.379),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'agent_index': np.ndarray((207,), dtype=int64, min=4.0, max=4.0, mean=4.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'behaviour_logits': np.ndarray((207, 2), dtype=float32, min=-0.003, max=0.004, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'dones': np.ndarray((207,), dtype=bool, min=0.0, max=1.0, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'eps_id': np.ndarray((207,), dtype=int64, min=414283089.0, max=414283089.0, mean=414283089.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'infos': np.ndarray((207,), dtype=object, head={'outflow': 739.7260273972602, 'mean_vel': 8.604745033873435, 'cost2': 0.0, 'cost1': 0.34109533241580603}),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'new_obs': np.ndarray((207, 12), dtype=float32, min=-0.104, max=1.0, mean=0.11),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'obs': np.ndarray((207, 12), dtype=float32, min=-0.104, max=1.0, mean=0.111),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_actions': np.ndarray((207, 1), dtype=float32, min=-2.117, max=2.912, mean=0.02),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_rewards': np.ndarray((207,), dtype=float32, min=0.837, max=1.723, mean=1.004),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'rewards': np.ndarray((207,), dtype=float32, min=0.837, max=1.713, mean=0.999),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           't': np.ndarray((207,), dtype=int64, min=118.0, max=324.0, mean=221.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'unroll_id': np.ndarray((207,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'value_targets': np.ndarray((207,), dtype=float32, min=0.841, max=32.481, mean=21.381),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'vf_preds': np.ndarray((207,), dtype=float32, min=-0.0, max=0.002, mean=0.002)},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   'flow_1.5': { 'data': { 'action_prob': np.ndarray((144,), dtype=float32, min=0.01, max=0.4, mean=0.284),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'actions': np.ndarray((144, 1), dtype=float32, min=-2.711, max=2.378, mean=-0.022),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'advantages': np.ndarray((144,), dtype=float32, min=0.96, max=31.835, mean=21.62),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'agent_index': np.ndarray((144,), dtype=int64, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'behaviour_logits': np.ndarray((144, 2), dtype=float32, min=-0.003, max=0.004, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'dones': np.ndarray((144,), dtype=bool, min=0.0, max=1.0, mean=0.007),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'eps_id': np.ndarray((144,), dtype=int64, min=414283089.0, max=414283089.0, mean=414283089.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'infos': np.ndarray((144,), dtype=object, head={'outflow': 864.0, 'mean_vel': 5.33910011184582, 'cost2': 0.0, 'cost1': 0.20567629744699786}),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'new_obs': np.ndarray((144, 12), dtype=float32, min=-0.233, max=1.0, mean=0.114),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'obs': np.ndarray((144, 12), dtype=float32, min=-0.233, max=1.0, mean=0.116),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_actions': np.ndarray((144, 1), dtype=float32, min=-2.711, max=2.378, mean=-0.01),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_rewards': np.ndarray((144,), dtype=float32, min=0.96, max=1.599, mean=1.075),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'rewards': np.ndarray((144,), dtype=float32, min=0.96, max=1.592, mean=1.071),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           't': np.ndarray((144,), dtype=int64, min=181.0, max=324.0, mean=252.5),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'unroll_id': np.ndarray((144,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'value_targets': np.ndarray((144,), dtype=float32, min=0.962, max=31.835, mean=21.622),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'vf_preds': np.ndarray((144,), dtype=float32, min=-0.0, max=0.002, mean=0.002)},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                 'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:31,288\tINFO policy_evaluator.py:474 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m { 'count': 375,\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   'policy_batches': { 'rl': { 'data': { 'action_prob': np.ndarray((1200,), dtype=float32, min=0.006, max=0.401, mean=0.28),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'actions': np.ndarray((1200, 1), dtype=float32, min=-2.804, max=2.912, mean=-0.02),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'advantages': np.ndarray((1200,), dtype=float32, min=0.839, max=94.008, mean=39.505),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'agent_index': np.ndarray((1200,), dtype=int64, min=0.0, max=5.0, mean=2.57),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'behaviour_logits': np.ndarray((1200, 2), dtype=float32, min=-0.006, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'dones': np.ndarray((1200,), dtype=bool, min=0.0, max=1.0, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'eps_id': np.ndarray((1200,), dtype=int64, min=414283089.0, max=1469289490.0, mean=518904557.099),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'infos': np.ndarray((1200,), dtype=object, head={'outflow': 445.5445544554455, 'mean_vel': 18.949074768428677, 'cost2': 0.0, 'cost1': 0.6584389678280226}),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'new_obs': np.ndarray((1200, 12), dtype=float32, min=-0.244, max=1.0, mean=0.276),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'obs': np.ndarray((1200, 12), dtype=float32, min=-0.244, max=1.0, mean=0.275),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'prev_actions': np.ndarray((1200, 1), dtype=float32, min=-2.804, max=2.912, mean=-0.018),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'prev_rewards': np.ndarray((1200,), dtype=float32, min=0.0, max=3.815, mean=1.836),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'rewards': np.ndarray((1200,), dtype=float32, min=0.722, max=3.815, mean=1.848),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         't': np.ndarray((1200,), dtype=int64, min=0.0, max=324.0, mean=164.567),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'unroll_id': np.ndarray((1200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'value_targets': np.ndarray((1200,), dtype=float32, min=0.841, max=94.012, mean=39.507),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'vf_preds': np.ndarray((1200,), dtype=float32, min=-0.0, max=0.005, mean=0.002)},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                               'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n"
     ]
    }
   ],
   "source": [
    "samples = agent.sample(agent.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in samples.policy_batches.values():\n",
    "    sample.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m 2019-05-27 21:31:40,532\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m 2019-05-27 21:31:40.534995: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m 2019-05-27 21:31:40,561\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m 2019-05-27 21:31:40.563023: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m 2019-05-27 21:31:40,560\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m 2019-05-27 21:31:40.563335: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m 2019-05-27 21:31:41,898\tINFO policy_evaluator.py:437 -- Generating sample batch of size 375.0\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m 2019-05-27 21:31:43,394\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'flow_1.0': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.513),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m        'flow_1.1': np.ndarray((12,), dtype=float32, min=0.025, max=0.876, mean=0.199)}}\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m 2019-05-27 21:31:43,394\tINFO sampler.py:309 -- Info return from env: {0: {'flow_1.0': {}, 'flow_1.1': {}}}\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m 2019-05-27 21:31:43,395\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.025, max=0.876, mean=0.199)\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m 2019-05-27 21:31:43,395\tINFO sampler.py:411 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.025, max=0.876, mean=0.199)\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m 2019-05-27 21:31:43,396\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m { 'rl': [ { 'data': { 'agent_id': 'flow_1.1',\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                       'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                       'info': {},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                       'obs': np.ndarray((12,), dtype=float32, min=0.025, max=0.876, mean=0.199),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                       'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                       'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                       'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m             'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m           { 'data': { 'agent_id': 'flow_1.0',\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                       'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                       'info': {},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                       'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.513),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                       'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                       'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                       'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m             'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m 2019-05-27 21:31:43,396\tINFO tf_run_builder.py:89 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m 2019-05-27 21:31:43,439\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m { 'rl': ( np.ndarray((2, 1), dtype=float32, min=1.873, max=2.146, mean=2.009),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m           [],\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m           { 'action_prob': np.ndarray((2,), dtype=float32, min=0.04, max=0.067, mean=0.053),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m             'behaviour_logits': np.ndarray((2, 2), dtype=float32, min=-0.01, max=-0.0, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m             'vf_preds': np.ndarray((2,), dtype=float32, min=-0.002, max=-0.001, mean=-0.001)})}\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m 2019-05-27 21:31:46,672\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m { 'flow_1.0': { 'data': { 'action_prob': np.ndarray((23,), dtype=float32, min=0.033, max=0.403, mean=0.27),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'actions': np.ndarray((23, 1), dtype=float32, min=-1.785, max=2.217, mean=0.138),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'advantages': np.ndarray((23,), dtype=float32, min=3.302, max=48.446, mean=29.778),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'agent_index': np.ndarray((23,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'behaviour_logits': np.ndarray((23, 2), dtype=float32, min=-0.011, max=-0.002, mean=-0.006),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'dones': np.ndarray((23,), dtype=bool, min=0.0, max=1.0, mean=0.043),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'eps_id': np.ndarray((23,), dtype=int64, min=819129012.0, max=819129012.0, mean=819129012.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'infos': np.ndarray((23,), dtype=object, head={'outflow': 445.5445544554455, 'mean_vel': 18.941840652017397, 'cost1': 0.6585311941549772, 'cost2': 0.0}),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'new_obs': np.ndarray((23, 12), dtype=float32, min=-0.032, max=1.0, mean=0.542),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'obs': np.ndarray((23, 12), dtype=float32, min=-0.032, max=1.0, mean=0.539),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'prev_actions': np.ndarray((23, 1), dtype=float32, min=-1.785, max=2.217, mean=0.141),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'prev_rewards': np.ndarray((23,), dtype=float32, min=0.0, max=3.655, mean=3.061),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'rewards': np.ndarray((23,), dtype=float32, min=3.01, max=3.655, mean=3.205),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           't': np.ndarray((23,), dtype=int64, min=0.0, max=22.0, mean=11.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'unroll_id': np.ndarray((23,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'value_targets': np.ndarray((23,), dtype=float32, min=3.301, max=48.445, mean=29.777),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'vf_preds': np.ndarray((23,), dtype=float32, min=-0.002, max=-0.001, mean=-0.001)},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m   'flow_1.1': { 'data': { 'action_prob': np.ndarray((123,), dtype=float32, min=0.014, max=0.403, mean=0.275),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'actions': np.ndarray((123, 1), dtype=float32, min=-2.356, max=2.567, mean=0.014),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'advantages': np.ndarray((123,), dtype=float32, min=3.114, max=71.971, mean=56.682),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'agent_index': np.ndarray((123,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'behaviour_logits': np.ndarray((123, 2), dtype=float32, min=-0.01, max=-0.0, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'dones': np.ndarray((123,), dtype=bool, min=0.0, max=1.0, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'eps_id': np.ndarray((123,), dtype=int64, min=819129012.0, max=819129012.0, mean=819129012.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'infos': np.ndarray((123,), dtype=object, head={'outflow': 445.5445544554455, 'mean_vel': 18.941840652017397, 'cost1': 0.6585311941549772, 'cost2': 0.0}),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'new_obs': np.ndarray((123, 12), dtype=float32, min=-0.129, max=1.0, mean=0.465),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'obs': np.ndarray((123, 12), dtype=float32, min=-0.129, max=1.0, mean=0.462),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'prev_actions': np.ndarray((123, 1), dtype=float32, min=-2.356, max=2.567, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'prev_rewards': np.ndarray((123,), dtype=float32, min=0.0, max=3.15, mean=2.572),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'rewards': np.ndarray((123,), dtype=float32, min=0.79, max=3.15, mean=2.598),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           't': np.ndarray((123,), dtype=int64, min=0.0, max=122.0, mean=61.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'unroll_id': np.ndarray((123,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'value_targets': np.ndarray((123,), dtype=float32, min=3.113, max=71.969, mean=56.682),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'vf_preds': np.ndarray((123,), dtype=float32, min=-0.003, max=0.001, mean=-0.0)},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m   'flow_1.2': { 'data': { 'action_prob': np.ndarray((310,), dtype=float32, min=0.001, max=0.403, mean=0.287),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'actions': np.ndarray((310, 1), dtype=float32, min=-3.34, max=2.92, mean=-0.085),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'advantages': np.ndarray((310,), dtype=float32, min=3.035, max=77.092, mean=54.725),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'agent_index': np.ndarray((310,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'behaviour_logits': np.ndarray((310, 2), dtype=float32, min=-0.01, max=-0.0, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'dones': np.ndarray((310,), dtype=bool, min=0.0, max=1.0, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'eps_id': np.ndarray((310,), dtype=int64, min=819129012.0, max=819129012.0, mean=819129012.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'infos': np.ndarray((310,), dtype=object, head={'outflow': 543.103448275862, 'mean_vel': 18.633173271843734, 'cost1': 0.6694310299501279, 'cost2': 0.0}),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'new_obs': np.ndarray((310, 12), dtype=float32, min=-0.129, max=1.0, mean=0.443),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'obs': np.ndarray((310, 12), dtype=float32, min=-0.129, max=1.0, mean=0.442),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'prev_actions': np.ndarray((310, 1), dtype=float32, min=-3.34, max=2.92, mean=-0.081),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'prev_rewards': np.ndarray((310,), dtype=float32, min=0.528, max=3.159, mean=2.236),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'rewards': np.ndarray((310,), dtype=float32, min=0.528, max=3.159, mean=2.241),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           't': np.ndarray((310,), dtype=int64, min=15.0, max=324.0, mean=169.5),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'unroll_id': np.ndarray((310,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'value_targets': np.ndarray((310,), dtype=float32, min=3.034, max=77.092, mean=54.726),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'vf_preds': np.ndarray((310,), dtype=float32, min=-0.001, max=0.002, mean=0.001)},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m   'flow_1.3': { 'data': { 'action_prob': np.ndarray((253,), dtype=float32, min=0.002, max=0.4, mean=0.288),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'actions': np.ndarray((253, 1), dtype=float32, min=-3.294, max=2.909, mean=-0.006),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'advantages': np.ndarray((253,), dtype=float32, min=1.09, max=27.414, mean=19.801),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'agent_index': np.ndarray((253,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'behaviour_logits': np.ndarray((253, 2), dtype=float32, min=-0.006, max=0.0, mean=-0.003),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'dones': np.ndarray((253,), dtype=bool, min=0.0, max=1.0, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'eps_id': np.ndarray((253,), dtype=int64, min=819129012.0, max=819129012.0, mean=819129012.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'infos': np.ndarray((253,), dtype=object, head={'outflow': 676.3005780346821, 'mean_vel': 10.83359506524939, 'cost1': 0.42642957573847134, 'cost2': 0.0}),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'new_obs': np.ndarray((253, 12), dtype=float32, min=-0.083, max=1.0, mean=0.199),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'obs': np.ndarray((253, 12), dtype=float32, min=-0.083, max=1.0, mean=0.199),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'prev_actions': np.ndarray((253, 1), dtype=float32, min=-3.294, max=2.909, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'prev_rewards': np.ndarray((253,), dtype=float32, min=0.469, max=1.469, mean=0.864),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'rewards': np.ndarray((253,), dtype=float32, min=0.469, max=1.469, mean=0.862),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           't': np.ndarray((253,), dtype=int64, min=72.0, max=324.0, mean=198.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'unroll_id': np.ndarray((253,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'value_targets': np.ndarray((253,), dtype=float32, min=1.093, max=27.418, mean=19.805),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'vf_preds': np.ndarray((253,), dtype=float32, min=0.002, max=0.005, mean=0.004)},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m   'flow_1.4': { 'data': { 'action_prob': np.ndarray((194,), dtype=float32, min=0.013, max=0.399, mean=0.284),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'actions': np.ndarray((194, 1), dtype=float32, min=-2.613, max=2.316, mean=0.032),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'advantages': np.ndarray((194,), dtype=float32, min=0.431, max=14.03, mean=8.079),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'agent_index': np.ndarray((194,), dtype=int64, min=4.0, max=4.0, mean=4.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'behaviour_logits': np.ndarray((194, 2), dtype=float32, min=-0.006, max=0.001, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'dones': np.ndarray((194,), dtype=bool, min=0.0, max=1.0, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'eps_id': np.ndarray((194,), dtype=int64, min=819129012.0, max=819129012.0, mean=819129012.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'infos': np.ndarray((194,), dtype=object, head={'outflow': 814.655172413793, 'mean_vel': 4.4771031257690685, 'cost1': 0.16728940619608032, 'cost2': 0.0}),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'new_obs': np.ndarray((194, 12), dtype=float32, min=-0.081, max=1.0, mean=0.109),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'obs': np.ndarray((194, 12), dtype=float32, min=-0.081, max=1.0, mean=0.11),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'prev_actions': np.ndarray((194, 1), dtype=float32, min=-2.613, max=2.316, mean=0.038),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'prev_rewards': np.ndarray((194,), dtype=float32, min=0.319, max=1.28, mean=0.395),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'rewards': np.ndarray((194,), dtype=float32, min=0.319, max=1.272, mean=0.39),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           't': np.ndarray((194,), dtype=int64, min=131.0, max=324.0, mean=227.5),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'unroll_id': np.ndarray((194,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'value_targets': np.ndarray((194,), dtype=float32, min=0.436, max=14.032, mean=8.083),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                           'vf_preds': np.ndarray((194,), dtype=float32, min=0.002, max=0.005, mean=0.004)},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                 'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m 2019-05-27 21:31:48,686\tINFO policy_evaluator.py:474 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m { 'count': 375,\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m   'policy_batches': { 'rl': { 'data': { 'action_prob': np.ndarray((1021,), dtype=float32, min=0.001, max=0.403, mean=0.284),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         'actions': np.ndarray((1021, 1), dtype=float32, min=-3.34, max=2.92, mean=-0.012),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         'advantages': np.ndarray((1021,), dtype=float32, min=0.431, max=77.092, mean=33.514),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         'agent_index': np.ndarray((1021,), dtype=int64, min=0.0, max=4.0, mean=2.234),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         'behaviour_logits': np.ndarray((1021, 2), dtype=float32, min=-0.011, max=0.001, mean=-0.003),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         'dones': np.ndarray((1021,), dtype=bool, min=0.0, max=1.0, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         'eps_id': np.ndarray((1021,), dtype=int64, min=819129012.0, max=1053036638.0, mean=846162410.5),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         'infos': np.ndarray((1021,), dtype=object, head={'outflow': 445.5445544554455, 'mean_vel': 18.941840652017397, 'cost1': 0.6585311941549772, 'cost2': 0.0}),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         'new_obs': np.ndarray((1021, 12), dtype=float32, min=-0.129, max=1.0, mean=0.314),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         'obs': np.ndarray((1021, 12), dtype=float32, min=-0.129, max=1.0, mean=0.313),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         'prev_actions': np.ndarray((1021, 1), dtype=float32, min=-3.34, max=2.92, mean=-0.01),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         'prev_rewards': np.ndarray((1021,), dtype=float32, min=0.0, max=3.655, mean=1.556),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         'rewards': np.ndarray((1021,), dtype=float32, min=0.319, max=3.655, mean=1.568),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         't': np.ndarray((1021,), dtype=int64, min=0.0, max=324.0, mean=154.166),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         'unroll_id': np.ndarray((1021,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         'value_targets': np.ndarray((1021,), dtype=float32, min=0.436, max=77.092, mean=33.516),\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                                         'vf_preds': np.ndarray((1021,), dtype=float32, min=-0.003, max=0.005, mean=0.002)},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m                               'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-27 21:31:54,339\tINFO policy_evaluator.py:564 -- Training on concatenated sample batches:\n",
      "\n",
      "{ 'count': 2250,\n",
      "  'policy_batches': { 'rl': { 'data': { 'action_prob': np.ndarray((6003,), dtype=float32, min=0.0, max=0.403, mean=0.282),\n",
      "                                        'actions': np.ndarray((6003, 1), dtype=float32, min=-3.57, max=3.873, mean=0.001),\n",
      "                                        'advantages': np.ndarray((6003,), dtype=float32, min=0.408, max=77.767, mean=34.04),\n",
      "                                        'agent_index': np.ndarray((6003,), dtype=int64, min=0.0, max=5.0, mean=2.382),\n",
      "                                        'behaviour_logits': np.ndarray((6003, 2), dtype=float32, min=-0.011, max=0.001, mean=-0.003),\n",
      "                                        'dones': np.ndarray((6003,), dtype=bool, min=0.0, max=1.0, mean=0.006),\n",
      "                                        'eps_id': np.ndarray((6003,), dtype=int64, min=196699457.0, max=1946196971.0, mean=1063088589.475),\n",
      "                                        'infos': np.ndarray((6003,), dtype=object, head={'mean_vel': 3.7332574670253966, 'cost2': 0.0, 'outflow': 1152.0, 'cost1': 0.14109418764968784}),\n",
      "                                        'new_obs': np.ndarray((6003, 12), dtype=float32, min=-0.447, max=1.0, mean=0.316),\n",
      "                                        'obs': np.ndarray((6003, 12), dtype=float32, min=-0.447, max=1.0, mean=0.315),\n",
      "                                        'prev_actions': np.ndarray((6003, 1), dtype=float32, min=-3.57, max=3.873, mean=0.002),\n",
      "                                        'prev_rewards': np.ndarray((6003,), dtype=float32, min=0.0, max=3.655, mean=1.59),\n",
      "                                        'rewards': np.ndarray((6003,), dtype=float32, min=0.092, max=3.655, mean=1.599),\n",
      "                                        't': np.ndarray((6003,), dtype=int64, min=0.0, max=324.0, mean=154.113),\n",
      "                                        'unroll_id': np.ndarray((6003,), dtype=int64, min=0.0, max=1.0, mean=0.477),\n",
      "                                        'value_targets': np.ndarray((6003,), dtype=float32, min=0.411, max=77.765, mean=34.041),\n",
      "                                        'vf_preds': np.ndarray((6003,), dtype=float32, min=-0.003, max=0.005, mean=0.001)},\n",
      "                              'type': 'SampleBatch'}},\n",
      "  'type': 'MultiAgentBatch'}\n",
      "\n",
      "2019-05-27 21:31:54,340\tINFO tf_run_builder.py:89 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "2019-05-27 21:31:54,751\tINFO policy_evaluator.py:586 -- Training output:\n",
      "\n",
      "{ 'rl': { 'learner_stats': { 'cur_kl_coeff': 0.2,\n",
      "                             'cur_lr': 0.0005000000237487257,\n",
      "                             'entropy': 1.4166735,\n",
      "                             'kl': -1.0922057e-10,\n",
      "                             'model': {},\n",
      "                             'policy_loss': -34.039692,\n",
      "                             'total_loss': 1663.1292,\n",
      "                             'vf_explained_var': -8.428097e-05,\n",
      "                             'vf_loss': 1697.1686}}}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8261)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=8258)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=8260)\u001b[0m Loading configuration... done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'config': {'batch_mode': 'truncate_episodes',\n",
       "  'callbacks': {'on_episode_end': None,\n",
       "   'on_episode_start': None,\n",
       "   'on_episode_step': None,\n",
       "   'on_postprocess_traj': None,\n",
       "   'on_sample_end': None,\n",
       "   'on_train_result': None},\n",
       "  'clip_actions': False,\n",
       "  'clip_param': 0.3,\n",
       "  'clip_rewards': None,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'compress_observations': False,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'entropy_coeff': 0.0,\n",
       "  'env': 'MultiWaveAttenuationMergePOEnvCustomRew-v0',\n",
       "  'env_config': {'flow_params': '{\\n    \"env\": {\\n        \"additional_params\": {\\n            \"FLOW_RATE\": 2000,\\n            \"FLOW_RATE_MERGE\": 100,\\n            \"RL_PENETRATION\": 0.1,\\n            \"buf_length\": 1,\\n            \"eta1\": 1.0,\\n            \"eta2\": 0.2,\\n            \"eta3\": 0.1,\\n            \"max_accel\": 3,\\n            \"max_decel\": 3,\\n            \"reward_scale\": 1.0,\\n            \"t_min\": 1.0,\\n            \"target_velocity\": 25\\n        },\\n        \"evaluate\": false,\\n        \"horizon\": 750,\\n        \"sims_per_step\": 2,\\n        \"warmup_steps\": 100\\n    },\\n    \"env_name\": \"MultiWaveAttenuationMergePOEnvCustomRew\",\\n    \"exp_tag\": \"multi_merge\",\\n    \"initial\": {\\n        \"additional_params\": {},\\n        \"bunching\": 0,\\n        \"edges_distribution\": \"all\",\\n        \"lanes_distribution\": Infinity,\\n        \"min_gap\": 0,\\n        \"perturbation\": 0.0,\\n        \"shuffle\": false,\\n        \"spacing\": \"uniform\",\\n        \"x0\": 0\\n    },\\n    \"net\": {\\n        \"additional_params\": {\\n            \"highway_lanes\": 1,\\n            \"merge_lanes\": 1,\\n            \"merge_length\": 100,\\n            \"post_merge_length\": 100,\\n            \"pre_merge_length\": 600,\\n            \"speed_limit\": 30\\n        },\\n        \"inflows\": {\\n            \"_InFlows__flows\": [\\n                {\\n                    \"begin\": \"1\",\\n                    \"departLane\": \"free\",\\n                    \"departSpeed\": \"10\",\\n                    \"end\": \"2000000.0\",\\n                    \"name\": \"flow_0\",\\n                    \"route\": \"routeinflow_highway\",\\n                    \"vehsPerHour\": \"1800.0\",\\n                    \"vtype\": \"human\"\\n                },\\n                {\\n                    \"begin\": \"1\",\\n                    \"departLane\": \"free\",\\n                    \"departSpeed\": \"10\",\\n                    \"end\": \"2000000.0\",\\n                    \"name\": \"flow_1\",\\n                    \"route\": \"routeinflow_highway\",\\n                    \"vehsPerHour\": \"200.0\",\\n                    \"vtype\": \"rl\"\\n                },\\n                {\\n                    \"begin\": \"1\",\\n                    \"departLane\": \"free\",\\n                    \"departSpeed\": \"7.5\",\\n                    \"end\": \"2000000.0\",\\n                    \"name\": \"flow_2\",\\n                    \"route\": \"routeinflow_merge\",\\n                    \"vehsPerHour\": \"100\",\\n                    \"vtype\": \"human\"\\n                }\\n            ],\\n            \"num_flows\": 3\\n        },\\n        \"netfile\": null,\\n        \"no_internal_links\": false,\\n        \"osm_path\": null\\n    },\\n    \"scenario\": \"MergeScenario\",\\n    \"sim\": {\\n        \"emission_path\": null,\\n        \"lateral_resolution\": null,\\n        \"no_step_log\": true,\\n        \"num_clients\": 1,\\n        \"overtake_right\": false,\\n        \"port\": null,\\n        \"print_warnings\": true,\\n        \"pxpm\": 2,\\n        \"render\": false,\\n        \"restart_instance\": true,\\n        \"save_render\": false,\\n        \"seed\": null,\\n        \"show_radius\": false,\\n        \"sight_radius\": 25,\\n        \"sim_step\": 0.2,\\n        \"teleport_time\": -1\\n    },\\n    \"simulator\": \"traci\",\\n    \"veh\": [\\n        {\\n            \"acceleration_controller\": [\\n                \"SimCarFollowingController\",\\n                {}\\n            ],\\n            \"car_following_params\": {\\n                \"controller_params\": {\\n                    \"accel\": 1.0,\\n                    \"carFollowModel\": \"IDM\",\\n                    \"decel\": 1.5,\\n                    \"impatience\": 0.5,\\n                    \"maxSpeed\": 30,\\n                    \"minGap\": 2.5,\\n                    \"sigma\": 0.5,\\n                    \"speedDev\": 0.1,\\n                    \"speedFactor\": 1.0,\\n                    \"tau\": 1.0\\n                },\\n                \"speed_mode\": 1\\n            },\\n            \"initial_speed\": 0,\\n            \"lane_change_controller\": [\\n                \"SimLaneChangeController\",\\n                {}\\n            ],\\n            \"lane_change_params\": {\\n                \"controller_params\": {\\n                    \"laneChangeModel\": \"LC2013\",\\n                    \"lcCooperative\": \"1.0\",\\n                    \"lcKeepRight\": \"1.0\",\\n                    \"lcSpeedGain\": \"1.0\",\\n                    \"lcStrategic\": \"1.0\"\\n                },\\n                \"lane_change_mode\": 512\\n            },\\n            \"num_vehicles\": 5,\\n            \"routing_controller\": null,\\n            \"veh_id\": \"human\"\\n        },\\n        {\\n            \"acceleration_controller\": [\\n                \"RLController\",\\n                {}\\n            ],\\n            \"car_following_params\": {\\n                \"controller_params\": {\\n                    \"accel\": 1.0,\\n                    \"carFollowModel\": \"IDM\",\\n                    \"decel\": 1.5,\\n                    \"impatience\": 0.5,\\n                    \"maxSpeed\": 30,\\n                    \"minGap\": 2.5,\\n                    \"sigma\": 0.5,\\n                    \"speedDev\": 0.1,\\n                    \"speedFactor\": 1.0,\\n                    \"tau\": 1.0\\n                },\\n                \"speed_mode\": 1\\n            },\\n            \"initial_speed\": 0,\\n            \"lane_change_controller\": [\\n                \"SimLaneChangeController\",\\n                {}\\n            ],\\n            \"lane_change_params\": {\\n                \"controller_params\": {\\n                    \"laneChangeModel\": \"LC2013\",\\n                    \"lcCooperative\": \"1.0\",\\n                    \"lcKeepRight\": \"1.0\",\\n                    \"lcSpeedGain\": \"1.0\",\\n                    \"lcStrategic\": \"1.0\"\\n                },\\n                \"lane_change_mode\": 512\\n            },\\n            \"num_vehicles\": 0,\\n            \"routing_controller\": null,\\n            \"veh_id\": \"rl\"\\n        }\\n    ]\\n}'},\n",
       "  'expert_path': './expert_sample',\n",
       "  'gamma': 0.99,\n",
       "  'grad_clip': None,\n",
       "  'horizon': 750,\n",
       "  'ignore_worker_failures': False,\n",
       "  'input': 'sampler',\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'kl_coeff': 0.2,\n",
       "  'kl_target': 0.01,\n",
       "  'lambda': 0.97,\n",
       "  'local_evaluator_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
       "   'intra_op_parallelism_threads': 8},\n",
       "  'log_level': 'INFO',\n",
       "  'lr': 0.0005,\n",
       "  'lr_schedule': None,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'model': {'conv_activation': 'relu',\n",
       "   'conv_filters': None,\n",
       "   'custom_model': None,\n",
       "   'custom_options': {},\n",
       "   'custom_preprocessor': None,\n",
       "   'dim': 84,\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_hiddens': [128, 64, 32],\n",
       "   'framestack': True,\n",
       "   'free_log_std': False,\n",
       "   'grayscale': False,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action_reward': False,\n",
       "   'max_seq_len': 20,\n",
       "   'squash_to_range': False,\n",
       "   'use_lstm': False,\n",
       "   'zero_mean': True},\n",
       "  'monitor': False,\n",
       "  'multiagent': {'policies_to_train': ['rl'],\n",
       "   'policy_graphs': {'rl': (ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph,\n",
       "     Box(12,),\n",
       "     Box(1,),\n",
       "     {})},\n",
       "   'policy_mapping_fn': tune.function(<function <lambda> at 0x7f4b85906ea0>)},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'num_gpus': 0,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'num_sgd_iter': 10,\n",
       "  'num_train': 2,\n",
       "  'num_workers': 3,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'optimizer': {},\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'postprocess_inputs': False,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'remote_worker_envs': False,\n",
       "  'sample_async': False,\n",
       "  'sample_batch_size': 375.0,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'simple_optimizer': False,\n",
       "  'soft_horizon': False,\n",
       "  'straggler_mitigation': False,\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'allow_soft_placement': True,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'intra_op_parallelism_threads': 2,\n",
       "   'log_device_placement': False},\n",
       "  'theta_lr': 0.1,\n",
       "  'train_batch_size': 2250,\n",
       "  'use_gae': True,\n",
       "  'vf_clip_param': 1000000.0,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'vf_share_layers': False},\n",
       " 'custom_metrics': {'theta_norm': 0.45812005},\n",
       " 'date': '2019-05-27_21-32-17',\n",
       " 'done': False,\n",
       " 'episode_len_mean': 325.0,\n",
       " 'episode_reward_max': 1484.7070303162793,\n",
       " 'episode_reward_mean': 1425.3082392163735,\n",
       " 'episode_reward_min': 1382.5962092420662,\n",
       " 'episodes_this_iter': 6,\n",
       " 'episodes_total': 6,\n",
       " 'experiment_id': 'cff548d78e0142a7bbce4affc028b6f2',\n",
       " 'hostname': 'kronos',\n",
       " 'iterations_since_restore': 1,\n",
       " 'node_ip': '169.237.32.118',\n",
       " 'num_metric_batches_dropped': 0,\n",
       " 'off_policy_estimator': {},\n",
       " 'pid': 8222,\n",
       " 'policy_reward_mean': {'rl': 237.55137320272888},\n",
       " 'sampler_perf': {'mean_env_wait_ms': 9.650325605717336,\n",
       "  'mean_inference_ms': 1.006301868869918,\n",
       "  'mean_processing_ms': 4.418328950438478},\n",
       " 'time_since_restore': 47.395947217941284,\n",
       " 'time_this_iter_s': 47.395947217941284,\n",
       " 'time_total_s': 47.395947217941284,\n",
       " 'timestamp': 1558992737,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'timesteps_total': None,\n",
       " 'training_iteration': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
