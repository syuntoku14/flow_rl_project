{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* find the varid horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, pickle, argparse, json, logging\n",
    "from gym import ObservationWrapper\n",
    "from copy import deepcopy\n",
    "import ray\n",
    "\n",
    "from gail import GAILTrainer\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import PPOPolicyGraph\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "from ray.rllib.agents import Trainer\n",
    "from ray.rllib.evaluation import PolicyEvaluator, SampleBatch, MultiAgentBatch\n",
    "from ray.rllib.evaluation.metrics import collect_metrics\n",
    "from ray.rllib.offline.json_reader import JsonReader\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.utils import merge_dicts\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.evaluation.postprocessing import discount\n",
    "from ray.rllib.evaluation.sample_batch import DEFAULT_POLICY_ID\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder, get_flow_params\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_cpus = 3\n",
    "num_rollouts = 3\n",
    "horizon = 750\n",
    "gae_lambda = 0.97\n",
    "step_size = 5e-4\n",
    "num_iter = 10\n",
    "benchmark_name = \"multi_merge\"\n",
    "exp_name = \"test_ir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '169.237.32.118',\n",
       " 'object_store_address': '/tmp/ray/session_2019-05-29_06-18-58_17631/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-05-29_06-18-58_17631/sockets/raylet',\n",
       " 'redis_address': '169.237.32.118:36754',\n",
       " 'webui_url': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=num_cpus, logging_level=40, ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = deepcopy(DEFAULT_CONFIG)\n",
    "config[\"num_workers\"] = min(num_cpus, num_rollouts)\n",
    "config[\"train_batch_size\"] = horizon * num_rollouts\n",
    "config[\"sample_batch_size\"] = horizon / 2\n",
    "config[\"use_gae\"] = True\n",
    "config[\"horizon\"] = horizon\n",
    "config[\"lambda\"] = gae_lambda\n",
    "config[\"lr\"] = step_size\n",
    "config[\"vf_clip_param\"] = 1e6\n",
    "config[\"num_sgd_iter\"] = 10\n",
    "config['clip_actions'] = False  # FIXME(ev) temporary ray bug\n",
    "config[\"model\"][\"fcnet_hiddens\"] = [128, 64, 32]\n",
    "config[\"observation_filter\"] = \"NoFilter\"\n",
    "config[\"entropy_coeff\"] = 0.0\n",
    "config[\"expert_path\"] = '/headless/rl_project/flow_codes/ModelBased/expert_sample'\n",
    "config[\"discrim_hidden_size\"] = 128\n",
    "\n",
    "benchmark = __import__(\n",
    "            \"flow.benchmarks.%s\" % benchmark_name, fromlist=[\"flow_params\"])\n",
    "flow_params = benchmark.gail_flow_params\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(\n",
    "    flow_params, cls=FlowParamsEncoder, sort_keys=True, indent=4)\n",
    "config['env_config']['flow_params'] = flow_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_env, env_name = make_create_env(params=flow_params, version=0)\n",
    "register_env(env_name, create_env)\n",
    "env = create_env()\n",
    "\n",
    "POLICY_ID = DEFAULT_POLICY_ID\n",
    "default_policy = (PPOPolicyGraph, env.observation_space, env.action_space, {})\n",
    "policy_graph = {POLICY_ID: default_policy}\n",
    "config[\"multiagent\"] = {\n",
    "        'policy_graphs': policy_graph,\n",
    "        'policy_mapping_fn': tune.function(lambda agent_id: POLICY_ID),\n",
    "        'policies_to_train': [POLICY_ID]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.linear1   = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2   = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3   = nn.Linear(hidden_size, 1)\n",
    "        self.linear3.weight.data.mul_(0.1)\n",
    "        self.linear3.bias.data.mul_(0.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.linear1(x))\n",
    "        x = F.tanh(self.linear2(x))\n",
    "        prob = F.sigmoid(self.linear3(x))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnvPolicyEvaluator(PolicyEvaluator):\n",
    "    def set_state_dict(self, state_dict):\n",
    "        self.env.set_state_dict(state_dict)\n",
    "        \n",
    "    def init_discriminator(self, hidden_size):\n",
    "        self.env.init_discriminator(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAILTrainer(Trainer):\n",
    "    _allow_unknown_configs = True\n",
    "    _name = \"GAIL\"\n",
    "    _default_config = DEFAULT_CONFIG\n",
    "    _policy_graph = PPOPolicyGraph\n",
    "    \n",
    "    @override(Trainer)\n",
    "    def _init(self, config, env_name):\n",
    "        self.train_batch_size = self.config[\"train_batch_size\"]\n",
    "        self.num_sgd_iter = self.config[\"num_sgd_iter\"]\n",
    "        \n",
    "        # load expert trajectory\n",
    "        self.expert_reader = JsonReader(self.config[\"expert_path\"])\n",
    "        self.expert_samples = self.expert_reader.next()\n",
    "               \n",
    "        # set evaluators\n",
    "        self.local_evaluator = self.make_local_evaluator(\n",
    "             env_name, self._policy_graph, self.config)        \n",
    "        self.remote_evaluators = self.make_remote_evaluators(\n",
    "            env_name, self._policy_graph, self.config[\"num_workers\"])\n",
    "       \n",
    "        # discriminator\n",
    "        num_inputs = self.local_evaluator.env.observation_space.shape[0]\n",
    "        num_outputs = self.local_evaluator.env.action_space.shape[0]\n",
    "        self.discrim_criterion = nn.BCELoss()\n",
    "        self.discriminator = Discriminator(num_inputs+num_outputs,\n",
    "                                           config[\"discrim_hidden_size\"])\n",
    "        self.optimizer_discrim = optim.Adam(self.discriminator.parameters(),\n",
    "                                            lr=config[\"lr\"])\n",
    " \n",
    "        # share discriminators\n",
    "        self.local_evaluator.init_discriminator(config[\"discrim_hidden_size\"])\n",
    "        for e in self.remote_evaluators:\n",
    "            e.init_discriminator.remote(config[\"discrim_hidden_size\"])\n",
    "        self.set_state_dict()\n",
    "            \n",
    "    def set_state_dict(self):\n",
    "        state_dict =  self.discriminator.state_dict()\n",
    "        self.local_evaluator.set_state_dict(state_dict)\n",
    "        for e in self.remote_evaluators:\n",
    "            e.set_state_dict.remote(state_dict)  \n",
    "        \n",
    "    def get_state_action_from_samples(self, samples):\n",
    "        state_action = np.hstack((samples[\"obs\"], samples[\"actions\"]))\n",
    "        state_action = torch.FloatTensor(state_action)\n",
    "        return state_action\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        # set local weights to remote\n",
    "        weights = ray.put(self.local_evaluator.get_weights())\n",
    "        for e in self.remote_evaluators:\n",
    "            e.set_weights.remote(weights)\n",
    "            \n",
    "        samples = []\n",
    "        while sum(s.count for s in samples) < sample_size:\n",
    "            samples.extend(\n",
    "                ray.get([\n",
    "                    e.sample.remote() for e in self.remote_evaluators\n",
    "                ]))\n",
    "        samples = SampleBatch.concat_samples(samples)\n",
    "        return samples\n",
    "    \n",
    "    def train_policy_by_samples(self, samples):\n",
    "        # train policy by given samples\n",
    "        for i in range(self.num_sgd_iter):\n",
    "            fetches = self.local_evaluator.learn_on_batch(samples)\n",
    "            \n",
    "        def update(pi, pi_id):\n",
    "            if pi_id in fetches:\n",
    "                pi.update_kl(fetches[pi_id]['learner_stats'][\"kl\"])\n",
    "            else:\n",
    "                logger.debug(\n",
    "                    \"No data for {}, not updating kl\".format(pi_id))\n",
    "        self.local_evaluator.foreach_trainable_policy(update)       \n",
    "   \n",
    "    def train_discriminator_by_state_action(self, state_action, expert_state_action):\n",
    "        fake = self.discriminator(state_action)\n",
    "        real = self.discriminator(expert_state_action)       \n",
    "        self.optimizer_discrim.zero_grad()\n",
    "        # if perfect, fake == 1, real == 0\n",
    "        discrim_loss = self.discrim_criterion(fake, torch.ones((state_action.shape[0], 1)).cpu())\n",
    "        discrim_loss += self.discrim_criterion(real, \n",
    "                       torch.zeros((expert_state_action.size(0), 1)).cpu())        \n",
    "        discrim_loss.backward()\n",
    "        self.optimizer_discrim.step()\n",
    "        \n",
    "        return discrim_loss\n",
    "        \n",
    "    @override(Trainer)    \n",
    "    def _train(self):\n",
    "        samples = self.sample(self.train_batch_size)\n",
    "        samples.shuffle()\n",
    "        self.expert_samples = self.expert_reader.next()\n",
    "        self.expert_samples.shuffle()\n",
    "        state_action = self.get_state_action_from_samples(samples)\n",
    "        expert_state_action = self.get_state_action_from_samples(self.expert_samples)\n",
    "        \n",
    "        self.train_policy_by_samples(samples)\n",
    "        discrim_loss = self.train_discriminator_by_state_action(state_action, expert_state_action)\n",
    "        \n",
    "        res = collect_metrics(self.local_evaluator, self.remote_evaluators)\n",
    "        res[\"custom_metrics\"][\"discrim_loss\"] =  discrim_loss.data.item()\n",
    "        pretty_print(res)\n",
    "        return res\n",
    "\n",
    "    @override(Trainer)\n",
    "    def __getstate__(self):\n",
    "        state = super().__getstate__()\n",
    "        state[\"discrim_state_dict\"] = self.discriminator.state_dict()\n",
    "        return state\n",
    "    \n",
    "    @override(Trainer)\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        self.discriminator.load_state_dict(state[\"discrim_state_dict\"])\n",
    "\n",
    "    def make_local_evaluator(self,\n",
    "                             env_creator,\n",
    "                             policy_graph,\n",
    "                             extra_config=None):\n",
    "        \"\"\"Convenience method to return configured local evaluator.\"\"\"\n",
    "\n",
    "        return self._make_evaluator(\n",
    "            CustomEnvPolicyEvaluator,\n",
    "            env_creator,\n",
    "            policy_graph,\n",
    "            0,\n",
    "            merge_dicts(\n",
    "                # important: allow local tf to use more CPUs for optimization\n",
    "                merge_dicts(\n",
    "                    self.config, {\n",
    "                        \"tf_session_args\": self.\n",
    "                        config[\"local_evaluator_tf_session_args\"]\n",
    "                    }),\n",
    "                extra_config or {}))        \n",
    "    \n",
    "    def make_remote_evaluators(self, env_creator, policy_graph, count):\n",
    "        \"\"\"Convenience method to return a number of remote evaluators.\"\"\"\n",
    "\n",
    "        remote_args = {\n",
    "            \"num_cpus\": self.config[\"num_cpus_per_worker\"],\n",
    "            \"num_gpus\": self.config[\"num_gpus_per_worker\"],\n",
    "            \"resources\": self.config[\"custom_resources_per_worker\"],\n",
    "        }\n",
    "\n",
    "        cls = CustomEnvPolicyEvaluator.as_remote(**remote_args).remote\n",
    "\n",
    "        return [\n",
    "            self._make_evaluator(cls, env_creator, policy_graph, i + 1,\n",
    "                                 self.config) for i in range(count)\n",
    "        ]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-29 06:19:07,832\tWARNING json_reader.py:52 -- Treating input directory as glob pattern: /headless/rl_project/flow_codes/ModelBased/expert_sample/*.json\n",
      "2019-05-29 06:19:07,834\tINFO json_reader.py:65 -- Found 1 input files.\n",
      "2019-05-29 06:19:08,956\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "/opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "2019-05-29 06:19:10,411\tINFO policy_evaluator.py:728 -- Built policy map: {'default_policy': <ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph object at 0x7f0ee4305f98>}\n",
      "2019-05-29 06:19:10,413\tINFO policy_evaluator.py:729 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f0ee4305be0>}\n",
      "2019-05-29 06:19:10,414\tINFO policy_evaluator.py:343 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f0ee43846a0>}\n"
     ]
    }
   ],
   "source": [
    "agent = GAILTrainer(config, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17669)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17669)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=17669)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17668)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17668)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=17668)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m 2019-05-29 06:19:22,148\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m 2019-05-29 06:19:22.150866: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=17669)\u001b[0m 2019-05-29 06:19:22,203\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=17669)\u001b[0m 2019-05-29 06:19:22.205582: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=17668)\u001b[0m 2019-05-29 06:19:22,344\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=17668)\u001b[0m 2019-05-29 06:19:22.346354: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=17669)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=17669)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=17668)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=17668)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m 2019-05-29 06:19:23,464\tINFO policy_evaluator.py:437 -- Generating sample batch of size 375.0\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17669)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17669)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=17669)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17668)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17668)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=17668)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m 2019-05-29 06:19:24,984\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'flow_1.0': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.507),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m        'flow_1.1': np.ndarray((12,), dtype=float32, min=0.013, max=0.824, mean=0.192)}}\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m 2019-05-29 06:19:24,984\tINFO sampler.py:309 -- Info return from env: {0: {'flow_1.0': {}, 'flow_1.1': {}}}\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m 2019-05-29 06:19:24,984\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.507)\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m 2019-05-29 06:19:24,984\tINFO sampler.py:411 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.507)\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m 2019-05-29 06:19:24,986\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'flow_1.0',\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                                   'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.507),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                                   'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                       { 'data': { 'agent_id': 'flow_1.1',\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                                   'obs': np.ndarray((12,), dtype=float32, min=0.013, max=0.824, mean=0.192),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                                   'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m 2019-05-29 06:19:24,986\tINFO tf_run_builder.py:89 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m 2019-05-29 06:19:25,026\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m { 'default_policy': ( np.ndarray((2, 1), dtype=float32, min=0.025, max=1.022, mean=0.523),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                       { 'action_prob': np.ndarray((2,), dtype=float32, min=0.238, max=0.399, mean=0.318),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                         'behaviour_logits': np.ndarray((2, 2), dtype=float32, min=-0.001, max=0.006, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                         'vf_preds': np.ndarray((2,), dtype=float32, min=-0.001, max=0.001, mean=-0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17668)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17668)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=17668)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m 2019-05-29 06:19:49,098\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m { 'flow_1.0': { 'data': { 'action_prob': np.ndarray((26,), dtype=float32, min=0.016, max=0.398, mean=0.314),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'actions': np.ndarray((26, 1), dtype=float32, min=-1.418, max=2.552, mean=0.064),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'advantages': np.ndarray((26,), dtype=float32, min=0.685, max=11.273, mean=6.819),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'agent_index': np.ndarray((26,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'behaviour_logits': np.ndarray((26, 2), dtype=float32, min=0.004, max=0.007, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'dones': np.ndarray((26,), dtype=bool, min=0.0, max=1.0, mean=0.038),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'eps_id': np.ndarray((26,), dtype=int64, min=661138880.0, max=661138880.0, mean=661138880.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'infos': np.ndarray((26,), dtype=object, head={'outflow': 445.5445544554455, 'mean_vel': 18.4120540803279, 'cost2': 0.0, 'cost1': 0.6576125807235955}),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'new_obs': np.ndarray((26, 12), dtype=float32, min=-0.023, max=1.0, mean=0.528),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'obs': np.ndarray((26, 12), dtype=float32, min=-0.023, max=1.0, mean=0.525),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'prev_actions': np.ndarray((26, 1), dtype=float32, min=-1.418, max=2.552, mean=0.064),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'prev_rewards': np.ndarray((26,), dtype=float32, min=0.0, max=0.689, mean=0.661),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'rewards': np.ndarray((26,), dtype=float32, min=0.684, max=0.689, mean=0.687),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           't': np.ndarray((26,), dtype=int64, min=0.0, max=25.0, mean=12.5),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'unroll_id': np.ndarray((26,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'value_targets': np.ndarray((26,), dtype=float32, min=0.685, max=11.272, mean=6.818),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'vf_preds': np.ndarray((26,), dtype=float32, min=-0.001, max=0.0, mean=-0.001)},\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m   'flow_1.1': { 'data': { 'action_prob': np.ndarray((110,), dtype=float32, min=0.006, max=0.399, mean=0.269),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'actions': np.ndarray((110, 1), dtype=float32, min=-2.873, max=2.21, mean=0.031),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'advantages': np.ndarray((110,), dtype=float32, min=0.684, max=17.173, mean=13.543),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'agent_index': np.ndarray((110,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'behaviour_logits': np.ndarray((110, 2), dtype=float32, min=-0.001, max=0.007, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'dones': np.ndarray((110,), dtype=bool, min=0.0, max=1.0, mean=0.009),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'eps_id': np.ndarray((110,), dtype=int64, min=661138880.0, max=661138880.0, mean=661138880.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'infos': np.ndarray((110,), dtype=object, head={'outflow': 445.5445544554455, 'mean_vel': 18.4120540803279, 'cost2': 0.0, 'cost1': 0.6576125807235955}),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'new_obs': np.ndarray((110, 12), dtype=float32, min=-0.05, max=1.0, mean=0.471),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'obs': np.ndarray((110, 12), dtype=float32, min=-0.05, max=1.0, mean=0.467),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'prev_actions': np.ndarray((110, 1), dtype=float32, min=-2.873, max=2.21, mean=0.021),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'prev_rewards': np.ndarray((110,), dtype=float32, min=0.0, max=0.694, mean=0.681),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'rewards': np.ndarray((110,), dtype=float32, min=0.684, max=0.694, mean=0.688),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           't': np.ndarray((110,), dtype=int64, min=0.0, max=109.0, mean=54.5),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'unroll_id': np.ndarray((110,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'value_targets': np.ndarray((110,), dtype=float32, min=0.685, max=17.174, mean=13.543),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'vf_preds': np.ndarray((110,), dtype=float32, min=-0.001, max=0.001, mean=-0.0)},\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m   'flow_1.2': { 'data': { 'action_prob': np.ndarray((310,), dtype=float32, min=0.01, max=0.398, mean=0.283),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'actions': np.ndarray((310, 1), dtype=float32, min=-2.707, max=2.725, mean=0.023),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'advantages': np.ndarray((310,), dtype=float32, min=0.686, max=17.464, mean=15.959),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'agent_index': np.ndarray((310,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'behaviour_logits': np.ndarray((310, 2), dtype=float32, min=-0.005, max=0.007, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'dones': np.ndarray((310,), dtype=bool, min=0.0, max=1.0, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'eps_id': np.ndarray((310,), dtype=int64, min=661138880.0, max=661138880.0, mean=661138880.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'infos': np.ndarray((310,), dtype=object, head={'outflow': 465.5172413793103, 'mean_vel': 18.782991523579046, 'cost2': 0.0, 'cost1': 0.6811872072975181}),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'new_obs': np.ndarray((310, 12), dtype=float32, min=-0.131, max=1.0, mean=0.456),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'obs': np.ndarray((310, 12), dtype=float32, min=-0.131, max=1.0, mean=0.455),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'prev_actions': np.ndarray((310, 1), dtype=float32, min=-2.707, max=2.725, mean=0.026),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'prev_rewards': np.ndarray((310,), dtype=float32, min=0.0, max=0.701, mean=0.685),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'rewards': np.ndarray((310,), dtype=float32, min=0.683, max=0.701, mean=0.688),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           't': np.ndarray((310,), dtype=int64, min=15.0, max=324.0, mean=169.5),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'unroll_id': np.ndarray((310,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'value_targets': np.ndarray((310,), dtype=float32, min=0.687, max=17.465, mean=15.959),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'vf_preds': np.ndarray((310,), dtype=float32, min=-0.002, max=0.001, mean=-0.0)},\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m   'flow_1.3': { 'data': { 'action_prob': np.ndarray((257,), dtype=float32, min=0.027, max=0.399, mean=0.286),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'actions': np.ndarray((257, 1), dtype=float32, min=-2.253, max=2.328, mean=-0.078),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'advantages': np.ndarray((257,), dtype=float32, min=0.692, max=17.479, mean=15.754),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'agent_index': np.ndarray((257,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'behaviour_logits': np.ndarray((257, 2), dtype=float32, min=-0.006, max=0.004, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'dones': np.ndarray((257,), dtype=bool, min=0.0, max=1.0, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'eps_id': np.ndarray((257,), dtype=int64, min=661138880.0, max=661138880.0, mean=661138880.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'infos': np.ndarray((257,), dtype=object, head={'outflow': 905.3254437869821, 'mean_vel': 12.544219538445974, 'cost2': 0.0, 'cost1': 0.47241553819102683}),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'new_obs': np.ndarray((257, 12), dtype=float32, min=-0.131, max=1.0, mean=0.216),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'obs': np.ndarray((257, 12), dtype=float32, min=-0.131, max=1.0, mean=0.216),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'prev_actions': np.ndarray((257, 1), dtype=float32, min=-2.253, max=2.328, mean=-0.084),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'prev_rewards': np.ndarray((257,), dtype=float32, min=0.0, max=0.7, mean=0.688),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'rewards': np.ndarray((257,), dtype=float32, min=0.687, max=0.7, mean=0.691),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           't': np.ndarray((257,), dtype=int64, min=68.0, max=324.0, mean=196.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'unroll_id': np.ndarray((257,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'value_targets': np.ndarray((257,), dtype=float32, min=0.688, max=17.479, mean=15.752),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'vf_preds': np.ndarray((257,), dtype=float32, min=-0.004, max=0.001, mean=-0.002)},\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m   'flow_1.4': { 'data': { 'action_prob': np.ndarray((189,), dtype=float32, min=0.01, max=0.399, mean=0.277),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'actions': np.ndarray((189, 1), dtype=float32, min=-2.717, max=2.375, mean=0.076),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'advantages': np.ndarray((189,), dtype=float32, min=0.69, max=17.627, mean=15.253),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'agent_index': np.ndarray((189,), dtype=int64, min=4.0, max=4.0, mean=4.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'behaviour_logits': np.ndarray((189, 2), dtype=float32, min=-0.006, max=0.002, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'dones': np.ndarray((189,), dtype=bool, min=0.0, max=1.0, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'eps_id': np.ndarray((189,), dtype=int64, min=661138880.0, max=661138880.0, mean=661138880.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'infos': np.ndarray((189,), dtype=object, head={'outflow': 949.3670886075948, 'mean_vel': 5.529186736994415, 'cost2': 0.0, 'cost1': 0.20867129443283886}),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'new_obs': np.ndarray((189, 12), dtype=float32, min=-0.32, max=1.0, mean=0.17),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'obs': np.ndarray((189, 12), dtype=float32, min=-0.32, max=1.0, mean=0.171),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'prev_actions': np.ndarray((189, 1), dtype=float32, min=-2.717, max=2.375, mean=0.069),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'prev_rewards': np.ndarray((189,), dtype=float32, min=0.0, max=0.702, mean=0.691),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'rewards': np.ndarray((189,), dtype=float32, min=0.689, max=0.702, mean=0.695),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           't': np.ndarray((189,), dtype=int64, min=136.0, max=324.0, mean=230.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'unroll_id': np.ndarray((189,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'value_targets': np.ndarray((189,), dtype=float32, min=0.689, max=17.627, mean=15.252),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                           'vf_preds': np.ndarray((189,), dtype=float32, min=-0.001, max=0.0, mean=-0.001)},\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m                 'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m Loading configuration... done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17669)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17669)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=17669)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m 2019-05-29 06:19:51,135\tINFO policy_evaluator.py:474 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m { 'data': { 'action_prob': np.ndarray((1000,), dtype=float32, min=0.004, max=0.399, mean=0.282),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             'actions': np.ndarray((1000, 1), dtype=float32, min=-2.873, max=3.049, mean=0.016),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             'advantages': np.ndarray((1000,), dtype=float32, min=0.684, max=17.627, mean=14.484),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             'agent_index': np.ndarray((1000,), dtype=int64, min=0.0, max=4.0, mean=2.377),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             'behaviour_logits': np.ndarray((1000, 2), dtype=float32, min=-0.006, max=0.007, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             'dones': np.ndarray((1000,), dtype=bool, min=0.0, max=1.0, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             'eps_id': np.ndarray((1000,), dtype=int64, min=661138880.0, max=1433443911.0, mean=744547823.348),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             'infos': np.ndarray((1000,), dtype=object, head={'outflow': 445.5445544554455, 'mean_vel': 18.4120540803279, 'cost2': 0.0, 'cost1': 0.6576125807235955}),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             'new_obs': np.ndarray((1000, 12), dtype=float32, min=-0.32, max=1.0, mean=0.333),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             'obs': np.ndarray((1000, 12), dtype=float32, min=-0.32, max=1.0, mean=0.332),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             'prev_actions': np.ndarray((1000, 1), dtype=float32, min=-2.873, max=3.049, mean=0.013),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             'prev_rewards': np.ndarray((1000,), dtype=float32, min=0.0, max=0.702, mean=0.685),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             'rewards': np.ndarray((1000,), dtype=float32, min=0.683, max=0.702, mean=0.69),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             't': np.ndarray((1000,), dtype=int64, min=0.0, max=324.0, mean=155.305),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             'unroll_id': np.ndarray((1000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             'value_targets': np.ndarray((1000,), dtype=float32, min=0.685, max=17.627, mean=14.483),\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m             'vf_preds': np.ndarray((1000,), dtype=float32, min=-0.004, max=0.001, mean=-0.001)},\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=17666)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-29 06:19:52,047\tINFO policy_evaluator.py:564 -- Training on concatenated sample batches:\n",
      "\n",
      "{ 'data': { 'action_prob': np.ndarray((3058,), dtype=float32, min=0.0, max=0.399, mean=0.283),\n",
      "            'actions': np.ndarray((3058, 1), dtype=float32, min=-3.603, max=4.024, mean=0.011),\n",
      "            'advantages': np.ndarray((3058,), dtype=float32, min=0.684, max=17.627, mean=14.409),\n",
      "            'agent_index': np.ndarray((3058,), dtype=int64, min=0.0, max=5.0, mean=2.465),\n",
      "            'behaviour_logits': np.ndarray((3058, 2), dtype=float32, min=-0.007, max=0.008, mean=0.002),\n",
      "            'dones': np.ndarray((3058,), dtype=bool, min=0.0, max=1.0, mean=0.006),\n",
      "            'eps_id': np.ndarray((3058,), dtype=int64, min=64280840.0, max=1742552928.0, mean=1279435003.714),\n",
      "            'infos': np.ndarray((3058,), dtype=object, head={'cost1': 0.0396621877457352, 'cost2': 0.0, 'mean_vel': 1.039616562488652, 'outflow': 468.0}),\n",
      "            'new_obs': np.ndarray((3058, 12), dtype=float32, min=-0.32, max=1.0, mean=0.321),\n",
      "            'obs': np.ndarray((3058, 12), dtype=float32, min=-0.32, max=1.0, mean=0.32),\n",
      "            'prev_actions': np.ndarray((3058, 1), dtype=float32, min=-3.603, max=4.024, mean=0.009),\n",
      "            'prev_rewards': np.ndarray((3058,), dtype=float32, min=0.0, max=0.702, mean=0.685),\n",
      "            'rewards': np.ndarray((3058,), dtype=float32, min=0.683, max=0.702, mean=0.69),\n",
      "            't': np.ndarray((3058,), dtype=int64, min=0.0, max=324.0, mean=159.996),\n",
      "            'unroll_id': np.ndarray((3058,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "            'value_targets': np.ndarray((3058,), dtype=float32, min=0.684, max=17.627, mean=14.408),\n",
      "            'vf_preds': np.ndarray((3058,), dtype=float32, min=-0.004, max=0.002, mean=-0.001)},\n",
      "  'type': 'SampleBatch'}\n",
      "\n",
      "2019-05-29 06:19:52,049\tINFO tf_run_builder.py:89 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "2019-05-29 06:19:52,424\tINFO policy_evaluator.py:586 -- Training output:\n",
      "\n",
      "{ 'learner_stats': { 'cur_kl_coeff': 0.2,\n",
      "                     'cur_lr': 0.0005000000237487257,\n",
      "                     'entropy': 1.4201114,\n",
      "                     'kl': 1.1694828e-10,\n",
      "                     'model': {},\n",
      "                     'policy_loss': -14.408919,\n",
      "                     'total_loss': 212.46538,\n",
      "                     'vf_explained_var': -3.33786e-06,\n",
      "                     'vf_loss': 226.87433}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'config': {'batch_mode': 'truncate_episodes',\n",
       "  'callbacks': {'on_episode_end': None,\n",
       "   'on_episode_start': None,\n",
       "   'on_episode_step': None,\n",
       "   'on_postprocess_traj': None,\n",
       "   'on_sample_end': None,\n",
       "   'on_train_result': None},\n",
       "  'clip_actions': False,\n",
       "  'clip_param': 0.3,\n",
       "  'clip_rewards': None,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'compress_observations': False,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'discrim_hidden_size': 128,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'env': 'MultiWaveAttenuationMergePOEnvGAIL-v0',\n",
       "  'env_config': {'flow_params': '{\\n    \"env\": {\\n        \"additional_params\": {\\n            \"FLOW_RATE\": 2000,\\n            \"FLOW_RATE_MERGE\": 100,\\n            \"RL_PENETRATION\": 0.1,\\n            \"buf_length\": 1,\\n            \"eta1\": 1.0,\\n            \"eta2\": 0.2,\\n            \"eta3\": 0.1,\\n            \"max_accel\": 3,\\n            \"max_decel\": 3,\\n            \"reward_scale\": 1.0,\\n            \"t_min\": 1.0,\\n            \"target_velocity\": 25\\n        },\\n        \"evaluate\": false,\\n        \"horizon\": 750,\\n        \"sims_per_step\": 2,\\n        \"warmup_steps\": 100\\n    },\\n    \"env_name\": \"MultiWaveAttenuationMergePOEnvGAIL\",\\n    \"exp_tag\": \"multi_merge\",\\n    \"initial\": {\\n        \"additional_params\": {},\\n        \"bunching\": 0,\\n        \"edges_distribution\": \"all\",\\n        \"lanes_distribution\": Infinity,\\n        \"min_gap\": 0,\\n        \"perturbation\": 0.0,\\n        \"shuffle\": false,\\n        \"spacing\": \"uniform\",\\n        \"x0\": 0\\n    },\\n    \"net\": {\\n        \"additional_params\": {\\n            \"highway_lanes\": 1,\\n            \"merge_lanes\": 1,\\n            \"merge_length\": 100,\\n            \"post_merge_length\": 100,\\n            \"pre_merge_length\": 600,\\n            \"speed_limit\": 30\\n        },\\n        \"inflows\": {\\n            \"_InFlows__flows\": [\\n                {\\n                    \"begin\": 1,\\n                    \"departLane\": \"free\",\\n                    \"departSpeed\": 10,\\n                    \"edge\": \"inflow_highway\",\\n                    \"end\": 2000000.0,\\n                    \"name\": \"flow_0\",\\n                    \"vehsPerHour\": 1800.0,\\n                    \"vtype\": \"human\"\\n                },\\n                {\\n                    \"begin\": 1,\\n                    \"departLane\": \"free\",\\n                    \"departSpeed\": 10,\\n                    \"edge\": \"inflow_highway\",\\n                    \"end\": 2000000.0,\\n                    \"name\": \"flow_1\",\\n                    \"vehsPerHour\": 200.0,\\n                    \"vtype\": \"rl\"\\n                },\\n                {\\n                    \"begin\": 1,\\n                    \"departLane\": \"free\",\\n                    \"departSpeed\": 7.5,\\n                    \"edge\": \"inflow_merge\",\\n                    \"end\": 2000000.0,\\n                    \"name\": \"flow_2\",\\n                    \"vehsPerHour\": 100,\\n                    \"vtype\": \"human\"\\n                }\\n            ],\\n            \"num_flows\": 3\\n        },\\n        \"netfile\": null,\\n        \"no_internal_links\": false,\\n        \"osm_path\": null\\n    },\\n    \"scenario\": \"MergeScenario\",\\n    \"sim\": {\\n        \"emission_path\": null,\\n        \"lateral_resolution\": null,\\n        \"no_step_log\": true,\\n        \"num_clients\": 1,\\n        \"overtake_right\": false,\\n        \"port\": null,\\n        \"print_warnings\": true,\\n        \"pxpm\": 2,\\n        \"render\": false,\\n        \"restart_instance\": true,\\n        \"save_render\": false,\\n        \"seed\": null,\\n        \"show_radius\": false,\\n        \"sight_radius\": 25,\\n        \"sim_step\": 0.2,\\n        \"teleport_time\": -1\\n    },\\n    \"simulator\": \"traci\",\\n    \"veh\": [\\n        {\\n            \"acceleration_controller\": [\\n                \"SimCarFollowingController\",\\n                {}\\n            ],\\n            \"car_following_params\": {\\n                \"controller_params\": {\\n                    \"accel\": 1.0,\\n                    \"carFollowModel\": \"IDM\",\\n                    \"decel\": 1.5,\\n                    \"impatience\": 0.5,\\n                    \"maxSpeed\": 30,\\n                    \"minGap\": 2.5,\\n                    \"sigma\": 0.5,\\n                    \"speedDev\": 0.1,\\n                    \"speedFactor\": 1.0,\\n                    \"tau\": 1.0\\n                },\\n                \"speed_mode\": 1\\n            },\\n            \"initial_speed\": 0,\\n            \"lane_change_controller\": [\\n                \"SimLaneChangeController\",\\n                {}\\n            ],\\n            \"lane_change_params\": {\\n                \"controller_params\": {\\n                    \"laneChangeModel\": \"LC2013\",\\n                    \"lcCooperative\": \"1.0\",\\n                    \"lcKeepRight\": \"1.0\",\\n                    \"lcSpeedGain\": \"1.0\",\\n                    \"lcStrategic\": \"1.0\"\\n                },\\n                \"lane_change_mode\": 512\\n            },\\n            \"num_vehicles\": 5,\\n            \"routing_controller\": null,\\n            \"veh_id\": \"human\"\\n        },\\n        {\\n            \"acceleration_controller\": [\\n                \"RLController\",\\n                {}\\n            ],\\n            \"car_following_params\": {\\n                \"controller_params\": {\\n                    \"accel\": 1.0,\\n                    \"carFollowModel\": \"IDM\",\\n                    \"decel\": 1.5,\\n                    \"impatience\": 0.5,\\n                    \"maxSpeed\": 30,\\n                    \"minGap\": 2.5,\\n                    \"sigma\": 0.5,\\n                    \"speedDev\": 0.1,\\n                    \"speedFactor\": 1.0,\\n                    \"tau\": 1.0\\n                },\\n                \"speed_mode\": 1\\n            },\\n            \"initial_speed\": 0,\\n            \"lane_change_controller\": [\\n                \"SimLaneChangeController\",\\n                {}\\n            ],\\n            \"lane_change_params\": {\\n                \"controller_params\": {\\n                    \"laneChangeModel\": \"LC2013\",\\n                    \"lcCooperative\": \"1.0\",\\n                    \"lcKeepRight\": \"1.0\",\\n                    \"lcSpeedGain\": \"1.0\",\\n                    \"lcStrategic\": \"1.0\"\\n                },\\n                \"lane_change_mode\": 512\\n            },\\n            \"num_vehicles\": 0,\\n            \"routing_controller\": null,\\n            \"veh_id\": \"rl\"\\n        }\\n    ]\\n}'},\n",
       "  'expert_path': '/headless/rl_project/flow_codes/ModelBased/expert_sample',\n",
       "  'gamma': 0.99,\n",
       "  'grad_clip': None,\n",
       "  'horizon': 750,\n",
       "  'ignore_worker_failures': False,\n",
       "  'input': 'sampler',\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'kl_coeff': 0.2,\n",
       "  'kl_target': 0.01,\n",
       "  'lambda': 0.97,\n",
       "  'local_evaluator_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
       "   'intra_op_parallelism_threads': 8},\n",
       "  'log_level': 'INFO',\n",
       "  'lr': 0.0005,\n",
       "  'lr_schedule': None,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'model': {'conv_activation': 'relu',\n",
       "   'conv_filters': None,\n",
       "   'custom_model': None,\n",
       "   'custom_options': {},\n",
       "   'custom_preprocessor': None,\n",
       "   'dim': 84,\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_hiddens': [128, 64, 32],\n",
       "   'framestack': True,\n",
       "   'free_log_std': False,\n",
       "   'grayscale': False,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action_reward': False,\n",
       "   'max_seq_len': 20,\n",
       "   'squash_to_range': False,\n",
       "   'use_lstm': False,\n",
       "   'zero_mean': True},\n",
       "  'monitor': False,\n",
       "  'multiagent': {'policies_to_train': ['default_policy'],\n",
       "   'policy_graphs': {'default_policy': (ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph,\n",
       "     Box(12,),\n",
       "     Box(1,),\n",
       "     {})},\n",
       "   'policy_mapping_fn': tune.function(<function <lambda> at 0x7f0ee45ebd08>)},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'num_gpus': 0,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'num_sgd_iter': 10,\n",
       "  'num_workers': 3,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'optimizer': {},\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'postprocess_inputs': False,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'remote_worker_envs': False,\n",
       "  'sample_async': False,\n",
       "  'sample_batch_size': 375.0,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'simple_optimizer': False,\n",
       "  'soft_horizon': False,\n",
       "  'straggler_mitigation': False,\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'allow_soft_placement': True,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'intra_op_parallelism_threads': 2,\n",
       "   'log_device_placement': False},\n",
       "  'train_batch_size': 2250,\n",
       "  'use_gae': True,\n",
       "  'vf_clip_param': 1000000.0,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'vf_share_layers': False},\n",
       " 'custom_metrics': {'discrim_loss': 1.3874508142471313},\n",
       " 'date': '2019-05-29_06-19-52',\n",
       " 'done': False,\n",
       " 'episode_len_mean': 325.0,\n",
       " 'episode_reward_max': 689.7177656292915,\n",
       " 'episode_reward_mean': 628.7871652245522,\n",
       " 'episode_reward_min': 581.079970896244,\n",
       " 'episodes_this_iter': 3,\n",
       " 'episodes_total': 3,\n",
       " 'experiment_id': '3517d890c6734e86a67ed950710a0db2',\n",
       " 'hostname': 'kronos',\n",
       " 'iterations_since_restore': 1,\n",
       " 'node_ip': '169.237.32.118',\n",
       " 'num_metric_batches_dropped': 0,\n",
       " 'off_policy_estimator': {},\n",
       " 'pid': 17631,\n",
       " 'policy_reward_mean': {},\n",
       " 'sampler_perf': {'mean_env_wait_ms': 64.52270560230768,\n",
       "  'mean_inference_ms': 4.026200754422668,\n",
       "  'mean_processing_ms': 4.858033031436569},\n",
       " 'time_since_restore': 39.46525287628174,\n",
       " 'time_this_iter_s': 39.46525287628174,\n",
       " 'time_total_s': 39.46525287628174,\n",
       " 'timestamp': 1559110792,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'timesteps_total': None,\n",
       " 'training_iteration': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:22,497\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:22.499366: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m 2019-05-27 21:18:22,702\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m 2019-05-27 21:18:22.703959: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m 2019-05-27 21:18:22,982\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m 2019-05-27 21:18:22.984504: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:23,838\tINFO policy_evaluator.py:437 -- Generating sample batch of size 375.0\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:25,441\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'flow_1.0': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.513),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m        'flow_1.1': np.ndarray((12,), dtype=float32, min=0.025, max=0.879, mean=0.199)}}\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:25,441\tINFO sampler.py:309 -- Info return from env: {0: {'flow_1.0': {}, 'flow_1.1': {}}}\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:25,442\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.025, max=0.879, mean=0.199)\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:25,442\tINFO sampler.py:411 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.025, max=0.879, mean=0.199)\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:25,443\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m { 'rl': [ { 'data': { 'agent_id': 'flow_1.1',\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'info': {},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'obs': np.ndarray((12,), dtype=float32, min=0.025, max=0.879, mean=0.199),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m             'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m           { 'data': { 'agent_id': 'flow_1.0',\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'info': {},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.513),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                       'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m             'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:25,444\tINFO tf_run_builder.py:89 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:25,482\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m { 'rl': ( np.ndarray((2, 1), dtype=float32, min=-0.342, max=-0.205, mean=-0.274),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m           [],\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m           { 'action_prob': np.ndarray((2,), dtype=float32, min=0.377, max=0.392, mean=0.384),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m             'behaviour_logits': np.ndarray((2, 2), dtype=float32, min=-0.004, max=0.005, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m             'vf_preds': np.ndarray((2,), dtype=float32, min=0.001, max=0.004, mean=0.003)})}\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:29,234\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m { 'flow_1.0': { 'data': { 'action_prob': np.ndarray((23,), dtype=float32, min=0.034, max=0.4, mean=0.289),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'actions': np.ndarray((23, 1), dtype=float32, min=-2.066, max=2.213, mean=-0.202),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'advantages': np.ndarray((23,), dtype=float32, min=3.654, max=46.249, mean=29.795),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'agent_index': np.ndarray((23,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'behaviour_logits': np.ndarray((23, 2), dtype=float32, min=-0.005, max=0.006, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'dones': np.ndarray((23,), dtype=bool, min=0.0, max=1.0, mean=0.043),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'eps_id': np.ndarray((23,), dtype=int64, min=414283089.0, max=414283089.0, mean=414283089.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'infos': np.ndarray((23,), dtype=object, head={'outflow': 445.5445544554455, 'mean_vel': 18.949074768428677, 'cost2': 0.0, 'cost1': 0.6584389678280226}),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'new_obs': np.ndarray((23, 12), dtype=float32, min=-0.028, max=1.0, mean=0.541),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'obs': np.ndarray((23, 12), dtype=float32, min=-0.028, max=1.0, mean=0.538),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_actions': np.ndarray((23, 1), dtype=float32, min=-1.92, max=2.213, mean=-0.112),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_rewards': np.ndarray((23,), dtype=float32, min=0.0, max=3.715, mean=2.955),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'rewards': np.ndarray((23,), dtype=float32, min=2.789, max=3.715, mean=3.114),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           't': np.ndarray((23,), dtype=int64, min=0.0, max=22.0, mean=11.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'unroll_id': np.ndarray((23,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'value_targets': np.ndarray((23,), dtype=float32, min=3.656, max=46.254, mean=29.799),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'vf_preds': np.ndarray((23,), dtype=float32, min=0.002, max=0.005, mean=0.004)},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   'flow_1.1': { 'data': { 'action_prob': np.ndarray((134,), dtype=float32, min=0.008, max=0.4, mean=0.282),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'actions': np.ndarray((134, 1), dtype=float32, min=-2.804, max=2.593, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'advantages': np.ndarray((134,), dtype=float32, min=3.691, max=88.268, mean=69.107),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'agent_index': np.ndarray((134,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'behaviour_logits': np.ndarray((134, 2), dtype=float32, min=-0.006, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'dones': np.ndarray((134,), dtype=bool, min=0.0, max=1.0, mean=0.007),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'eps_id': np.ndarray((134,), dtype=int64, min=414283089.0, max=414283089.0, mean=414283089.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'infos': np.ndarray((134,), dtype=object, head={'outflow': 445.5445544554455, 'mean_vel': 18.949074768428677, 'cost2': 0.0, 'cost1': 0.6584389678280226}),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'new_obs': np.ndarray((134, 12), dtype=float32, min=-0.244, max=1.0, mean=0.505),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'obs': np.ndarray((134, 12), dtype=float32, min=-0.244, max=1.0, mean=0.502),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_actions': np.ndarray((134, 1), dtype=float32, min=-2.804, max=2.593, mean=-0.01),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_rewards': np.ndarray((134,), dtype=float32, min=0.0, max=3.815, mean=3.094),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'rewards': np.ndarray((134,), dtype=float32, min=1.179, max=3.815, mean=3.122),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           't': np.ndarray((134,), dtype=int64, min=0.0, max=133.0, mean=66.5),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'unroll_id': np.ndarray((134,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'value_targets': np.ndarray((134,), dtype=float32, min=3.693, max=88.271, mean=69.11),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'vf_preds': np.ndarray((134,), dtype=float32, min=-0.0, max=0.003, mean=0.002)},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   'flow_1.2': { 'data': { 'action_prob': np.ndarray((310,), dtype=float32, min=0.014, max=0.401, mean=0.278),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'actions': np.ndarray((310, 1), dtype=float32, min=-2.579, max=2.542, mean=-0.05),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'advantages': np.ndarray((310,), dtype=float32, min=3.773, max=94.008, mean=62.775),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'agent_index': np.ndarray((310,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'behaviour_logits': np.ndarray((310, 2), dtype=float32, min=-0.005, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'dones': np.ndarray((310,), dtype=bool, min=0.0, max=1.0, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'eps_id': np.ndarray((310,), dtype=int64, min=414283089.0, max=414283089.0, mean=414283089.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'infos': np.ndarray((310,), dtype=object, head={'outflow': 543.103448275862, 'mean_vel': 18.533559329035675, 'cost2': 0.0, 'cost1': 0.6695527472466182}),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'new_obs': np.ndarray((310, 12), dtype=float32, min=-0.244, max=1.0, mean=0.414),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'obs': np.ndarray((310, 12), dtype=float32, min=-0.244, max=1.0, mean=0.413),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_actions': np.ndarray((310, 1), dtype=float32, min=-2.579, max=2.542, mean=-0.052),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_rewards': np.ndarray((310,), dtype=float32, min=1.235, max=3.805, mean=2.601),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'rewards': np.ndarray((310,), dtype=float32, min=1.235, max=3.805, mean=2.607),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           't': np.ndarray((310,), dtype=int64, min=15.0, max=324.0, mean=169.5),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'unroll_id': np.ndarray((310,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'value_targets': np.ndarray((310,), dtype=float32, min=3.775, max=94.012, mean=62.777),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'vf_preds': np.ndarray((310,), dtype=float32, min=-0.0, max=0.004, mean=0.002)},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   'flow_1.3': { 'data': { 'action_prob': np.ndarray((263,), dtype=float32, min=0.01, max=0.4, mean=0.279),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'actions': np.ndarray((263, 1), dtype=float32, min=-2.672, max=2.722, mean=-0.025),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'advantages': np.ndarray((263,), dtype=float32, min=1.461, max=37.636, mean=26.898),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'agent_index': np.ndarray((263,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'behaviour_logits': np.ndarray((263, 2), dtype=float32, min=-0.004, max=0.004, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'dones': np.ndarray((263,), dtype=bool, min=0.0, max=1.0, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'eps_id': np.ndarray((263,), dtype=int64, min=414283089.0, max=414283089.0, mean=414283089.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'infos': np.ndarray((263,), dtype=object, head={'outflow': 883.4355828220858, 'mean_vel': 12.741131067810349, 'cost2': 0.0, 'cost1': 0.48809618741206434}),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'new_obs': np.ndarray((263, 12), dtype=float32, min=-0.146, max=1.0, mean=0.164),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'obs': np.ndarray((263, 12), dtype=float32, min=-0.146, max=1.0, mean=0.165),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_actions': np.ndarray((263, 1), dtype=float32, min=-2.672, max=2.722, mean=-0.026),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_rewards': np.ndarray((263,), dtype=float32, min=0.722, max=1.786, mean=1.191),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'rewards': np.ndarray((263,), dtype=float32, min=0.722, max=1.774, mean=1.189),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           't': np.ndarray((263,), dtype=int64, min=62.0, max=324.0, mean=193.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'unroll_id': np.ndarray((263,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'value_targets': np.ndarray((263,), dtype=float32, min=1.462, max=37.636, mean=26.899),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'vf_preds': np.ndarray((263,), dtype=float32, min=-0.0, max=0.002, mean=0.001)},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   'flow_1.4': { 'data': { 'action_prob': np.ndarray((207,), dtype=float32, min=0.006, max=0.399, mean=0.278),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'actions': np.ndarray((207, 1), dtype=float32, min=-2.117, max=2.912, mean=0.018),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'advantages': np.ndarray((207,), dtype=float32, min=0.839, max=32.481, mean=21.379),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'agent_index': np.ndarray((207,), dtype=int64, min=4.0, max=4.0, mean=4.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'behaviour_logits': np.ndarray((207, 2), dtype=float32, min=-0.003, max=0.004, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'dones': np.ndarray((207,), dtype=bool, min=0.0, max=1.0, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'eps_id': np.ndarray((207,), dtype=int64, min=414283089.0, max=414283089.0, mean=414283089.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'infos': np.ndarray((207,), dtype=object, head={'outflow': 739.7260273972602, 'mean_vel': 8.604745033873435, 'cost2': 0.0, 'cost1': 0.34109533241580603}),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'new_obs': np.ndarray((207, 12), dtype=float32, min=-0.104, max=1.0, mean=0.11),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'obs': np.ndarray((207, 12), dtype=float32, min=-0.104, max=1.0, mean=0.111),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_actions': np.ndarray((207, 1), dtype=float32, min=-2.117, max=2.912, mean=0.02),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_rewards': np.ndarray((207,), dtype=float32, min=0.837, max=1.723, mean=1.004),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'rewards': np.ndarray((207,), dtype=float32, min=0.837, max=1.713, mean=0.999),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           't': np.ndarray((207,), dtype=int64, min=118.0, max=324.0, mean=221.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'unroll_id': np.ndarray((207,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'value_targets': np.ndarray((207,), dtype=float32, min=0.841, max=32.481, mean=21.381),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'vf_preds': np.ndarray((207,), dtype=float32, min=-0.0, max=0.002, mean=0.002)},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   'flow_1.5': { 'data': { 'action_prob': np.ndarray((144,), dtype=float32, min=0.01, max=0.4, mean=0.284),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'actions': np.ndarray((144, 1), dtype=float32, min=-2.711, max=2.378, mean=-0.022),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'advantages': np.ndarray((144,), dtype=float32, min=0.96, max=31.835, mean=21.62),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'agent_index': np.ndarray((144,), dtype=int64, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'behaviour_logits': np.ndarray((144, 2), dtype=float32, min=-0.003, max=0.004, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'dones': np.ndarray((144,), dtype=bool, min=0.0, max=1.0, mean=0.007),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'eps_id': np.ndarray((144,), dtype=int64, min=414283089.0, max=414283089.0, mean=414283089.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'infos': np.ndarray((144,), dtype=object, head={'outflow': 864.0, 'mean_vel': 5.33910011184582, 'cost2': 0.0, 'cost1': 0.20567629744699786}),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'new_obs': np.ndarray((144, 12), dtype=float32, min=-0.233, max=1.0, mean=0.114),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'obs': np.ndarray((144, 12), dtype=float32, min=-0.233, max=1.0, mean=0.116),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_actions': np.ndarray((144, 1), dtype=float32, min=-2.711, max=2.378, mean=-0.01),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'prev_rewards': np.ndarray((144,), dtype=float32, min=0.96, max=1.599, mean=1.075),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'rewards': np.ndarray((144,), dtype=float32, min=0.96, max=1.592, mean=1.071),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           't': np.ndarray((144,), dtype=int64, min=181.0, max=324.0, mean=252.5),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'unroll_id': np.ndarray((144,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'value_targets': np.ndarray((144,), dtype=float32, min=0.962, max=31.835, mean=21.622),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                           'vf_preds': np.ndarray((144,), dtype=float32, min=-0.0, max=0.002, mean=0.002)},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                 'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m 2019-05-27 21:18:31,288\tINFO policy_evaluator.py:474 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m { 'count': 375,\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   'policy_batches': { 'rl': { 'data': { 'action_prob': np.ndarray((1200,), dtype=float32, min=0.006, max=0.401, mean=0.28),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'actions': np.ndarray((1200, 1), dtype=float32, min=-2.804, max=2.912, mean=-0.02),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'advantages': np.ndarray((1200,), dtype=float32, min=0.839, max=94.008, mean=39.505),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'agent_index': np.ndarray((1200,), dtype=int64, min=0.0, max=5.0, mean=2.57),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'behaviour_logits': np.ndarray((1200, 2), dtype=float32, min=-0.006, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'dones': np.ndarray((1200,), dtype=bool, min=0.0, max=1.0, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'eps_id': np.ndarray((1200,), dtype=int64, min=414283089.0, max=1469289490.0, mean=518904557.099),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'infos': np.ndarray((1200,), dtype=object, head={'outflow': 445.5445544554455, 'mean_vel': 18.949074768428677, 'cost2': 0.0, 'cost1': 0.6584389678280226}),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'new_obs': np.ndarray((1200, 12), dtype=float32, min=-0.244, max=1.0, mean=0.276),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'obs': np.ndarray((1200, 12), dtype=float32, min=-0.244, max=1.0, mean=0.275),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'prev_actions': np.ndarray((1200, 1), dtype=float32, min=-2.804, max=2.912, mean=-0.018),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'prev_rewards': np.ndarray((1200,), dtype=float32, min=0.0, max=3.815, mean=1.836),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'rewards': np.ndarray((1200,), dtype=float32, min=0.722, max=3.815, mean=1.848),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         't': np.ndarray((1200,), dtype=int64, min=0.0, max=324.0, mean=164.567),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'unroll_id': np.ndarray((1200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'value_targets': np.ndarray((1200,), dtype=float32, min=0.841, max=94.012, mean=39.507),\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                                         'vf_preds': np.ndarray((1200,), dtype=float32, min=-0.0, max=0.005, mean=0.002)},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m                               'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7454)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7457)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=7456)\u001b[0m Loading configuration... done.\n"
     ]
    }
   ],
   "source": [
    "samples = agent.sample(agent.train_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9192)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=9192)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=9192)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=9189)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=9189)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=9189)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=9191)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=9191)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=9191)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=9189)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=9189)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=9189)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=9191)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=9191)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=9191)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=9192)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=9192)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=9192)\u001b[0m Loading configuration... done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-28 20:25:11,840\tINFO policy_evaluator.py:564 -- Training on concatenated sample batches:\n",
      "\n",
      "{ 'count': 2250,\n",
      "  'policy_batches': { 'rl': { 'data': { 'action_prob': np.ndarray((6064,), dtype=float32, min=0.001, max=0.4, mean=0.284),\n",
      "                                        'actions': np.ndarray((6064, 1), dtype=float32, min=-3.446, max=3.51, mean=0.005),\n",
      "                                        'advantages': np.ndarray((6064,), dtype=float32, min=-23.186, max=-0.288, mean=-15.745),\n",
      "                                        'agent_index': np.ndarray((6064,), dtype=int64, min=0.0, max=5.0, mean=2.5),\n",
      "                                        'behaviour_logits': np.ndarray((6064, 2), dtype=float32, min=-0.003, max=0.004, mean=-0.0),\n",
      "                                        'dones': np.ndarray((6064,), dtype=bool, min=0.0, max=1.0, mean=0.005),\n",
      "                                        'eps_id': np.ndarray((6064,), dtype=int64, min=34288929.0, max=1873683181.0, mean=757703806.976),\n",
      "                                        'infos': np.ndarray((6064,), dtype=object, head={'outflow': 805.9701492537313, 'cost2': 0.0, 'cost1': 0.28833636102515314, 'mean_vel': 7.66967815867466}),\n",
      "                                        'new_obs': np.ndarray((6064, 12), dtype=float32, min=-0.497, max=1.0, mean=0.319),\n",
      "                                        'obs': np.ndarray((6064, 12), dtype=float32, min=-0.497, max=1.0, mean=0.319),\n",
      "                                        'prev_actions': np.ndarray((6064, 1), dtype=float32, min=-3.446, max=3.51, mean=0.003),\n",
      "                                        'prev_rewards': np.ndarray((6064,), dtype=float32, min=-1.0, max=0.0, mean=-0.728),\n",
      "                                        'rewards': np.ndarray((6064,), dtype=float32, min=-1.0, max=-0.27, mean=-0.73),\n",
      "                                        't': np.ndarray((6064,), dtype=int64, min=0.0, max=324.0, mean=169.596),\n",
      "                                        'unroll_id': np.ndarray((6064,), dtype=int64, min=2.0, max=3.0, mean=2.529),\n",
      "                                        'value_targets': np.ndarray((6064,), dtype=float32, min=-23.185, max=-0.284, mean=-15.743),\n",
      "                                        'vf_preds': np.ndarray((6064,), dtype=float32, min=-0.001, max=0.005, mean=0.001)},\n",
      "                              'type': 'SampleBatch'}},\n",
      "  'type': 'MultiAgentBatch'}\n",
      "\n",
      "2019-05-28 20:25:11,841\tINFO tf_run_builder.py:89 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "2019-05-28 20:25:12,244\tINFO policy_evaluator.py:586 -- Training output:\n",
      "\n",
      "{ 'rl': { 'learner_stats': { 'cur_kl_coeff': 0.2,\n",
      "                             'cur_lr': 0.0005000000237487257,\n",
      "                             'entropy': 1.4187908,\n",
      "                             'kl': 1.9658525e-11,\n",
      "                             'model': {},\n",
      "                             'policy_loss': 15.74481,\n",
      "                             'total_loss': 296.43738,\n",
      "                             'vf_explained_var': 0.00011467934,\n",
      "                             'vf_loss': 280.69257}}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'config': {'batch_mode': 'truncate_episodes',\n",
       "  'callbacks': {'on_episode_end': None,\n",
       "   'on_episode_start': None,\n",
       "   'on_episode_step': None,\n",
       "   'on_postprocess_traj': None,\n",
       "   'on_sample_end': None,\n",
       "   'on_train_result': None},\n",
       "  'clip_actions': False,\n",
       "  'clip_param': 0.3,\n",
       "  'clip_rewards': None,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'compress_observations': False,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'entropy_coeff': 0.0,\n",
       "  'env': 'MultiWaveAttenuationMergePOEnv-v0',\n",
       "  'env_config': {'flow_params': '{\\n    \"env\": {\\n        \"additional_params\": {\\n            \"FLOW_RATE\": 2000,\\n            \"FLOW_RATE_MERGE\": 100,\\n            \"RL_PENETRATION\": 0.1,\\n            \"buf_length\": 1,\\n            \"eta1\": 1.0,\\n            \"eta2\": 0.2,\\n            \"eta3\": 0.1,\\n            \"max_accel\": 3,\\n            \"max_decel\": 3,\\n            \"reward_scale\": 1.0,\\n            \"t_min\": 1.0,\\n            \"target_velocity\": 25\\n        },\\n        \"evaluate\": false,\\n        \"horizon\": 750,\\n        \"sims_per_step\": 2,\\n        \"warmup_steps\": 100\\n    },\\n    \"env_name\": \"MultiWaveAttenuationMergePOEnv\",\\n    \"exp_tag\": \"multi_merge\",\\n    \"initial\": {\\n        \"additional_params\": {},\\n        \"bunching\": 0,\\n        \"edges_distribution\": \"all\",\\n        \"lanes_distribution\": Infinity,\\n        \"min_gap\": 0,\\n        \"perturbation\": 0.0,\\n        \"shuffle\": false,\\n        \"spacing\": \"uniform\",\\n        \"x0\": 0\\n    },\\n    \"net\": {\\n        \"additional_params\": {\\n            \"highway_lanes\": 1,\\n            \"merge_lanes\": 1,\\n            \"merge_length\": 100,\\n            \"post_merge_length\": 100,\\n            \"pre_merge_length\": 600,\\n            \"speed_limit\": 30\\n        },\\n        \"inflows\": {\\n            \"_InFlows__flows\": [\\n                {\\n                    \"begin\": 1,\\n                    \"departLane\": \"free\",\\n                    \"departSpeed\": 10,\\n                    \"edge\": \"inflow_highway\",\\n                    \"end\": 2000000.0,\\n                    \"name\": \"flow_0\",\\n                    \"vehsPerHour\": 1800.0,\\n                    \"vtype\": \"human\"\\n                },\\n                {\\n                    \"begin\": 1,\\n                    \"departLane\": \"free\",\\n                    \"departSpeed\": 10,\\n                    \"edge\": \"inflow_highway\",\\n                    \"end\": 2000000.0,\\n                    \"name\": \"flow_1\",\\n                    \"vehsPerHour\": 200.0,\\n                    \"vtype\": \"rl\"\\n                },\\n                {\\n                    \"begin\": 1,\\n                    \"departLane\": \"free\",\\n                    \"departSpeed\": 7.5,\\n                    \"edge\": \"inflow_merge\",\\n                    \"end\": 2000000.0,\\n                    \"name\": \"flow_2\",\\n                    \"vehsPerHour\": 100,\\n                    \"vtype\": \"human\"\\n                }\\n            ],\\n            \"num_flows\": 3\\n        },\\n        \"netfile\": null,\\n        \"no_internal_links\": false,\\n        \"osm_path\": null\\n    },\\n    \"scenario\": \"MergeScenario\",\\n    \"sim\": {\\n        \"emission_path\": null,\\n        \"lateral_resolution\": null,\\n        \"no_step_log\": true,\\n        \"num_clients\": 1,\\n        \"overtake_right\": false,\\n        \"port\": null,\\n        \"print_warnings\": true,\\n        \"pxpm\": 2,\\n        \"render\": false,\\n        \"restart_instance\": true,\\n        \"save_render\": false,\\n        \"seed\": null,\\n        \"show_radius\": false,\\n        \"sight_radius\": 25,\\n        \"sim_step\": 0.2,\\n        \"teleport_time\": -1\\n    },\\n    \"simulator\": \"traci\",\\n    \"veh\": [\\n        {\\n            \"acceleration_controller\": [\\n                \"SimCarFollowingController\",\\n                {}\\n            ],\\n            \"car_following_params\": {\\n                \"controller_params\": {\\n                    \"accel\": 1.0,\\n                    \"carFollowModel\": \"IDM\",\\n                    \"decel\": 1.5,\\n                    \"impatience\": 0.5,\\n                    \"maxSpeed\": 30,\\n                    \"minGap\": 2.5,\\n                    \"sigma\": 0.5,\\n                    \"speedDev\": 0.1,\\n                    \"speedFactor\": 1.0,\\n                    \"tau\": 1.0\\n                },\\n                \"speed_mode\": 1\\n            },\\n            \"initial_speed\": 0,\\n            \"lane_change_controller\": [\\n                \"SimLaneChangeController\",\\n                {}\\n            ],\\n            \"lane_change_params\": {\\n                \"controller_params\": {\\n                    \"laneChangeModel\": \"LC2013\",\\n                    \"lcCooperative\": \"1.0\",\\n                    \"lcKeepRight\": \"1.0\",\\n                    \"lcSpeedGain\": \"1.0\",\\n                    \"lcStrategic\": \"1.0\"\\n                },\\n                \"lane_change_mode\": 512\\n            },\\n            \"num_vehicles\": 5,\\n            \"routing_controller\": null,\\n            \"veh_id\": \"human\"\\n        },\\n        {\\n            \"acceleration_controller\": [\\n                \"RLController\",\\n                {}\\n            ],\\n            \"car_following_params\": {\\n                \"controller_params\": {\\n                    \"accel\": 1.0,\\n                    \"carFollowModel\": \"IDM\",\\n                    \"decel\": 1.5,\\n                    \"impatience\": 0.5,\\n                    \"maxSpeed\": 30,\\n                    \"minGap\": 2.5,\\n                    \"sigma\": 0.5,\\n                    \"speedDev\": 0.1,\\n                    \"speedFactor\": 1.0,\\n                    \"tau\": 1.0\\n                },\\n                \"speed_mode\": 1\\n            },\\n            \"initial_speed\": 0,\\n            \"lane_change_controller\": [\\n                \"SimLaneChangeController\",\\n                {}\\n            ],\\n            \"lane_change_params\": {\\n                \"controller_params\": {\\n                    \"laneChangeModel\": \"LC2013\",\\n                    \"lcCooperative\": \"1.0\",\\n                    \"lcKeepRight\": \"1.0\",\\n                    \"lcSpeedGain\": \"1.0\",\\n                    \"lcStrategic\": \"1.0\"\\n                },\\n                \"lane_change_mode\": 512\\n            },\\n            \"num_vehicles\": 0,\\n            \"routing_controller\": null,\\n            \"veh_id\": \"rl\"\\n        }\\n    ]\\n}'},\n",
       "  'expert_path': '/headless/rl_project/flow_codes/ModelBased/expert_sample',\n",
       "  'gamma': 0.99,\n",
       "  'grad_clip': None,\n",
       "  'horizon': 750,\n",
       "  'ignore_worker_failures': False,\n",
       "  'input': 'sampler',\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'kl_coeff': 0.2,\n",
       "  'kl_target': 0.01,\n",
       "  'lambda': 0.97,\n",
       "  'local_evaluator_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
       "   'intra_op_parallelism_threads': 8},\n",
       "  'log_level': 'INFO',\n",
       "  'lr': 0.0005,\n",
       "  'lr_schedule': None,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'model': {'conv_activation': 'relu',\n",
       "   'conv_filters': None,\n",
       "   'custom_model': None,\n",
       "   'custom_options': {},\n",
       "   'custom_preprocessor': None,\n",
       "   'dim': 84,\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_hiddens': [128, 64, 32],\n",
       "   'framestack': True,\n",
       "   'free_log_std': False,\n",
       "   'grayscale': False,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action_reward': False,\n",
       "   'max_seq_len': 20,\n",
       "   'squash_to_range': False,\n",
       "   'use_lstm': False,\n",
       "   'zero_mean': True},\n",
       "  'monitor': False,\n",
       "  'multiagent': {'policies_to_train': ['rl'],\n",
       "   'policy_graphs': {'rl': (ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph,\n",
       "     Box(12,),\n",
       "     Box(1,),\n",
       "     {})},\n",
       "   'policy_mapping_fn': tune.function(<function <lambda> at 0x7f550641ee18>)},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'num_gpus': 0,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'num_sgd_iter': 10,\n",
       "  'num_train': 2,\n",
       "  'num_workers': 3,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'optimizer': {},\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'postprocess_inputs': False,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'remote_worker_envs': False,\n",
       "  'sample_async': False,\n",
       "  'sample_batch_size': 375.0,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'simple_optimizer': False,\n",
       "  'soft_horizon': False,\n",
       "  'straggler_mitigation': False,\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'allow_soft_placement': True,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'intra_op_parallelism_threads': 2,\n",
       "   'log_device_placement': False},\n",
       "  'theta_lr': 0.1,\n",
       "  'train_batch_size': 2250,\n",
       "  'use_gae': True,\n",
       "  'vf_clip_param': 1000000.0,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'vf_share_layers': False},\n",
       " 'custom_metrics': {},\n",
       " 'date': '2019-05-28_20-25-12',\n",
       " 'done': False,\n",
       " 'episode_len_mean': 325.0,\n",
       " 'episode_reward_max': -490.6953199638094,\n",
       " 'episode_reward_mean': -654.9300107996338,\n",
       " 'episode_reward_min': -842.1682233935616,\n",
       " 'episodes_this_iter': 12,\n",
       " 'episodes_total': 12,\n",
       " 'experiment_id': 'b2e84f72284748c8941973052b59aa9d',\n",
       " 'hostname': 'kronos',\n",
       " 'iterations_since_restore': 1,\n",
       " 'node_ip': '169.237.32.118',\n",
       " 'num_metric_batches_dropped': 0,\n",
       " 'off_policy_estimator': {},\n",
       " 'pid': 9154,\n",
       " 'policy_reward_mean': {'rl': -128.8386906491084},\n",
       " 'sampler_perf': {'mean_env_wait_ms': 9.610279332312802,\n",
       "  'mean_inference_ms': 1.0053818156500538,\n",
       "  'mean_processing_ms': 4.462189762268813},\n",
       " 'time_since_restore': 12.045937299728394,\n",
       " 'time_this_iter_s': 12.045937299728394,\n",
       " 'time_total_s': 12.045937299728394,\n",
       " 'timestamp': 1559075112,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'timesteps_total': None,\n",
       " 'training_iteration': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
