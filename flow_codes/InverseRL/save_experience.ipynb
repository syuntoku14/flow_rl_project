{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-30 20:28:51,755\tINFO node.py:469 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-30_20-28-51_38/logs.\n",
      "2019-05-30 20:28:51,938\tINFO services.py:407 -- Waiting for redis server at 127.0.0.1:41375 to respond...\n",
      "2019-05-30 20:28:52,191\tINFO services.py:407 -- Waiting for redis server at 127.0.0.1:49651 to respond...\n",
      "2019-05-30 20:28:52,195\tINFO services.py:804 -- Starting Redis shard with 10.0 GB max memory.\n",
      "2019-05-30 20:28:52,301\tINFO node.py:483 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-30_20-28-51_38/logs.\n",
      "2019-05-30 20:28:52,304\tINFO services.py:1427 -- Starting the Plasma object store with 18.23 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.138.0.2',\n",
       " 'object_store_address': '/tmp/ray/session_2019-05-30_20-28-51_38/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-05-30_20-28-51_38/sockets/raylet',\n",
       " 'redis_address': '10.138.0.2:41375',\n",
       " 'webui_url': None}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym, pickle, argparse, json, logging\n",
    "from copy import deepcopy\n",
    "import ray\n",
    "from gail.gail import GAILTrainer\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import PPOPolicyGraph\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "from ray.rllib.agents import Trainer\n",
    "from ray.rllib.evaluation import PolicyEvaluator, SampleBatch, MultiAgentSampleBatchBuilder\n",
    "from ray.rllib.offline.json_writer import JsonWriter\n",
    "from ray.rllib.offline.json_reader import JsonReader\n",
    "from ray.rllib.evaluation.sample_batch import DEFAULT_POLICY_ID\n",
    "from ray.rllib.evaluation.metrics import collect_metrics\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder, get_flow_params\n",
    "logger = logging.getLogger(__name__)\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpus = 11\n",
    "num_rollouts = 3 \n",
    "horizon = 750\n",
    "gae_lambda = 0.97\n",
    "step_size = 5e-4\n",
    "num_iter = 10\n",
    "benchmark_name = \"multi_merge\"\n",
    "exp_name = \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = deepcopy(DEFAULT_CONFIG)\n",
    "config[\"num_workers\"] = min(num_cpus, num_rollouts)\n",
    "config[\"train_batch_size\"] = horizon * num_rollouts\n",
    "config[\"sample_batch_size\"] = horizon\n",
    "config[\"use_gae\"] = True\n",
    "config[\"horizon\"] = horizon\n",
    "config[\"lambda\"] = gae_lambda\n",
    "config[\"lr\"] = step_size\n",
    "config[\"vf_clip_param\"] = 1e6\n",
    "config[\"num_sgd_iter\"] = 10\n",
    "config['clip_actions'] = False  # FIXME(ev) temporary ray bug\n",
    "config[\"model\"][\"fcnet_hiddens\"] = [128, 64, 32]\n",
    "config[\"observation_filter\"] = \"NoFilter\"\n",
    "config[\"entropy_coeff\"] = 0.0\n",
    "config[\"expert_path\"] = '/headless/rl_project/flow_codes/InverseRL/expert_sample'\n",
    "config[\"discrim_hidden_size\"] = 128\n",
    "\n",
    "benchmark = __import__(\n",
    "            \"flow.benchmarks.%s\" % benchmark_name, fromlist=[\"flow_params\"])\n",
    "flow_params = benchmark.gail_flow_params\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(\n",
    "    flow_params, cls=FlowParamsEncoder, sort_keys=True, indent=4)\n",
    "config['env_config']['flow_params'] = flow_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_env, env_name = make_create_env(params=flow_params, version=0)\n",
    "register_env(env_name, create_env)\n",
    "env = create_env()\n",
    "\n",
    "# we don't need this config\n",
    "POLICY_ID = DEFAULT_POLICY_ID \n",
    "default_policy = (PPOPolicyGraph, env.observation_space, env.action_space, {})\n",
    "policy_graph = {POLICY_ID: default_policy}\n",
    "config[\"multiagent\"] = {\n",
    "        'policy_graphs': policy_graph,\n",
    "        'policy_mapping_fn': tune.function(lambda agent_id: POLICY_ID),\n",
    "        'policies_to_train': [POLICY_ID]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-30 20:29:01,504\tWARNING json_reader.py:52 -- Treating input directory as glob pattern: /headless/rl_project/flow_codes/InverseRL/expert_sample/*.json\n",
      "2019-05-30 20:29:01,511\tINFO json_reader.py:65 -- Found 1 input files.\n",
      "2019-05-30 20:29:02,968\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "/opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "2019-05-30 20:29:05,275\tINFO policy_evaluator.py:728 -- Built policy map: {'default_policy': <ray.rllib.agents.ppo.ppo_policy_graph.PPOPolicyGraph object at 0x7fb9040c1c18>}\n",
      "2019-05-30 20:29:05,276\tINFO policy_evaluator.py:729 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fb9040c18d0>}\n",
      "2019-05-30 20:29:05,277\tINFO policy_evaluator.py:343 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fb8eee416a0>}\n",
      "2019-05-30 20:29:05,360\tWARNING worker.py:334 -- WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n"
     ]
    }
   ],
   "source": [
    "agent = GAILTrainer(config, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best.wgt', 'rb') as f:\n",
    "    weights = pickle.load(f)\n",
    "agent.set_weights(weights) # set weights to local evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = JsonWriter(\"./expert_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m 2019-05-30 20:29:23,095\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m 2019-05-30 20:29:23.096938: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m 2019-05-30 20:29:23,107\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m 2019-05-30 20:29:23.109556: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m 2019-05-30 20:29:23,137\tINFO policy_evaluator.py:311 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m 2019-05-30 20:29:23.139529: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m /opt/conda/envs/flow-latest/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m 2019-05-30 20:29:24,705\tINFO policy_evaluator.py:437 -- Generating sample batch of size 750\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m 2019-05-30 20:29:26,330\tINFO sampler.py:308 -- Raw obs from env: { 0: { 'flow_1.0': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.513),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m        'flow_1.1': np.ndarray((12,), dtype=float32, min=0.025, max=0.874, mean=0.198)}}\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m 2019-05-30 20:29:26,331\tINFO sampler.py:309 -- Info return from env: {0: {'flow_1.0': {}, 'flow_1.1': {}}}\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m 2019-05-30 20:29:26,331\tINFO sampler.py:407 -- Preprocessed obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.513)\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m 2019-05-30 20:29:26,331\tINFO sampler.py:411 -- Filtered obs: np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.513)\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m 2019-05-30 20:29:26,333\tINFO sampler.py:525 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'flow_1.0',\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                                   'obs': np.ndarray((12,), dtype=float32, min=0.0, max=1.0, mean=0.513),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                                   'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                       { 'data': { 'agent_id': 'flow_1.1',\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                                   'obs': np.ndarray((12,), dtype=float32, min=0.025, max=0.874, mean=0.198),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                                   'prev_action': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m 2019-05-30 20:29:26,333\tINFO tf_run_builder.py:89 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m 2019-05-30 20:29:26,861\tINFO sampler.py:552 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m { 'default_policy': ( np.ndarray((2, 1), dtype=float32, min=0.813, max=1.168, mean=0.991),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                       { 'action_prob': np.ndarray((2,), dtype=float32, min=0.337, max=0.745, mean=0.541),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                         'behaviour_logits': np.ndarray((2, 2), dtype=float32, min=-0.778, max=0.914, mean=-0.083),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                         'vf_preds': np.ndarray((2,), dtype=float32, min=-26.603, max=-6.667, mean=-16.635)})}\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m 2019-05-30 20:29:31,842\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m { 'flow_1.0': { 'data': { 'action_prob': np.ndarray((22,), dtype=float32, min=0.153, max=0.947, mean=0.687),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'actions': np.ndarray((22, 1), dtype=float32, min=0.485, max=1.771, mean=1.064),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'advantages': np.ndarray((22,), dtype=float32, min=1.137, max=15.25, mean=9.136),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'agent_index': np.ndarray((22,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'behaviour_logits': np.ndarray((22, 2), dtype=float32, min=-1.017, max=1.292, mean=0.093),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'dones': np.ndarray((22,), dtype=bool, min=0.0, max=1.0, mean=0.045),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'eps_id': np.ndarray((22,), dtype=int64, min=1967330184.0, max=1967330184.0, mean=1967330184.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'infos': np.ndarray((22,), dtype=object, head={'cost1': 0.6565134828220534, 'mean_vel': 18.771205544616368, 'cost2': 0.0, 'outflow': 445.5445544554455}),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'new_obs': np.ndarray((22, 12), dtype=float32, min=0.0, max=1.0, mean=0.544),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'obs': np.ndarray((22, 12), dtype=float32, min=0.0, max=1.0, mean=0.54),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_actions': np.ndarray((22, 1), dtype=float32, min=0.0, max=1.771, mean=0.997),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_rewards': np.ndarray((22,), dtype=float32, min=0.0, max=0.696, mean=0.661),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'rewards': np.ndarray((22,), dtype=float32, min=0.688, max=0.696, mean=0.692),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           't': np.ndarray((22,), dtype=int64, min=0.0, max=21.0, mean=10.5),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'unroll_id': np.ndarray((22,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'value_targets': np.ndarray((22,), dtype=float32, min=0.695, max=8.584, mean=5.53),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'vf_preds': np.ndarray((22,), dtype=float32, min=-6.691, max=-0.442, mean=-3.607)},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m   'flow_1.1': { 'data': { 'action_prob': np.ndarray((75,), dtype=float32, min=0.037, max=0.913, mean=0.608),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'actions': np.ndarray((75, 1), dtype=float32, min=-0.738, max=1.992, mean=0.749),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'advantages': np.ndarray((75,), dtype=float32, min=2.574, max=29.865, mean=21.582),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'agent_index': np.ndarray((75,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'behaviour_logits': np.ndarray((75, 2), dtype=float32, min=-0.861, max=1.107, mean=-0.013),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'dones': np.ndarray((75,), dtype=bool, min=0.0, max=1.0, mean=0.013),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'eps_id': np.ndarray((75,), dtype=int64, min=1967330184.0, max=1967330184.0, mean=1967330184.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'infos': np.ndarray((75,), dtype=object, head={'cost1': 0.6565134828220534, 'mean_vel': 18.771205544616368, 'cost2': 0.0, 'outflow': 445.5445544554455}),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'new_obs': np.ndarray((75, 12), dtype=float32, min=-0.029, max=1.0, mean=0.433),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'obs': np.ndarray((75, 12), dtype=float32, min=-0.029, max=1.0, mean=0.427),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_actions': np.ndarray((75, 1), dtype=float32, min=-0.738, max=1.992, mean=0.739),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_rewards': np.ndarray((75,), dtype=float32, min=0.0, max=0.698, mean=0.683),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'rewards': np.ndarray((75,), dtype=float32, min=0.686, max=0.698, mean=0.692),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           't': np.ndarray((75,), dtype=int64, min=0.0, max=74.0, mean=37.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'unroll_id': np.ndarray((75,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'value_targets': np.ndarray((75,), dtype=float32, min=0.671, max=8.608, mean=5.296),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'vf_preds': np.ndarray((75,), dtype=float32, min=-26.912, max=-1.88, mean=-16.286)},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m   'flow_1.2': { 'data': { 'action_prob': np.ndarray((111,), dtype=float32, min=0.031, max=1.231, mean=0.688),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'actions': np.ndarray((111, 1), dtype=float32, min=-0.475, max=1.801, mean=0.696),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'advantages': np.ndarray((111,), dtype=float32, min=5.498, max=33.834, mean=23.821),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'agent_index': np.ndarray((111,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'behaviour_logits': np.ndarray((111, 2), dtype=float32, min=-1.149, max=1.355, mean=-0.122),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'dones': np.ndarray((111,), dtype=bool, min=0.0, max=1.0, mean=0.009),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'eps_id': np.ndarray((111,), dtype=int64, min=1967330184.0, max=1967330184.0, mean=1967330184.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'infos': np.ndarray((111,), dtype=object, head={'cost1': 0.6816066826511952, 'mean_vel': 18.924058158725316, 'cost2': 0.0, 'outflow': 543.103448275862}),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'new_obs': np.ndarray((111, 12), dtype=float32, min=-0.029, max=1.0, mean=0.371),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'obs': np.ndarray((111, 12), dtype=float32, min=-0.029, max=1.0, mean=0.368),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_actions': np.ndarray((111, 1), dtype=float32, min=-0.475, max=1.801, mean=0.683),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_rewards': np.ndarray((111,), dtype=float32, min=0.0, max=0.697, mean=0.686),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'rewards': np.ndarray((111,), dtype=float32, min=0.687, max=0.697, mean=0.692),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           't': np.ndarray((111,), dtype=int64, min=15.0, max=125.0, mean=70.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'unroll_id': np.ndarray((111,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'value_targets': np.ndarray((111,), dtype=float32, min=-4.851, max=8.301, mean=3.504),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'vf_preds': np.ndarray((111,), dtype=float32, min=-35.662, max=-2.731, mean=-20.317)},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m   'flow_1.3': { 'data': { 'action_prob': np.ndarray((144,), dtype=float32, min=0.014, max=1.259, mean=0.758),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'actions': np.ndarray((144, 1), dtype=float32, min=-0.993, max=2.182, mean=0.737),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'advantages': np.ndarray((144,), dtype=float32, min=2.651, max=31.356, mean=23.749),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'agent_index': np.ndarray((144,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'behaviour_logits': np.ndarray((144, 2), dtype=float32, min=-1.195, max=1.325, mean=-0.111),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'dones': np.ndarray((144,), dtype=bool, min=0.0, max=1.0, mean=0.007),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'eps_id': np.ndarray((144,), dtype=int64, min=1967330184.0, max=1967330184.0, mean=1967330184.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'infos': np.ndarray((144,), dtype=object, head={'cost1': 0.6565226825887692, 'mean_vel': 18.116499159745523, 'cost2': 0.0, 'outflow': 883.4355828220858}),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'new_obs': np.ndarray((144, 12), dtype=float32, min=-0.341, max=1.0, mean=0.375),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'obs': np.ndarray((144, 12), dtype=float32, min=-0.341, max=1.0, mean=0.373),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_actions': np.ndarray((144, 1), dtype=float32, min=-0.993, max=2.182, mean=0.73),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_rewards': np.ndarray((144,), dtype=float32, min=0.0, max=0.7, mean=0.687),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'rewards': np.ndarray((144,), dtype=float32, min=0.685, max=0.7, mean=0.691),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           't': np.ndarray((144,), dtype=int64, min=62.0, max=205.0, mean=133.5),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'unroll_id': np.ndarray((144,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'value_targets': np.ndarray((144,), dtype=float32, min=-4.613, max=7.734, mean=2.388),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'vf_preds': np.ndarray((144,), dtype=float32, min=-34.672, max=-1.956, mean=-21.361)},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m   'flow_1.4': { 'data': { 'action_prob': np.ndarray((164,), dtype=float32, min=0.033, max=1.313, mean=0.738),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'actions': np.ndarray((164, 1), dtype=float32, min=-1.244, max=2.173, mean=0.614),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'advantages': np.ndarray((164,), dtype=float32, min=2.82, max=33.781, mean=25.6),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'agent_index': np.ndarray((164,), dtype=int64, min=4.0, max=4.0, mean=4.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'behaviour_logits': np.ndarray((164, 2), dtype=float32, min=-1.278, max=1.358, mean=-0.168),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'dones': np.ndarray((164,), dtype=bool, min=0.0, max=1.0, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'eps_id': np.ndarray((164,), dtype=int64, min=1967330184.0, max=1967330184.0, mean=1967330184.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'infos': np.ndarray((164,), dtype=object, head={'cost1': 0.6142178533238456, 'mean_vel': 16.959172984931605, 'cost2': 0.0, 'outflow': 1140.8450704225352}),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'new_obs': np.ndarray((164, 12), dtype=float32, min=-0.395, max=1.0, mean=0.293),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'obs': np.ndarray((164, 12), dtype=float32, min=-0.395, max=1.0, mean=0.291),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_actions': np.ndarray((164, 1), dtype=float32, min=-1.244, max=2.173, mean=0.611),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_rewards': np.ndarray((164,), dtype=float32, min=0.0, max=0.701, mean=0.688),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'rewards': np.ndarray((164,), dtype=float32, min=0.686, max=0.701, mean=0.692),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           't': np.ndarray((164,), dtype=int64, min=112.0, max=275.0, mean=193.5),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'unroll_id': np.ndarray((164,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'value_targets': np.ndarray((164,), dtype=float32, min=-9.15, max=6.806, mean=-1.599),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'vf_preds': np.ndarray((164,), dtype=float32, min=-40.926, max=-2.132, mean=-27.199)},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m   'flow_1.5': { 'data': { 'action_prob': np.ndarray((162,), dtype=float32, min=0.003, max=1.327, mean=0.72),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'actions': np.ndarray((162, 1), dtype=float32, min=-2.857, max=2.261, mean=0.442),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'advantages': np.ndarray((162,), dtype=float32, min=13.597, max=31.764, mean=26.591),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'agent_index': np.ndarray((162,), dtype=int64, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'behaviour_logits': np.ndarray((162, 2), dtype=float32, min=-1.318, max=1.355, mean=-0.262),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'dones': np.ndarray((162,), dtype=bool, min=0.0, max=1.0, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'eps_id': np.ndarray((162,), dtype=int64, min=1967330184.0, max=1967330184.0, mean=1967330184.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'infos': np.ndarray((162,), dtype=object, head={'cost1': 0.5022761363955273, 'mean_vel': 13.204631603430053, 'cost2': 0.0, 'outflow': 1080.0}),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'new_obs': np.ndarray((162, 12), dtype=float32, min=-0.536, max=1.0, mean=0.245),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'obs': np.ndarray((162, 12), dtype=float32, min=-0.536, max=1.0, mean=0.244),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_actions': np.ndarray((162, 1), dtype=float32, min=-2.857, max=2.261, mean=0.436),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_rewards': np.ndarray((162,), dtype=float32, min=0.0, max=0.708, mean=0.689),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'rewards': np.ndarray((162,), dtype=float32, min=0.685, max=0.708, mean=0.693),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           't': np.ndarray((162,), dtype=int64, min=163.0, max=324.0, mean=243.5),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'unroll_id': np.ndarray((162,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'value_targets': np.ndarray((162,), dtype=float32, min=-10.118, max=2.747, mean=-4.71),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'vf_preds': np.ndarray((162,), dtype=float32, min=-40.446, max=-12.907, mean=-31.301)},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m   'flow_1.6': { 'data': { 'action_prob': np.ndarray((102,), dtype=float32, min=0.061, max=1.499, mean=0.606),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'actions': np.ndarray((102, 1), dtype=float32, min=-1.838, max=1.718, mean=0.045),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'advantages': np.ndarray((102,), dtype=float32, min=25.72, max=34.879, mean=30.096),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'agent_index': np.ndarray((102,), dtype=int64, min=6.0, max=6.0, mean=6.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'behaviour_logits': np.ndarray((102, 2), dtype=float32, min=-1.351, max=0.646, mean=-0.325),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'dones': np.ndarray((102,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'eps_id': np.ndarray((102,), dtype=int64, min=1967330184.0, max=1967330184.0, mean=1967330184.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'infos': np.ndarray((102,), dtype=object, head={'cost1': 0.4280586276910177, 'mean_vel': 11.525355109924133, 'cost2': 0.0, 'outflow': 1476.0}),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'new_obs': np.ndarray((102, 12), dtype=float32, min=-0.536, max=1.0, mean=0.153),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'obs': np.ndarray((102, 12), dtype=float32, min=-0.536, max=1.0, mean=0.154),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_actions': np.ndarray((102, 1), dtype=float32, min=-1.838, max=1.718, mean=0.043),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_rewards': np.ndarray((102,), dtype=float32, min=0.0, max=0.704, mean=0.687),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'rewards': np.ndarray((102,), dtype=float32, min=0.685, max=0.704, mean=0.694),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           't': np.ndarray((102,), dtype=int64, min=223.0, max=324.0, mean=273.5),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'unroll_id': np.ndarray((102,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'value_targets': np.ndarray((102,), dtype=float32, min=-12.364, max=0.693, mean=-7.262),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'vf_preds': np.ndarray((102,), dtype=float32, min=-42.738, max=-28.666, mean=-37.358)},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                 'type': 'SampleBatch'},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m   'flow_1.7': { 'data': { 'action_prob': np.ndarray((40,), dtype=float32, min=0.057, max=0.714, mean=0.387),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'actions': np.ndarray((40, 1), dtype=float32, min=-1.842, max=1.651, mean=-0.206),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'advantages': np.ndarray((40,), dtype=float32, min=13.658, max=35.978, mean=29.07),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'agent_index': np.ndarray((40,), dtype=int64, min=7.0, max=7.0, mean=7.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'behaviour_logits': np.ndarray((40, 2), dtype=float32, min=-0.604, max=0.549, mean=-0.268),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'dones': np.ndarray((40,), dtype=bool, min=0.0, max=1.0, mean=0.025),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'eps_id': np.ndarray((40,), dtype=int64, min=1967330184.0, max=1967330184.0, mean=1967330184.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'infos': np.ndarray((40,), dtype=object, head={'cost1': 0.38637502245236366, 'mean_vel': 10.358772530696344, 'cost2': 0.0, 'outflow': 1548.0}),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'new_obs': np.ndarray((40, 12), dtype=float32, min=-0.101, max=1.0, mean=0.184),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'obs': np.ndarray((40, 12), dtype=float32, min=-0.101, max=1.0, mean=0.187),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_actions': np.ndarray((40, 1), dtype=float32, min=-1.842, max=1.651, mean=-0.19),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'prev_rewards': np.ndarray((40,), dtype=float32, min=0.0, max=0.704, mean=0.68),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'rewards': np.ndarray((40,), dtype=float32, min=0.69, max=0.704, mean=0.697),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           't': np.ndarray((40,), dtype=int64, min=285.0, max=324.0, mean=304.5),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'unroll_id': np.ndarray((40,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'value_targets': np.ndarray((40,), dtype=float32, min=-6.477, max=0.698, mean=-4.104),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                           'vf_preds': np.ndarray((40,), dtype=float32, min=-39.333, max=-18.612, mean=-33.174)},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m                 'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m 2019-05-30 20:29:41,315\tINFO policy_evaluator.py:474 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m { 'data': { 'action_prob': np.ndarray((1779,), dtype=float32, min=0.002, max=1.536, mean=0.712),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             'actions': np.ndarray((1779, 1), dtype=float32, min=-2.857, max=2.261, mean=0.548),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             'advantages': np.ndarray((1779,), dtype=float32, min=0.905, max=35.978, mean=23.493),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             'agent_index': np.ndarray((1779,), dtype=int64, min=0.0, max=7.0, mean=3.445),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             'behaviour_logits': np.ndarray((1779, 2), dtype=float32, min=-1.352, max=1.358, mean=-0.176),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             'dones': np.ndarray((1779,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             'eps_id': np.ndarray((1779,), dtype=int64, min=871746833.0, max=1967330184.0, mean=1799563771.887),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             'infos': np.ndarray((1779,), dtype=object, head={'cost1': 0.6565134828220534, 'mean_vel': 18.771205544616368, 'cost2': 0.0, 'outflow': 445.5445544554455}),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             'new_obs': np.ndarray((1779, 12), dtype=float32, min=-0.536, max=1.0, mean=0.332),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             'obs': np.ndarray((1779, 12), dtype=float32, min=-0.536, max=1.0, mean=0.33),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             'prev_actions': np.ndarray((1779, 1), dtype=float32, min=-2.857, max=2.261, mean=0.539),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             'prev_rewards': np.ndarray((1779,), dtype=float32, min=0.0, max=0.708, mean=0.685),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             'rewards': np.ndarray((1779,), dtype=float32, min=0.685, max=0.708, mean=0.693),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             't': np.ndarray((1779,), dtype=int64, min=0.0, max=324.0, mean=153.379),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             'unroll_id': np.ndarray((1779,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             'value_targets': np.ndarray((1779,), dtype=float32, min=-26.776, max=8.795, mean=0.895),\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m             'vf_preds': np.ndarray((1779,), dtype=float32, min=-42.738, max=-0.437, mean=-22.598)},\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-30 20:29:42,467\tINFO json_writer.py:97 -- Writing to new output file <_io.TextIOWrapper name='/headless/rl_project/flow_codes/InverseRL/expert_sample/output-2019-05-30_20-29-42_worker-0_0.json' mode='w' encoding='UTF-8'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Warning: Teleporting vehicle 'flow_0.34'; collision with vehicle 'flow_2.3', lane=':center_1_0', gap=-1.00, time=126.20 stage=move.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Warning: Vehicle 'flow_0.34' ends teleporting on edge 'center', time 126.20.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Warning: Teleporting vehicle 'flow_0.32'; collision with vehicle 'flow_2.3', lane=':center_1_0', gap=-1.00, time=123.00 stage=move.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=367)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=353)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Success.\n",
      "\u001b[2m\u001b[36m(pid=338)\u001b[0m Loading configuration... done.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    samples = agent.sample(agent.train_batch_size)\n",
    "    writer.write(sample_batch=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-30 20:31:25,864\tWARNING json_reader.py:52 -- Treating input directory as glob pattern: /headless/rl_project/flow_codes/InverseRL/expert_sample/*.json\n",
      "2019-05-30 20:31:25,866\tINFO json_reader.py:65 -- Found 1 input files.\n"
     ]
    }
   ],
   "source": [
    "reader = JsonReader(\"./expert_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5247"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = reader.next()\n",
    "sample.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5247"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5247"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.slice(0, sample.count).count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
