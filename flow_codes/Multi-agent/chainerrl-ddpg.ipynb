{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "\n",
    "- [ ] add reward scaling\n",
    "- [ ] override train_agent_batch_with_evaluation\n",
    "- [ ] support calllable object(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()  # NOQA\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "from collections import deque\n",
    "\n",
    "import chainer\n",
    "from chainer import optimizers\n",
    "import gym\n",
    "from gym import spaces\n",
    "import gym.wrappers\n",
    "import numpy as np\n",
    "\n",
    "import chainerrl\n",
    "from chainerrl.agents.ddpg import DDPG\n",
    "from chainerrl.agents.ddpg import DDPGModel\n",
    "from chainerrl import experiments\n",
    "from chainerrl import explorers\n",
    "from chainerrl import misc\n",
    "from chainerrl import policy\n",
    "from chainerrl import q_functions\n",
    "from chainerrl import replay_buffer\n",
    "\n",
    "from flow.multiagent_envs import MultiWaveAttenuationMergePOEnv\n",
    "from flow.scenarios import MergeScenario\n",
    "from flow.utils.registry import make_create_env\n",
    "\n",
    "benchmark_name = 'multi_merge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = None\n",
    "seed = 0\n",
    "final_exploration_steps = 10**6\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 1e-3\n",
    "steps = 10 ** 7\n",
    "n_hidden_channels = 300\n",
    "n_hidden_layers = 3\n",
    "replay_start_size = 5000\n",
    "n_update_times = 1\n",
    "target_update_interval = 1\n",
    "target_update_method = 'soft'\n",
    "soft_update_tau = 1e-2\n",
    "update_interval = 4\n",
    "eval_n_runs = 100\n",
    "eval_interval = 10**5\n",
    "gamma = 0.995\n",
    "minibatch_size = 200\n",
    "use_bn = True\n",
    "reward_scale_factor = 1e-2\n",
    "return_window_size = 100\n",
    "step_offset = 0\n",
    "log_interval = 5\n",
    "num_envs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed used in ChainerRL\n",
    "misc.set_random_seed(seed)\n",
    "\n",
    "# Set different random seeds for different subprocesses.\n",
    "# If seed=0 and processes=4, subprocess seeds are [0, 1, 2, 3].\n",
    "# If seed=1 and processes=4, subprocess seeds are [4, 5, 6, 7].\n",
    "process_seeds = np.arange(num_envs) + seed * num_envs\n",
    "assert process_seeds.max() < 2 ** 32\n",
    "\n",
    "outdir = experiments.prepare_output_dir('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark  = __import__(\n",
    "    \"flow.benchmarks.%s\" % benchmark_name, fromlist=[\"flow_params\"])\n",
    "flow_params = benchmark.flow_params\n",
    "HORIZON = flow_params['env'].horizon\n",
    "\n",
    "create_env, env_name = make_create_env(params=flow_params, version=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(create_env):\n",
    "    def _thunk():\n",
    "        env = create_env()\n",
    "        return env\n",
    "    return _thunk\n",
    "\n",
    "def make_batch_env(test):\n",
    "    return chainerrl.envs.MultiprocessVectorEnv(\n",
    "        [make_env(create_env) for i in range(num_envs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_env = create_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep_limit = flow_params[\"env\"].horizon\n",
    "obs_size = np.asarray(sample_env.observation_space.shape).prod()\n",
    "action_space = sample_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = np.asarray(action_space.shape).prod()\n",
    "if use_bn:\n",
    "    q_func = q_functions.FCBNLateActionSAQFunction(\n",
    "        obs_size, action_size,\n",
    "        n_hidden_channels=n_hidden_channels,\n",
    "        n_hidden_layers=n_hidden_layers,\n",
    "        normalize_input=True)\n",
    "    pi = policy.FCBNDeterministicPolicy(\n",
    "        obs_size, action_size=action_size,\n",
    "        n_hidden_channels=n_hidden_channels,\n",
    "        n_hidden_layers=n_hidden_layers,\n",
    "        min_action=action_space.low, max_action=action_space.high,\n",
    "        bound_action=True,\n",
    "        normalize_input=True)\n",
    "else:\n",
    "    q_func = q_functions.FCSAQFunction(\n",
    "        obs_size, action_size,\n",
    "        n_hidden_channels=n_hidden_channels,\n",
    "        n_hidden_layers=n_hidden_layers)\n",
    "    pi = policy.FCDeterministicPolicy(\n",
    "        obs_size, action_size=action_size,\n",
    "        n_hidden_channels=n_hidden_channels,\n",
    "        n_hidden_layers=n_hidden_layers,\n",
    "        min_action=action_space.low, max_action=action_space.high,\n",
    "        bound_action=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DDPGModel(q_func=q_func, policy=pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DDPGModel(q_func=q_func, policy=pi)\n",
    "opt_a = optimizers.Adam(alpha=actor_lr)\n",
    "opt_c = optimizers.Adam(alpha=critic_lr)\n",
    "opt_a.setup(model['policy'])\n",
    "opt_c.setup(model['q_function'])\n",
    "opt_a.add_hook(chainer.optimizer.GradientClipping(1.0), 'hook_a')\n",
    "opt_c.add_hook(chainer.optimizer.GradientClipping(1.0), 'hook_c')\n",
    "\n",
    "rbuf = replay_buffer.ReplayBuffer(5 * 10 ** 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bufferには\n",
    "1. state: self.batch_last_obs[i]\n",
    "2. action: self.batch_last_action[i]\n",
    "3. reward: batch_reward[i]\n",
    "4. next_state: batch_obs[i]\n",
    "5. next_action: action or None\n",
    "6. is_state_terminal: batch_done\n",
    "\n",
    "が追加される\n",
    "\n",
    "一方、envからは\n",
    "1. batch_obs: [{\"key\": \"value\"}, {\"key\": nparray}]\n",
    "2. batch_rew: [{\"key\": \"value\"}, {\"key\": float}]\n",
    "3. batch_done: [{\"key\": \"value\"}, {\"key\": bool}]\n",
    "\n",
    "が追加される\n",
    "\n",
    "agentが可能なのは\n",
    "1. batch_act: obs[np.array, ...]\n",
    "2. batch_train_act: obs[np.array, ...]\n",
    "3. batch_obs_and_train: obs[np.array, ...], rew[float, ...], done[bool, ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_MA(DDPG):\n",
    "    def batch_act(self, batch_obs):\n",
    "        \"\"\"Select a batch of actions for evaluation.\n",
    "        Args:\n",
    "            batch_obs (Sequence of ~object): Observations.\n",
    "        Returns:\n",
    "            Sequence of ~object: Actions.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_actions = []\n",
    "        for env_obs in batch_obs:\n",
    "            keys, obss = list(env_obs.keys()), list(env_obs.values())\n",
    "            obss = [obs.astype(np.float32) for obs in obss]\n",
    "            \n",
    "            with chainer.using_config('train', False), chainer.no_backprop_mode():\n",
    "                batch_xs = self.batch_states(obss, self.xp, self.phi)\n",
    "                batch_action = self.policy(batch_xs).sample()\n",
    "                # Q is not needed here, but log it just for information\n",
    "                q = self.q_function(batch_xs, batch_action)\n",
    "\n",
    "            batch_actions.append({key:action for key, action in zip(keys, batch_action)})\n",
    "            \n",
    "            # Update stats\n",
    "            self.average_q *= self.average_q_decay\n",
    "            self.average_q += (1 - self.average_q_decay) * float(\n",
    "                q.array.mean(axis=0))\n",
    "            \n",
    "        self.logger.debug('t:%s a:%s q:%s',\n",
    "                          self.t, batch_action.array[0], q.array)\n",
    "        return batch_actions\n",
    "\n",
    "    def batch_act_and_train(self, batch_obs):\n",
    "        \"\"\"Select a batch of actions for training.\n",
    "        Args:\n",
    "            batch_obs (Sequence of ~object): Observations.\n",
    "        Returns:\n",
    "            Sequence of ~object: Actions.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_greedy_action = self.batch_act(batch_obs)\n",
    "        batch_action = [\n",
    "            self.explorer.select_action(\n",
    "                self.t, lambda: batch_greedy_action[i])\n",
    "            for i in range(len(batch_greedy_action))]\n",
    "\n",
    "        self.batch_last_obs = list(batch_obs)\n",
    "        self.batch_last_action = list(batch_action)\n",
    "\n",
    "        return batch_action\n",
    "\n",
    "    def batch_observe_and_train(\n",
    "            self, batch_obs, batch_reward, batch_done, batch_reset):\n",
    "        \"\"\"Observe a batch of action consequences for training.\n",
    "        Args:\n",
    "            batch_obs (Sequence of ~object): Observations.\n",
    "            batch_reward (Sequence of float): Rewards.\n",
    "            batch_done (Sequence of boolean): Boolean values where True\n",
    "                indicates the current state is terminal.\n",
    "            batch_reset (Sequence of boolean): Boolean values where True\n",
    "                indicates the current episode will be reset, even if the\n",
    "                current state is not terminal.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # when a new car is added to the highway\n",
    "        if len(batch_obs) > len(self.batch_last_obs):\n",
    "            self.batch_last_obs.append(None)\n",
    "            self.batch_last_action.append(None)\n",
    "            \n",
    "        for i in range(len(batch_obs)):\n",
    "            self.t += 1\n",
    "            # Update the target network\n",
    "            if self.t % self.target_update_interval == 0:\n",
    "                self.sync_target_network()\n",
    "            if self.batch_last_obs[i] is not None:\n",
    "                assert self.batch_last_action[i] is not None\n",
    "                # Add a transition to the replay buffer\n",
    "                self.replay_buffer.append(\n",
    "                    state=self.batch_last_obs[i],\n",
    "                    action=self.batch_last_action[i],\n",
    "                    reward=batch_reward[i],\n",
    "                    next_state=batch_obs[i],\n",
    "                    next_action=None,\n",
    "                    is_state_terminal=batch_done[i],\n",
    "                )\n",
    "                if batch_reset[i] or batch_done[i]:\n",
    "                    self.batch_last_obs[i] = None\n",
    "            self.replay_updater.update_if_necessary(self.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ou_sigma = (action_space.high - action_space.low) * 0.2\n",
    "explorer = explorers.AdditiveOU(sigma=ou_sigma)\n",
    "agent = DDPG_MA(model, opt_a, opt_c, rbuf, gamma=gamma,\n",
    "             explorer=explorer, replay_start_size=replay_start_size,\n",
    "             target_update_method=target_update_method,\n",
    "             target_update_interval=target_update_interval,\n",
    "             update_interval=update_interval,\n",
    "             soft_update_tau=soft_update_tau,\n",
    "             n_times_update=n_update_times,\n",
    "             gpu=gpu, minibatch_size=minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'flow_1.1': variable([0.08803371]), 'flow_1.2': variable([0.03473603])},\n",
       " {'flow_1.1': variable([0.08545651]), 'flow_1.2': variable([0.034552])}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.batch_act(batch_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = make_batch_env(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. experiments.train_agent_batch_with_evaluation()\n",
    "\n",
    "2. contents in train_agent_batch\n",
    "\n",
    "    1. actions = agent.batch_act_and_train(obss)\n",
    "    2. env.step(actions)\n",
    "    3. compute resets\n",
    "    4. agent.batch_observe_and_train(obss, rs, dones, resets): in this env, done==True when horizon is reached. So resets is same as dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_act_and_train_MA(batch_obs):\n",
    "    \"\"\"\n",
    "    do batch_act_and_train in a multi-agent batch env\n",
    "    batch is by the agents in an env, and for loop by the # of env\n",
    "    \"\"\"\n",
    "    batch_actions = []\n",
    "    for batch in batch_obs:\n",
    "        keys, obss = list(batch.keys()), list(batch.values())\n",
    "        obss = [obs.astype(np.float32) for obs in obss]\n",
    "        actions = agent.batch_act_and_train(obss)\n",
    "        batch_actions.append({key:action for key, action in zip(keys, actions)})\n",
    "    return batch_actions\n",
    "\n",
    "def batch_observe_and_train_MA(batch_obs, batch_rs, batch_dones):\n",
    "    \"\"\"\n",
    "    do batch_observe_and_train in a multi-agent batch env\n",
    "    batch is by the agents in an env, and for loop by the # of env\n",
    "    \"\"\"\n",
    "    # train: the agents in a same env is batched, and for_loop by the env num\n",
    "    for obss, rss, doness in zip(batch_obs, batch_rs, batch_dones):\n",
    "        keys, obss = list(obss.keys()), list(obss.values())\n",
    "        obss = [obs.astype(np.float32) for obs in obss]\n",
    "        rss = [float(rs) for rs in rss.values()]\n",
    "        doness = list([doness[key] for key in keys])  # without __all__\n",
    "        resetss = doness\n",
    "        print(\"last {}\".format(len(agent.batch_last_obs)))\n",
    "        print(\"current {}\".format(len(obss)))\n",
    "        agent.batch_observe_and_train(obss, rss, doness, resetss)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "recent_returns = deque(maxlen=return_window_size)\n",
    "\n",
    "num_envs = env.num_envs\n",
    "episode_r = np.zeros(num_envs, dtype=np.float64)\n",
    "episode_idx = np.zeros(num_envs, dtype='i')\n",
    "episode_len = np.zeros(num_envs, dtype='i')\n",
    "\n",
    "# o_0, r_0\n",
    "batch_obs = env.reset()\n",
    "rs = np.zeros(num_envs, dtype='f')\n",
    "\n",
    "t = step_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'flow_1.10': array([-0.40924096], dtype=float32), 'flow_1.9': array([-0.20380986], dtype=float32), 'flow_1.12': array([-2.7184482], dtype=float32), 'flow_1.11': array([-3.8482678], dtype=float32)}, {'flow_1.2': array([-1.7086694], dtype=float32), 'flow_1.1': array([-3.1255903], dtype=float32)}]\n",
      "last 2\n",
      "current 4\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-1851580e886d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Agent observes the consequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mbatch_observe_and_train_MA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_rs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mepisode_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-153-562e21b97af7>\u001b[0m in \u001b[0;36mbatch_observe_and_train_MA\u001b[0;34m(batch_obs, batch_rs, batch_dones)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"last {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_last_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"current {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_observe_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresetss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-131-ddf5957133a1>\u001b[0m in \u001b[0;36mbatch_observe_and_train\u001b[0;34m(self, batch_obs, batch_reward, batch_done, batch_reset)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_update_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_target_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_last_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_last_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;31m# Add a transition to the replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "    # a_t\n",
    "    batch_actions = batch_act_and_train_MA(batch_obs)\n",
    "    print(batch_actions)\n",
    "    # o_{t+1}, r_{t+1}\n",
    "    batch_obs, batch_rs, batch_dones, batch_infos = env.step(batch_actions)\n",
    "    rs = [np.mean(list(rss.values())) for rss in batch_rs] # each env mean reward\n",
    "    episode_r += rs\n",
    "    episode_len += 1\n",
    "\n",
    "    # mask for reset the env(when collision or horizon)\n",
    "    batch_reset = [done[\"__all__\"] for done in batch_dones]\n",
    "    not_batch_reset = np.logical_not(batch_reset)  # doesn't reset when True\n",
    "\n",
    "    # Agent observes the consequences\n",
    "    batch_observe_and_train_MA(batch_obs, batch_rs, batch_dones)\n",
    "\n",
    "    episode_idx += batch_reset\n",
    "    recent_returns.extend(episode_r[batch_reset])\n",
    "\n",
    "    for _ in range(num_envs):\n",
    "        t += 1\n",
    "\n",
    "    # logger should be here\n",
    "    # evaluator should be here\n",
    "\n",
    "    if t >= steps:\n",
    "        break\n",
    "\n",
    "    # Start new episodes if needed\n",
    "    episode_r[batch_reset] = 0\n",
    "    episode_len[batch_reset] = 0\n",
    "    batch_obs = env.reset(not_batch_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0], dtype=int32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_reset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
