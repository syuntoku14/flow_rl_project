{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark: figureeight0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch import optim\n",
    "\n",
    "import scipy.stats as ss\n",
    "import tensorboardX\n",
    "from tensorboardX import SummaryWriter\n",
    "import gym\n",
    "\n",
    "import ray\n",
    "from ray.experimental.queue import Queue\n",
    "\n",
    "# sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "# sys.path = list(set(sys.path))\n",
    "\n",
    "benchmark_name = 'figureeight0'\n",
    "print('benchmark: {}'.format(benchmark_name))\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "benchmark = __import__(\n",
    "    \"flow.benchmarks.%s\" % benchmark_name, fromlist=[\"flow_params\"])\n",
    "flow_params = benchmark.flow_params\n",
    "HORIZON = flow_params['env'].horizon\n",
    "\n",
    "def make_env(create_env):\n",
    "    def _thunk():\n",
    "        env = create_env()\n",
    "        return env\n",
    "    return _thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-04-10_06-53-52_1594/logs.\n",
      "Waiting for redis server at 127.0.0.1:49070 to respond...\n",
      "Waiting for redis server at 127.0.0.1:57238 to respond...\n",
      "Starting the Plasma object store with 13.355121049 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '169.237.32.118',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-04-10_06-53-52_1594/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-04-10_06-53-52_1594/sockets/raylet'],\n",
       " 'redis_address': '169.237.32.118:49070',\n",
       " 'webui_url': ''}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=3, include_webui=False, ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    '''\n",
    "    Neural network for continuous action space\n",
    "    '''\n",
    "    def __init__(self, num_inputs, num_outputs, fcnet_hiddens, std=0.0):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        last_layer_size = num_inputs\n",
    "        layers = []\n",
    "        \n",
    "        for size in fcnet_hiddens:\n",
    "            layers.append(nn.Linear(last_layer_size, size))\n",
    "            layers.append(nn.ReLU())\n",
    "            last_layer_size = size\n",
    "            \n",
    "        layers.append(nn.Linear(fcnet_hiddens[-1], num_outputs))\n",
    "        \n",
    "        self.actor = nn.Sequential(*layers)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        action_logits = self.actor(x.float())\n",
    "        return action_logits\n",
    "\n",
    "    \n",
    "def sample_noise(model):\n",
    "    '''\n",
    "    Sample noise for each parameter of the neural net\n",
    "    '''\n",
    "    nn_noise = []\n",
    "    for n in model.parameters():\n",
    "        noise = np.random.normal(size=n.data.numpy().shape)\n",
    "        nn_noise.append(noise)\n",
    "    return np.array(nn_noise)\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class Evaluator:\n",
    "    def __init__(self, fcnet_hiddens, create_env):\n",
    "        self.env = create_env()\n",
    "        num_inputs  = env.observation_space.shape[0]\n",
    "        num_outputs = env.action_space.shape[0]\n",
    "        self.max_accel = env.action_space.high[0]\n",
    "        self.min_accel = env.action_space.low[0]\n",
    "        self.model = NeuralNetwork(num_inputs, num_outputs, fcnet_hiddens)\n",
    "\n",
    "    def evaluate_neuralnet(self):\n",
    "        '''\n",
    "        Evaluate an agent running it in the environment and computing the total reward\n",
    "        '''\n",
    "        obs = self.env.reset()\n",
    "        game_reward = 0\n",
    "\n",
    "        while True:\n",
    "            # Output of the neural net\n",
    "            net_output = self.model(torch.tensor(obs))\n",
    "            # the action is the value clipped returned by the neural_net\n",
    "            action = torch.clamp(net_output, self.min_accel, self.max_accel)\n",
    "            new_obs, reward, done, _ = self.env.step(action.detach().cpu().numpy())\n",
    "            obs = new_obs\n",
    "\n",
    "            game_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return game_reward\n",
    "\n",
    "    def evaluate_noisy_net(self, noise):\n",
    "        '''\n",
    "        Evaluate a noisy agent by adding the noise to the plain agent\n",
    "        '''\n",
    "        old_dict = self.model.state_dict()\n",
    "\n",
    "        # add the noise to each parameter of the NN\n",
    "        for n, p in zip(noise, self.model.parameters()):\n",
    "            p.data += torch.FloatTensor(n * STD_NOISE)\n",
    "\n",
    "        # evaluate the agent with the noise\n",
    "        reward = self.evaluate_neuralnet()\n",
    "        # load the previous paramater (the ones without the noise)\n",
    "        self.model.load_state_dict(old_dict)\n",
    "\n",
    "        return reward       \n",
    "    \n",
    "    def evaluate_seed_reward(self, act_params):\n",
    "        # load the actor params\n",
    "        self.model.load_state_dict(act_params)\n",
    "\n",
    "        # get a random seed\n",
    "        seed = np.random.randint(1e6)\n",
    "        # set the new seed\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        noise = sample_noise(self.model)\n",
    "\n",
    "        pos_rew = self.evaluate_noisy_net(noise)\n",
    "        # Mirrored sampling\n",
    "        neg_rew = self.evaluate_noisy_net(-noise)\n",
    "\n",
    "        return [[pos_rew, neg_rew], seed]\n",
    "    \n",
    "    # def queue_get_put_results(self, params_queue, output_queue):\n",
    "    #     while True:\n",
    "    #         act_params = params_queue.get()\n",
    "    #         if act_params != None:\n",
    "    #             res = self.evaluate_seed_reward(act_params)\n",
    "    #             output_queue.put(res)\n",
    "    #         else:\n",
    "    #             break\n",
    "    \n",
    "    \n",
    "def make_batch_results(model, envs, batch_size):\n",
    "    batch_noise = []\n",
    "    batch_reward = []\n",
    "    task_dict = {ev:ev.evaluate_seed_reward.remote(model.state_dict()) for ev in envs}\n",
    "    for i in range(batch_size):\n",
    "        ready_id, yet_id = ray.wait(list(task_dict.values()))\n",
    "        ready_id = ready_id[0]   \n",
    "\n",
    "        p_rews, p_seed = ray.get(ready_id)\n",
    "        np.random.seed(p_seed)\n",
    "        noise = sample_noise(model)\n",
    "        batch_noise.append(noise)\n",
    "        batch_noise.append(-noise)\n",
    "\n",
    "        batch_reward.append(p_rews[0])\n",
    "        batch_reward.append(p_rews[1])\n",
    "\n",
    "        for ev, id_ in task_dict.items():\n",
    "            if id_ == ready_id:\n",
    "                task_dict[ev] = ev.evaluate_seed_reward.remote(model.state_dict())\n",
    "    \n",
    "    return batch_noise, batch_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_rank(rewards):\n",
    "    '''\n",
    "    Rank the rewards and normalize them.\n",
    "    '''\n",
    "    ranked = ss.rankdata(rewards)\n",
    "    norm = (ranked - 1) / (len(ranked) - 1)\n",
    "    norm -= 0.5\n",
    "    return norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ASY_ES_figureeight0_10_16.0.47_0.05_5_0.01_3\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "STD_NOISE = 0.05\n",
    "BATCH_SIZE = 5\n",
    "LEARNING_RATE = 0.01\n",
    "MAX_ITERATIONS = 3\n",
    "MAX_WORKERS = 3\n",
    "fcnet_hiddens = [100, 50, 25]\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date_time = \"{}_{}.{}.{}\".format(now.day, now.hour, now.minute, now.second)\n",
    "# Writer name\n",
    "writer_name = 'ASY_ES_{}_{}_{}_{}_{}_{}'.format(benchmark_name, date_time, str(STD_NOISE), str(BATCH_SIZE), str(LEARNING_RATE), str(MAX_ITERATIONS), str(MAX_WORKERS))\n",
    "print('Name:', writer_name)\n",
    "writer = SummaryWriter(log_dir='../content/runs/'+writer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Starting SUMO on port 37791\n"
     ]
    }
   ],
   "source": [
    "# Create Environment\n",
    "create_env, env_name = make_create_env(params=flow_params, version=0)\n",
    "envs = [Evaluator.remote(fcnet_hiddens, create_env) for i in range(MAX_WORKERS)]\n",
    "env = create_env()\n",
    "num_inputs  = env.observation_space.shape[0]\n",
    "num_outputs = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96 ms, sys: 112 ms, total: 208 ms\n",
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_noise, batch_reward = make_batch_results(actor, envs, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Mean: 437.41 Max: 490.44 Time: 27.64\n",
      "1 Mean: 382.67 Max: 522.5 Time: 36.06\n",
      "2 Mean: 441.18 Max: 506.8 Time: 32.96\n"
     ]
    }
   ],
   "source": [
    "# Initialize the agent\n",
    "actor = NeuralNetwork(num_inputs, num_outputs, fcnet_hiddens)\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.Adam(actor.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# Execute the main loop MAX_ITERATIONS times\n",
    "for n_iter in range(MAX_ITERATIONS):\n",
    "    it_time = time.time()\n",
    "\n",
    "    batch_noise, batch_reward = make_batch_results(actor, envs, BATCH_SIZE)\n",
    "\n",
    "    # Print some stats\n",
    "    print(n_iter, 'Mean:',np.round(np.mean(batch_reward), 2), 'Max:', np.round(np.max(batch_reward), 2), 'Time:', np.round(time.time()-it_time, 2))\n",
    "    writer.add_scalar('reward', np.mean(batch_reward), n_iter)\n",
    "\n",
    "    # Rank the reward and normalize it\n",
    "    batch_reward = normalized_rank(batch_reward)\n",
    "\n",
    "\n",
    "    th_update = []\n",
    "    optimizer.zero_grad()\n",
    "    # for each actor's parameter, and for each noise in the batch, update it by the reward * the noise value\n",
    "    for idx, p in enumerate(actor.parameters()):\n",
    "        upd_weights = np.zeros(p.data.shape)\n",
    "\n",
    "        for n,r in zip(batch_noise, batch_reward):\n",
    "            upd_weights += r*n[idx]\n",
    "\n",
    "        upd_weights = upd_weights / (BATCH_SIZE*STD_NOISE)\n",
    "        # put the updated weight on the gradient variable so that afterwards the optimizer will use it\n",
    "        p.grad = torch.FloatTensor( -upd_weights)\n",
    "        th_update.append(np.mean(upd_weights))\n",
    "\n",
    "    # Optimize the actor's NN\n",
    "    optimizer.step()\n",
    "\n",
    "    writer.add_scalar('loss', np.mean(th_update), n_iter)\n",
    "\n",
    "# tensorboard --logdir content/runs --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
