{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import ray\n",
    "\n",
    "import os, sys, pickle, time, math\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# ----------\n",
    "# Pytorch\n",
    "import torch\n",
    "dtype = torch.float64\n",
    "torch.set_default_dtype(dtype)\n",
    "device = torch.device('cuda', index=args.gpu_index) \\\n",
    "    if torch.cuda.is_available() else torch.device('cpu')\n",
    "# ----------\n",
    "# flow\n",
    "from flow.utils.registry import make_create_env\n",
    "benchmark_name = 'multi_merge'\n",
    "benchmark = __import__(\n",
    "    \"flow.benchmarks.%s\" % benchmark_name, fromlist=[\"flow_params\"])\n",
    "flow_params = benchmark.buffered_obs_flow_params\n",
    "\n",
    "# ----------\n",
    "# PyTorch-RL\n",
    "from models.mlp_policy import MultiAgentPolicy\n",
    "from models.mlp_critic import Value\n",
    "from core.a2c import a2c_step\n",
    "from core.common import estimate_advantages\n",
    "from core.multi_agent import MultiAgent\n",
    "from utils.remote_vector_env import MultiAgentVecEnv, dict_to_array\n",
    "from utils.replay_memory import MultiAgentMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-03_21-32-12_30/logs.\n",
      "Waiting for redis server at 127.0.0.1:35324 to respond...\n",
      "Waiting for redis server at 127.0.0.1:20943 to respond...\n",
      "Starting the Plasma object store with 3.2789331959999997 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8888/notebooks/ray_ui.ipynb?token=b1cbcb449c52e1eb09a8c097007e9bff2522911e4e72fd0c\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '168.150.112.131',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-05-03_21-32-12_30/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-05-03_21-32-12_30/sockets/raylet'],\n",
       " 'redis_address': '168.150.112.131:35324',\n",
       " 'webui_url': 'http://localhost:8888/notebooks/ray_ui.ipynb?token=b1cbcb449c52e1eb09a8c097007e9bff2522911e4e72fd0c'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Starting SUMO on port 48028\n"
     ]
    }
   ],
   "source": [
    "create_env, env_name = make_create_env(params=flow_params, version=0)\n",
    "sample_env = create_env()\n",
    "state_dim = sample_env.observation_space.shape[0]\n",
    "action_dim = sample_env.action_space.shape[0]\n",
    "env = MultiAgentVecEnv(create_env, num_envs=2, remote_env_batch_wait_ms=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define actor and critic\n",
    "policy_net = MultiAgentPolicy(state_dim, action_dim, activation='relu')\n",
    "value_net = Value(state_dim)\n",
    "optimizer_policy = torch.optim.Adam(policy_net.parameters(), lr=3e-4)\n",
    "optimizer_value = torch.optim.Adam(value_net.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Launching env 0 in remote actor\n",
      "Launching env 1 in remote actor\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = MultiAgent(env, policy_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.collect_samples(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in agent.batch_generator(600):\n",
    "    states = torch.tensor(batch.state).float().to(device)\n",
    "    actions = torch.tensor(batch.action).float().to(device)\n",
    "    rewards = torch.tensor(batch.reward).float().to(device)\n",
    "    masks = torch.tensor(batch.mask).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        values = value_net(states)\n",
    "        fixed_log_probs = policy_net.get_log_prob(states, actions)\n",
    "    advantages, returns = estimate_advantages(rewards, masks, values, 0.99, 0.95)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"perform mini-batch PPO update\"\"\"\n",
    "optim_iter_num = int(math.ceil(states.shape[0] / optim_batch_size))\n",
    "for _ in range(optim_epochs):\n",
    "    perm = np.arange(states.shape[0])\n",
    "    np.random.shuffle(perm)\n",
    "    perm = LongTensor(perm).to(device)\n",
    "\n",
    "    states, actions, returns, advantages, fixed_log_probs = \\\n",
    "        states[perm].clone(), actions[perm].clone(), returns[perm].clone(), advantages[perm].clone(), fixed_log_probs[perm].clone()\n",
    "\n",
    "    for i in range(optim_iter_num):\n",
    "        ind = slice(i * optim_batch_size, min((i + 1) * optim_batch_size, states.shape[0]))\n",
    "        states_b, actions_b, advantages_b, returns_b, fixed_log_probs_b = \\\n",
    "            states[ind], actions[ind], advantages[ind], returns[ind], fixed_log_probs[ind]\n",
    "\n",
    "        ppo_step(policy_net, value_net, optimizer_policy, optimizer_value, 1, states_b, actions_b, returns_b,\n",
    "                 advantages_b, fixed_log_probs_b, args.clip_epsilon, args.l2_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <Seeding should be here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: 0 is reseted at 699\n",
      "env: 1 is reseted at 699\n"
     ]
    }
   ],
   "source": [
    "agent.memory = MultiAgentMemory()\n",
    "state = agent.env.reset()\n",
    "for i in range(1000):\n",
    "    with torch.no_grad():\n",
    "        action = agent.policy_net.select_action(state)\n",
    "    next_state, rew, done, info = agent.env.step(action)\n",
    "    # if __all__ True, reset\n",
    "    need_reset = list(map(lambda value:1 if value[\"__all__\"] else 0,\\\n",
    "                                  list(done.values())))\n",
    "    action, state, listed_next_state, rew, done, info, id_list = \\\n",
    "            dict_to_array(action, state, next_state, rew, done, info)\n",
    "    mask = list(map(lambda d: 0 if d else 1, done))    \n",
    "    agent.memory.push(state, action, mask, listed_next_state, rew)\n",
    "    state = next_state\n",
    "    # reset if need_reset is 1\n",
    "    reseted_state = env.reset(need_reset)\n",
    "    for key, value in reseted_state.items():\n",
    "        state[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_advantages(rewards, masks, values, gamma, tau):\n",
    "    deltas = torch.FloatTensor(rewards.size(0), 1)\n",
    "    advantages = torch.FloatTensor(rewards.size(0), 1)\n",
    "\n",
    "    prev_value = 0\n",
    "    prev_advantage = 0\n",
    "\n",
    "    for i in reversed(range(rewards.size(0))):\n",
    "        deltas[i] = rewards[i] + gamma * prev_value * masks[i] - values[i]\n",
    "        advantages[i] = deltas[i] + gamma * tau * prev_advantage * masks[i]\n",
    "\n",
    "        prev_value = values[i, 0]\n",
    "        prev_advantage = advantages[i, 0]\n",
    "\n",
    "    returns = values + advantages\n",
    "    advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "    \n",
    "    return advantages, returns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
