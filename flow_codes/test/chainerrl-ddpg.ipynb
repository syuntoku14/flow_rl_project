{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "\n",
    "- [ ] add reward scaling\n",
    "- [x] override train_agent_batch_with_evaluation\n",
    "- [ ] support calllable object(hook)\n",
    "- [ ] change original function to \n",
    " - [ ] return float reward\n",
    " - [ ] only one Done\n",
    " - [ ] info[\"needs_reset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()  # NOQA\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "from collections import deque\n",
    "\n",
    "import chainer\n",
    "from chainer import optimizers\n",
    "import gym\n",
    "from gym import spaces\n",
    "import gym.wrappers\n",
    "import numpy as np\n",
    "\n",
    "from chainer import cuda\n",
    "import chainerrl\n",
    "from chainerrl.agents.ddpg_ma import DDPG_MA\n",
    "from chainerrl.agents.ddpg import DDPGModel\n",
    "from chainerrl import experiments\n",
    "from chainerrl import explorers\n",
    "from chainerrl import misc\n",
    "from chainerrl import policy\n",
    "from chainerrl import q_functions\n",
    "from chainerrl import replay_buffer\n",
    "\n",
    "from flow.multiagent_envs import MultiWaveAttenuationMergePOEnvMeanRew\n",
    "from flow.scenarios import MergeScenario\n",
    "from flow.utils.registry import make_create_env\n",
    "\n",
    "benchmark_name = 'multi_merge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(create_env):\n",
    "    def _thunk():\n",
    "        env = create_env()\n",
    "        return env\n",
    "    return _thunk\n",
    "\n",
    "def make_batch_env(test):\n",
    "    return chainerrl.envs.MultiprocessVectorEnv(\n",
    "        [make_env(create_env) for i in range(num_envs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = None\n",
    "seed = 0\n",
    "final_exploration_steps = 10**6\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 1e-3\n",
    "steps = 10 ** 7\n",
    "n_hidden_channels = 300\n",
    "n_hidden_layers = 3\n",
    "replay_start_size = 5000\n",
    "n_update_times = 1\n",
    "target_update_interval = 1\n",
    "target_update_method = 'soft'\n",
    "soft_update_tau = 1e-2\n",
    "update_interval = 4\n",
    "eval_n_runs = 100\n",
    "eval_interval = 10**5\n",
    "gamma = 0.995\n",
    "minibatch_size = 200\n",
    "use_bn = True\n",
    "reward_scale_factor = 1e-2\n",
    "return_window_size = 100\n",
    "step_offset = 0\n",
    "log_interval = 5\n",
    "num_envs = 2\n",
    "outdir = '../result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed used in ChainerRL\n",
    "misc.set_random_seed(seed)\n",
    "\n",
    "def reward_filter(r):\n",
    "    return r * args.reward_scale_factor\n",
    "\n",
    "# Set different random seeds for different subprocesses.\n",
    "# If seed=0 and processes=4, subprocess seeds are [0, 1, 2, 3].\n",
    "# If seed=1 and processes=4, subprocess seeds are [4, 5, 6, 7].\n",
    "process_seeds = np.arange(num_envs) + seed * num_envs\n",
    "assert process_seeds.max() < 2 ** 32\n",
    "\n",
    "# outdir = experiments.prepare_output_dir('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark  = __import__(\n",
    "    \"flow.benchmarks.%s\" % benchmark_name, fromlist=[\"flow_params\"])\n",
    "flow_params = benchmark.mean_rew_flow_params\n",
    "HORIZON = flow_params['env'].horizon\n",
    "\n",
    "create_env, env_name = make_create_env(params=flow_params, version=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_env = create_env()\n",
    "\n",
    "timestep_limit = flow_params[\"env\"].horizon\n",
    "obs_size = np.asarray(sample_env.observation_space.shape).prod()\n",
    "action_space = sample_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = np.asarray(action_space.shape).prod()\n",
    "if use_bn:\n",
    "    q_func = q_functions.FCBNLateActionSAQFunction(\n",
    "        obs_size, action_size,\n",
    "        n_hidden_channels=n_hidden_channels,\n",
    "        n_hidden_layers=n_hidden_layers,\n",
    "        normalize_input=True)\n",
    "    pi = policy.FCBNDeterministicPolicy(\n",
    "        obs_size, action_size=action_size,\n",
    "        n_hidden_channels=n_hidden_channels,\n",
    "        n_hidden_layers=n_hidden_layers,\n",
    "        min_action=action_space.low, max_action=action_space.high,\n",
    "        bound_action=True,\n",
    "        normalize_input=True)\n",
    "else:\n",
    "    q_func = q_functions.FCSAQFunction(\n",
    "        obs_size, action_size,\n",
    "        n_hidden_channels=n_hidden_channels,\n",
    "        n_hidden_layers=n_hidden_layers)\n",
    "    pi = policy.FCDeterministicPolicy(\n",
    "        obs_size, action_size=action_size,\n",
    "        n_hidden_channels=n_hidden_channels,\n",
    "        n_hidden_layers=n_hidden_layers,\n",
    "        min_action=action_space.low, max_action=action_space.high,\n",
    "        bound_action=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DDPGModel(q_func=q_func, policy=pi)\n",
    "opt_a = optimizers.Adam(alpha=actor_lr)\n",
    "opt_c = optimizers.Adam(alpha=critic_lr)\n",
    "opt_a.setup(model['policy'])\n",
    "opt_c.setup(model['q_function'])\n",
    "opt_a.add_hook(chainer.optimizer.GradientClipping(1.0), 'hook_a')\n",
    "opt_c.add_hook(chainer.optimizer.GradientClipping(1.0), 'hook_c')\n",
    "\n",
    "rbuf = replay_buffer.ReplayBuffer(5 * 10 ** 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ou_sigma = (action_space.high - action_space.low) * 0.2\n",
    "explorer = explorers.AdditiveOU(sigma=ou_sigma)\n",
    "agent = DDPG_MA(model, opt_a, opt_c, rbuf, gamma=gamma,\n",
    "             explorer=explorer, replay_start_size=replay_start_size,\n",
    "             target_update_method=target_update_method,\n",
    "             target_update_interval=target_update_interval,\n",
    "             update_interval=update_interval,\n",
    "             soft_update_tau=soft_update_tau,\n",
    "             n_times_update=n_update_times,\n",
    "             gpu=gpu, minibatch_size=minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = make_batch_env(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[99.91539795795482, 68.58812206607693, 107.17406151588956]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments.evaluator.batch_run_evaluation_episodes(env, agent, 1000, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_runs: 100 mean: 53.39794724552133 median: 53.39794724552133 stdev 75.51610119749932\n"
     ]
    }
   ],
   "source": [
    "# from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "\n",
    "eval_stats = experiments.eval_performance(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    n_steps=None,\n",
    "    n_episodes=2,\n",
    "    max_episode_len=timestep_limit)\n",
    "print('n_runs: {} mean: {} median: {} stdev {}'.format(\n",
    "    eval_n_runs, eval_stats['mean'], eval_stats['median'],\n",
    "    eval_stats['stdev']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. experiments.train_agent_batch_with_evaluation()\n",
    "\n",
    "2. contents in train_agent_batch\n",
    "\n",
    "    1. actions = agent.batch_act_and_train(obss)\n",
    "    2. env.step(actions)\n",
    "    3. compute resets\n",
    "    4. agent.batch_observe_and_train(obss, rs, dones, resets): in this env, done==True when horizon is reached. So resets is same as dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "recent_returns = deque(maxlen=return_window_size)\n",
    "\n",
    "num_envs = env.num_envs\n",
    "episode_r = np.zeros(num_envs, dtype=np.float64)\n",
    "episode_idx = np.zeros(num_envs, dtype='i')\n",
    "episode_len = np.zeros(num_envs, dtype='i')\n",
    "\n",
    "# o_0, r_0\n",
    "obss = env.reset()\n",
    "rs = np.zeros(num_envs, dtype='f')\n",
    "\n",
    "t = step_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:chainerrl.agents.ddpg:t:0 a:[0.17891979] q:[[-0.37691563]\n",
      " [-0.29338697]\n",
      " [-0.1590812 ]]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:0 noise:[1.5457015]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:0 noise:[1.2952096]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:0 noise:[1.6980542]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:0 noise:[1.1124804]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:0 noise:[0.69293666]\n",
      "DEBUG:chainerrl.agents.ddpg:t:4 a:[0.09080206] q:[[-0.29664254]\n",
      " [-0.1683121 ]]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:4 noise:[1.1955103]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:4 noise:[2.1975727]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:4 noise:[1.6391486]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:4 noise:[1.3692688]\n",
      "DEBUG:chainerrl.agents.ddpg:t:8 a:[0.09270164] q:[[-0.29954857]\n",
      " [-0.17745703]]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:8 noise:[1.4674435]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:8 noise:[1.4067122]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:8 noise:[0.76240957]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:8 noise:[0.6237172]\n",
      "INFO:root:outdir:/tmp/20190424T180219.614230k_yfljkp step:6 episode:1 last_R: nan average_R:nan\n",
      "INFO:root:statistics: [('average_q', -0.0014251107738743772), ('average_actor_loss', 0.0), ('average_critic_loss', 0.0)]\n",
      "DEBUG:chainerrl.agents.ddpg:t:12 a:[0.09666064] q:[[-0.29509437]\n",
      " [-0.18652314]]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:12 noise:[-0.5305212]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:12 noise:[-1.2070153]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:12 noise:[-1.5178901]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:12 noise:[-1.4459627]\n",
      "DEBUG:chainerrl.agents.ddpg:t:16 a:[0.07978249] q:[[-0.28174555]\n",
      " [-0.17764392]]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:16 noise:[-0.5022989]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:16 noise:[0.1075929]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:16 noise:[-1.5150074]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:16 noise:[-2.501397]\n",
      "INFO:root:outdir:/tmp/20190424T180219.614230k_yfljkp step:10 episode:1 last_R: nan average_R:nan\n",
      "INFO:root:statistics: [('average_q', -0.0023685470193550526), ('average_actor_loss', 0.0), ('average_critic_loss', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    # a_t\n",
    "    actions = agent.batch_act_and_train(obss)\n",
    "    # o_{t+1}, r_{t+1}\n",
    "    obss, rs, dones, infos = env.step(actions)\n",
    "    episode_r += rs\n",
    "    episode_len += 1\n",
    "\n",
    "    # mask for reset the env(when collision or horizon)\n",
    "    env_reset = dones\n",
    "    not_env_reset = np.logical_not(env_reset)  # doesn't reset when True\n",
    "\n",
    "    # Agent observes the consequences\n",
    "    agent.batch_observe_and_train(obss, rs, dones, env_reset)\n",
    "\n",
    "    episode_idx += env_reset\n",
    "    recent_returns.extend(episode_r[env_reset])\n",
    "\n",
    "    for _ in range(num_envs):\n",
    "        t += 1\n",
    "\n",
    "    # logger should be here\n",
    "    if (log_interval is not None\n",
    "            and t >= log_interval\n",
    "            and t % log_interval < num_envs):\n",
    "        logger.info(\n",
    "            'outdir:{} step:{} episode:{} last_R: {} average_R:{}'.format(  # NOQA\n",
    "                outdir,\n",
    "                t,\n",
    "                np.sum(episode_idx),\n",
    "                recent_returns[-1] if recent_returns else np.nan,\n",
    "                np.mean(recent_returns) if recent_returns else np.nan,\n",
    "            ))\n",
    "        logger.info('statistics: {}'.format(agent.get_statistics()))\n",
    "\n",
    "    # evaluator should be here\n",
    "\n",
    "    if t >= steps:\n",
    "        break\n",
    "\n",
    "    # Start new episodes if needed\n",
    "    episode_r[env_reset] = 0\n",
    "    episode_len[env_reset] = 0\n",
    "    obss = env.reset(not_env_reset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For not native multi-merge env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_MA(ddpg.DDPG):\n",
    "    def batch_act(self, batch_obs):\n",
    "        \"\"\"Select a batch of actions for evaluation.\n",
    "        Args:\n",
    "            batch_obs (Sequence of ~object): Observations.\n",
    "        Returns:\n",
    "            Sequence of ~object: Actions.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_actions = []\n",
    "        for env_obs in batch_obs:\n",
    "            keys, obss = list(env_obs.keys()), list(env_obs.values())\n",
    "            obss = [obs.astype(np.float32) for obs in obss]\n",
    "            \n",
    "            with chainer.using_config('train', False), chainer.no_backprop_mode():\n",
    "                batch_xs = self.batch_states(obss, self.xp, self.phi)\n",
    "                batch_action = self.policy(batch_xs).sample()\n",
    "                # Q is not needed here, but log it just for information\n",
    "                q = self.q_function(batch_xs, batch_action)\n",
    "\n",
    "            batch_actions.append({key:cuda.to_cpu(action.array) for key, action in zip(keys, batch_action)})\n",
    "            \n",
    "            # Update stats\n",
    "            self.average_q *= self.average_q_decay\n",
    "            self.average_q += (1 - self.average_q_decay) * float(\n",
    "                q.array.mean(axis=0))\n",
    "            \n",
    "        self.logger.debug('t:%s a:%s q:%s',\n",
    "                          self.t, batch_action.array[0], q.array)\n",
    "        return batch_actions\n",
    "\n",
    "    def batch_act_and_train(self, batch_obs):\n",
    "        \"\"\"Select a batch of actions for training.\n",
    "        Args:\n",
    "            batch_obs (Sequence of ~object): Observations.\n",
    "        Returns:\n",
    "            Sequence of ~object: Actions.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_greedy_action = self.batch_act(batch_obs)\n",
    "        batch_action = [\n",
    "            {key: self.explorer.select_action(self.t, lambda: action) for key, action in actions.items()}\n",
    "            for actions in batch_greedy_action] \n",
    "\n",
    "        self.batch_last_obs = list(batch_obs)\n",
    "        self.batch_last_action = list(batch_action)\n",
    "        \n",
    "        return batch_action\n",
    "\n",
    "    def batch_observe_and_train(\n",
    "            self, batch_obs, batch_reward, batch_done, batch_reset):\n",
    "        \"\"\"Observe a batch of action consequences for training.\n",
    "        Args:\n",
    "            batch_obs (Sequence of ~object): Observations.\n",
    "            batch_reward (Sequence of float): Rewards.\n",
    "            batch_done (Sequence of boolean): Boolean values where True\n",
    "                indicates the current state is terminal.\n",
    "            batch_reset (Sequence of boolean): Boolean values where True\n",
    "                indicates the current episode will be reset, even if the\n",
    "                current state is not terminal.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(len(batch_obs)):\n",
    "            if self.batch_last_obs[i] is not None:\n",
    "                assert self.batch_last_action[i] is not None           \n",
    "                for key in self.batch_last_obs[i].keys():\n",
    "                    if key in batch_obs[i]:\n",
    "                        self.t += 1\n",
    "                        # Update the target network\n",
    "                        if self.t % self.target_update_interval == 0:\n",
    "                            self.sync_target_network()\n",
    "                        # Add a transition to the replay buffer\n",
    "                        self.replay_buffer.append(\n",
    "                            state=self.batch_last_obs[i][key],\n",
    "                            action=self.batch_last_action[i][key],\n",
    "                            reward=batch_reward[i][key],\n",
    "                            next_state=batch_obs[i][key],\n",
    "                            next_action=None,\n",
    "                            is_state_terminal=batch_done[i][key]\n",
    "                        )\n",
    "                        self.replay_updater.update_if_necessary(self.t)\n",
    "                # set None when env is beggining\n",
    "                if batch_reset[i]:\n",
    "                    self.batch_last_obs[i] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:chainerrl.agents.ddpg:t:0 a:[0.08707273] q:[[-0.2751863 ]\n",
      " [-0.15872331]]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:0 noise:[1.5457015]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:0 noise:[1.2952096]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:0 noise:[1.6980542]\n",
      "DEBUG:chainerrl.explorers.additive_ou:t:0 noise:[1.1124804]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a24b29b778e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# o_{t+1}, r_{t+1}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mobss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mepisode_r\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mepisode_len\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a24b29b778e5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# o_{t+1}, r_{t+1}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mobss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mepisode_r\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mepisode_len\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    # a_t\n",
    "    actions = agent.batch_act_and_train(obss)\n",
    "    # o_{t+1}, r_{t+1}\n",
    "    obss, rs, dones, infos = env.step(actions)\n",
    "    episode_r += [np.mean(list(rs[i].values())) for i in range(len(rs))]\n",
    "    episode_len += 1\n",
    "\n",
    "    # mask for reset the env(when collision or horizon)\n",
    "    env_reset = [done[\"__all__\"] for done in dones]\n",
    "    not_env_reset = np.logical_not(env_reset)  # doesn't reset when True\n",
    "\n",
    "    # Agent observes the consequences\n",
    "    agent.batch_observe_and_train(obss, rs, dones, env_reset)\n",
    "\n",
    "    episode_idx += env_reset\n",
    "    recent_returns.extend(episode_r[env_reset])\n",
    "\n",
    "    for _ in range(num_envs):\n",
    "        t += 1\n",
    "\n",
    "    # logger should be here\n",
    "    if (log_interval is not None\n",
    "            and t >= log_interval\n",
    "            and t % log_interval < num_envs):\n",
    "        logger.info(\n",
    "            'outdir:{} step:{} episode:{} last_R: {} average_R:{}'.format(  # NOQA\n",
    "                outdir,\n",
    "                t,\n",
    "                np.sum(episode_idx),\n",
    "                recent_returns[-1] if recent_returns else np.nan,\n",
    "                np.mean(recent_returns) if recent_returns else np.nan,\n",
    "            ))\n",
    "        logger.info('statistics: {}'.format(agent.get_statistics()))\n",
    "\n",
    "    # evaluator should be here\n",
    "\n",
    "    if t >= steps:\n",
    "        break\n",
    "\n",
    "    # Start new episodes if needed\n",
    "    episode_r[env_reset] = 0\n",
    "    episode_len[env_reset] = 0\n",
    "    obss = env.reset(not_env_reset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
