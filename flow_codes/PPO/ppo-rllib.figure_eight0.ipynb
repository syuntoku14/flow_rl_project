{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.evaluation import PolicyEvaluator, TorchPolicyGraph\n",
    "from ray.rllib.utils.annotations import override\n",
    "from flow.utils.registry import make_create_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from threading import Lock\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "except ImportError:\n",
    "    pass  # soft dep\n",
    "\n",
    "from ray.rllib.evaluation import TorchPolicyGraph\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import PPOPolicyGraph, PPOLoss\n",
    "from ray.rllib.utils.annotations import override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ray.rllib.agents.ppo.ppo_policy_graph' from '/opt/conda/envs/flow/lib/python3.5/site-packages/ray/rllib/agents/ppo/ppo_policy_graph.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_policy_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-04-04_07-04-58_1260/logs.\n",
      "Waiting for redis server at 127.0.0.1:29382 to respond...\n",
      "Waiting for redis server at 127.0.0.1:51379 to respond...\n",
      "Starting the Plasma object store with 13.355121049 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '169.237.32.118',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-04-04_07-04-58_1260/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-04-04_07-04-58_1260/sockets/raylet'],\n",
       " 'redis_address': '169.237.32.118:29382',\n",
       " 'webui_url': ''}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=3, include_webui=False, ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_name = 'figureeight0'\n",
    "benchmark = __import__(\n",
    "    \"flow.benchmarks.%s\" % benchmark_name, fromlist=[\"flow_params\"])\n",
    "flow_params = benchmark.flow_params\n",
    "HORIZON = flow_params['env'].horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Environment\n",
    "num_envs = 3\n",
    "create_env, env_name = make_create_env(params=flow_params, version=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Starting SUMO on port 54405\n"
     ]
    }
   ],
   "source": [
    "env = create_env()\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_space, action_space, fcnet_hiddens, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        num_inputs = obs_space.shape[0]\n",
    "        num_outputs = action_space.shape[0]\n",
    "        last_layer_size = num_inputs\n",
    "        layers = []\n",
    "        \n",
    "        for size in fcnet_hiddens:\n",
    "            layers.append(nn.Linear(last_layer_size, size))\n",
    "            layers.append(nn.ReLU())\n",
    "            last_layer_size = size\n",
    "            \n",
    "        layers.append(nn.Linear(fcnet_hiddens[-1], num_outputs))\n",
    "        \n",
    "        self.critic = nn.Sequential(*layers)\n",
    "        self.actor = nn.Sequential(*layers)\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "            nn.init.constant_(m.bias, 0.1)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().view(mu.shape)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value\n",
    "\n",
    "    \n",
    "class PPOLoss(nn.Module):\n",
    "    def forward(self, model, state, action, log_prob, return_, advantage):\n",
    "        dist, value = model(state)\n",
    "        entropy = dist.entropy().mean()\n",
    "        new_log_prob = dist.log_prob(action)\n",
    "\n",
    "        ratio = (new_log_prob - log_prob).exp()\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "        actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "        critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "        loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "    \n",
    "\n",
    "class PPOTorchPolicyGraph(TorchPolicyGraph):\n",
    "    def __init__(self, observation_space, action_space, config):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.loss = PPOLoss()\n",
    "\n",
    "class TorchPolicyGraph(PolicyGraph):\n",
    "    \"\"\"Template for a PyTorch policy and loss to use with RLlib.\n",
    "    This is similar to TFPolicyGraph, but for PyTorch.\n",
    "    Attributes:\n",
    "        observation_space (gym.Space): observation space of the policy.\n",
    "        action_space (gym.Space): action space of the policy.\n",
    "        lock (Lock): Lock that must be held around PyTorch ops on this graph.\n",
    "            This is necessary when using the async sampler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, model, loss,\n",
    "                 loss_inputs):\n",
    "        \"\"\"Build a policy graph from policy and loss torch modules.\n",
    "        Note that module inputs will be CPU tensors. The model and loss modules\n",
    "        are responsible for moving inputs to the right device.\n",
    "        Arguments:\n",
    "            observation_space (gym.Space): observation space of the policy.\n",
    "            action_space (gym.Space): action space of the policy.\n",
    "            model (nn.Module): PyTorch policy module. Given observations as\n",
    "                input, this module must return a list of outputs where the\n",
    "                first item is action logits, and the rest can be any value.\n",
    "            loss (nn.Module): Loss defined as a PyTorch module. The inputs for\n",
    "                this module are defined by the `loss_inputs` param. This module\n",
    "                returns a single scalar loss. Note that this module should\n",
    "                internally be using the model module.\n",
    "            loss_inputs (list): List of SampleBatch columns that will be\n",
    "                passed to the loss module's forward() function when computing\n",
    "                the loss. For example, [\"obs\", \"action\", \"advantages\"].\n",
    "        \"\"\"\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.lock = Lock()\n",
    "        self._model = model\n",
    "        self._loss = loss\n",
    "        self._loss_inputs = loss_inputs\n",
    "        self._optimizer = self.optimizer()\n",
    "\n",
    "    @override(PolicyGraph)\n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches=None,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        info_batch=None,\n",
    "                        episodes=None,\n",
    "                        **kwargs):\n",
    "        with self.lock:\n",
    "            with torch.no_grad():\n",
    "                ob = torch.from_numpy(obs_batch).float()\n",
    "                model_out = self._model(ob, state_batches)\n",
    "                dist, value = model_out\n",
    "                actions = dist.sample()\n",
    "                return (actions.cpu().numpy(), dist, value)           \n",
    "            \n",
    "    @override(PolicyGraph)\n",
    "    def compute_gradients(self, postprocessed_batch):\n",
    "        with self.lock:\n",
    "            loss_in = []\n",
    "            for key in self._loss_inputs:\n",
    "                loss_in.append(torch.from_numpy(postprocessed_batch[key]))\n",
    "            loss_out = self._loss(*loss_in)\n",
    "            self._optimizer.zero_grad()\n",
    "            loss_out.backward()\n",
    "            # Note that return values are just references;\n",
    "            # calling zero_grad will modify the values\n",
    "            grads = []\n",
    "            for p in self._model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    grads.append(p.grad.data.numpy())\n",
    "                else:\n",
    "                    grads.append(None)\n",
    "            return grads, {}\n",
    "\n",
    "    @override(PolicyGraph)\n",
    "    def apply_gradients(self, gradients):\n",
    "        with self.lock:\n",
    "            for g, p in zip(gradients, self._model.parameters()):\n",
    "                if g is not None:\n",
    "                    p.grad = torch.from_numpy(g)\n",
    "            self._optimizer.step()\n",
    "            return {}\n",
    "\n",
    "    @override(PolicyGraph)\n",
    "    def get_weights(self):\n",
    "        with self.lock:\n",
    "            return self._model.state_dict()\n",
    "\n",
    "    @override(PolicyGraph)\n",
    "    def set_weights(self, weights):\n",
    "        with self.lock:\n",
    "            self._model.load_state_dict(weights)\n",
    "\n",
    "    @override(PolicyGraph)\n",
    "    def get_initial_state(self):\n",
    "        return [s.numpy() for s in self._model.state_init()]\n",
    "\n",
    "    def extra_action_out(self, model_out):\n",
    "        \"\"\"Returns dict of extra info to include in experience batch.\n",
    "        Arguments:\n",
    "            model_out (list): Outputs of the policy model module.\"\"\"\n",
    "        return {}\n",
    "\n",
    "    def optimizer(self):\n",
    "        \"\"\"Custom PyTorch optimizer to use.\"\"\"\n",
    "        return torch.optim.Adam(self._model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnet_hiddens = [100, 50, 25]\n",
    "model = ActorCritic(observation_space, action_space, fcnet_hiddens, std=0.05).cpu()\n",
    "ppoloss = PPOLoss()\n",
    "loss_inputs = [\"model\", \"state\", \"action\", \"log_prob\", \"return_\", \"advantage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "issubclass() arg 2 must be a class or tuple of classes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-a7f3640a5af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTorchPolicyGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppo_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: issubclass() arg 2 must be a class or tuple of classes"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_graph = TorchPolicyGraph(observation_space, action_space, model, ppoloss, loss_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Starting SUMO on port 49568\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'loss' and 'loss_inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-060d87222843>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTorchPolicyGraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/flow/lib/python3.5/site-packages/ray/rllib/evaluation/policy_evaluator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_creator, policy_graph, policy_mapping_fn, policies_to_train, tf_session_creator, batch_steps, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_filter, clip_rewards, clip_actions, env_config, model_config, policy_config, worker_index, monitor_path, log_dir, log_level, callbacks, input_creator, input_evaluation_method, output_creator)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             self.policy_map, self.preprocessors = self._build_policy_map(\n\u001b[0;32m--> 275\u001b[0;31m                 policy_dict, policy_config)\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiagent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mDEFAULT_POLICY_ID\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/flow/lib/python3.5/site-packages/ray/rllib/evaluation/policy_evaluator.py\u001b[0m in \u001b[0;36m_build_policy_map\u001b[0;34m(self, policy_dict, policy_config)\u001b[0m\n\u001b[1;32m    577\u001b[0m                     \"Tuple|DictFlatteningPreprocessor.\")\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m                 \u001b[0mpolicy_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'loss' and 'loss_inputs'"
     ]
    }
   ],
   "source": [
    "evaluator = PolicyEvaluator(env_creator=create_env, policy_graph=TorchPolicyGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluator.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"obs\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "device_id = ray.put(device)\n",
    "\n",
    "#Hyper params:\n",
    "lr = 5e-4\n",
    "training_iter = 500\n",
    "num_rollouts = 1\n",
    "num_steps = HORIZON * num_rollouts\n",
    "mini_batch_size = 128\n",
    "num_sgd_iter = 10\n",
    "fcnet_hiddens = [100, 50, 25]\n",
    "gae_lambda = 0.97\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, fcnet_hiddens).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "now = str(now).replace(':', '-').replace(' ', '-')\n",
    "now = now[:now.find('.')]\n",
    "result_path = './result/ppo/' + now\n",
    "os.makedirs(result_path)\n",
    "image_path = result_path + '/reward_history.png'\n",
    "\n",
    "test_rewards, num_iters = [], []\n",
    "\n",
    "state = envs.reset()\n",
    "\n",
    "for num_iter in trange(training_iter):\n",
    "    state = envs.reset()\n",
    "    trajectory = {'log_probs':[], 'values':[], 'states':[], \n",
    "                  'actions':[], 'rewards':[], 'masks':[]}\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = model(state)\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "        log_prob = dist.log_prob(action)\n",
    "        append_trajectory(trajectory, log_prob, value, state, action, reward, done, device)\n",
    "        state = next_state\n",
    "\n",
    "    if num_iter % 25 == 0:\n",
    "        model_id = ray.put(model)\n",
    "        results_ids = [ev.test_env.remote(device_id, model_id) for ev in evs]\n",
    "        test_reward = np.mean(ray.get(results_ids))\n",
    "        test_rewards.append(test_reward)\n",
    "        num_iters.append(num_iter)\n",
    "        plot_and_save(num_iters, test_rewards, image_path)\n",
    "        model_path = result_path + '/checkpoint' + str(num_iter) + '.pt'\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        send_line(url, headers, 'epoch: {}'.format(num_iter), image_path)\n",
    "       \n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, trajectory, tau=gae_lambda)\n",
    "    cat_trajectory(trajectory, returns)\n",
    "    \n",
    "    ppo_update(model, optimizer, num_sgd_iter, mini_batch_size, trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActorCritic(num_inputs, num_outputs, fcnet_hiddens).to(device)\n",
    "\n",
    "model_path = './result/ppo/2019-04-02-02-06-26/checkpoint475.pt'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "model_id = ray.put(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vis_env(benchmark_name)\n",
    "\n",
    "test_env(env, device, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 467.89781800000003,
   "position": {
    "height": "489.716px",
    "left": "1387.27px",
    "right": "20px",
    "top": "170px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
