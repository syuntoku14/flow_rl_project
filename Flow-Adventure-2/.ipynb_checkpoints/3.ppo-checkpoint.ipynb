{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "from common.figure8_env import gen_figure8_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 1 \n",
    "HORIZON = 1000\n",
    "render = True\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gen_figure8_env(HORIZON=HORIZON, render=render)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gen_figure8_env(HORIZON=HORIZON, render=render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done:215\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i in range(10000):\n",
    "    _, _, done, _ = env.step(1)\n",
    "    if done:\n",
    "        print('done:{}'.format(i))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.46364297, 0.38919663, 0.3497995 , 0.37582117, 0.43658876,\n",
       "        0.4913084 , 0.51750539, 0.49048484, 0.43484556, 0.3823904 ,\n",
       "        0.37462947, 0.45215658, 0.49225409, 0.522426  , 0.51422053,\n",
       "        0.48046089, 0.55581378, 0.6223893 , 0.6515063 , 0.71695369,\n",
       "        0.79218277, 0.87464194, 0.95781107, 1.0281164 , 0.05316551,\n",
       "        0.09735674, 0.16418469, 0.23951612, 0.32065718, 0.40503036]),\n",
       " 0.6578380008799711,\n",
       " True,\n",
       " {})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_envs(vis=False):\n",
    "    state = envs.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, _ = model(state)\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "        \n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward / float(envs.num_envs)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = HORIZON\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = 800\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = 15000\n",
    "frame_idx  = 0\n",
    "test_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state = envs.reset()\n",
    "early_stop = False\n",
    "\n",
    "log_probs = []\n",
    "values    = []\n",
    "states    = []\n",
    "actions   = []\n",
    "rewards   = []\n",
    "masks     = []\n",
    "entropy = 0\n",
    "\n",
    "for _ in range(num_steps):\n",
    "    state = torch.FloatTensor(state).to(device)\n",
    "    dist, value = model(state)\n",
    "\n",
    "    action = dist.sample()\n",
    "    next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "\n",
    "    log_prob = dist.log_prob(action)\n",
    "    entropy += dist.entropy().mean()\n",
    "\n",
    "    log_probs.append(log_prob)\n",
    "    values.append(value)\n",
    "    rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "    masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "\n",
    "    states.append(state)\n",
    "    actions.append(action)\n",
    "\n",
    "    state = next_state\n",
    "    frame_idx += 1\n",
    "\n",
    "    # if frame_idx % 1000 == 0:\n",
    "    #     print(\"test\")\n",
    "    #     test_reward = np.mean([test_env() for _ in range(10)])\n",
    "    #     test_rewards.append(test_reward)\n",
    "    #     plot(frame_idx, test_rewards)\n",
    "    #     if test_reward > threshold_reward: early_stop = True\n",
    "\n",
    "\n",
    "next_state = torch.FloatTensor(next_state).to(device)\n",
    "_, next_value = model(next_state)\n",
    "returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "returns   = torch.cat(returns).detach()\n",
    "log_probs = torch.cat(log_probs).detach()\n",
    "values    = torch.cat(values).detach()\n",
    "states    = torch.cat(states)\n",
    "actions   = torch.cat(actions)\n",
    "advantage = returns - values\n",
    "\n",
    "ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-20ddb579fca5>\u001b[0m in \u001b[0;36mtest_env\u001b[0;34m(vis)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_reward = np.mean([test_env() for _ in range(1)])\n",
    "test_rewards.append(test_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-94c23a0bc16c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mtest_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mtest_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-94c23a0bc16c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mtest_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mtest_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-20ddb579fca5>\u001b[0m in \u001b[0;36mtest_env\u001b[0;34m(vis)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/flow/flow/envs/base_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, rl_actions)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;31m# advance the simulation in the simulator by one step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;31m# store new observations in the vehicles and traffic lights class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/flow/flow/core/kernel/simulation/traci.py\u001b[0m in \u001b[0;36msimulation_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimulation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m\"\"\"See parent class.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulationStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sumo/tools/traci/connection.py\u001b[0m in \u001b[0;36msimulationStep\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnumSubs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0mresponses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readSubscription\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0mnumSubs\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sumo/tools/traci/connection.py\u001b[0m in \u001b[0;36m_readSubscription\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subscriptionMapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                     self._subscriptionMapping[response].add(\n\u001b[0;32m--> 184\u001b[0;31m                         objectID, varID, result)\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                     raise FatalTraCIError(\n",
      "\u001b[0;32m~/sumo/tools/traci/domain.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, refID, varID, data)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrefID\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrefID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrefID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvarID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvarID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefID\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sumo/tools/traci/domain.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self, varID, data)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvarID\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_valueFunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mFatalTraCIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown variable %02x.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvarID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_valueFunc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvarID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sumo/tools/traci/storage.py\u001b[0m in \u001b[0;36mreadStringList\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sumo/tools/traci/storage.py\u001b[0m in \u001b[0;36mreadString\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!%ss\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"latin1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sumo/tools/traci/storage.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, format)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moldPos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moldPos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state = envs.reset()\n",
    "early_stop = False\n",
    "\n",
    "while frame_idx < max_frames and not early_stop:\n",
    "\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 1000 == 0:\n",
    "            test_reward = np.mean([test_env() for _ in range(2)])\n",
    "            test_rewards.append(test_reward)\n",
    "            plot(frame_idx, test_rewards)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "            \n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Saving trajectories for GAIL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: -133.5056485070341\n",
      "episode: 1 reward: -3.3737309166625002\n",
      "episode: 2 reward: -135.0328820133956\n",
      "episode: 3 reward: -131.27964142064513\n",
      "episode: 4 reward: -125.12845453838382\n",
      "episode: 5 reward: -4.247933460422459\n",
      "episode: 6 reward: -395.59297834503883\n",
      "episode: 7 reward: -253.25736991568547\n",
      "episode: 8 reward: -135.50603026103278\n",
      "episode: 9 reward: -132.72095459732952\n",
      "episode: 10 reward: -133.89608385869212\n",
      "episode: 11 reward: -4.5990508813314035\n",
      "episode: 12 reward: -134.44470210766775\n",
      "episode: 13 reward: -801.7661346371387\n",
      "episode: 14 reward: -131.97725229377644\n",
      "episode: 15 reward: -266.76940521674015\n",
      "episode: 16 reward: -247.5062278004002\n",
      "episode: 17 reward: -4.914595620774103\n",
      "episode: 18 reward: -138.7990887577753\n",
      "episode: 19 reward: -268.3754189751262\n",
      "episode: 20 reward: -363.28764882256417\n",
      "episode: 21 reward: -128.15870842354997\n",
      "episode: 22 reward: -134.94598918501788\n",
      "episode: 23 reward: -309.9577786212293\n",
      "episode: 24 reward: -131.91670030817002\n",
      "episode: 25 reward: -134.65823444568952\n",
      "episode: 26 reward: -134.5615349098279\n",
      "episode: 27 reward: -273.5740578550409\n",
      "episode: 28 reward: -265.05553942459926\n",
      "episode: 29 reward: -258.0591054576666\n",
      "episode: 30 reward: -128.91060595426686\n",
      "episode: 31 reward: -656.2461074160591\n",
      "episode: 32 reward: -136.84071690580248\n",
      "episode: 33 reward: -259.2365200533221\n",
      "episode: 34 reward: -132.68644155022494\n",
      "episode: 35 reward: -260.66364797902054\n",
      "episode: 36 reward: -128.8211009270027\n",
      "episode: 37 reward: -384.53615237759317\n",
      "episode: 38 reward: -4.612904346743044\n",
      "episode: 39 reward: -401.1162060114804\n",
      "episode: 40 reward: -126.25334578262932\n",
      "episode: 41 reward: -3.845934927726255\n",
      "episode: 42 reward: -132.44253012402612\n",
      "episode: 43 reward: -134.1267203432647\n",
      "episode: 44 reward: -128.56866661753938\n",
      "episode: 45 reward: -4.97856955649956\n",
      "episode: 46 reward: -392.498679426522\n",
      "episode: 47 reward: -4.756869243844947\n",
      "episode: 48 reward: -4.59189846851519\n",
      "episode: 49 reward: -4.7496626929539225\n",
      "episode: 50 reward: -131.08999767991665\n",
      "episode: 51 reward: -138.17235302513578\n",
      "episode: 52 reward: -3.751761058079555\n",
      "episode: 53 reward: -260.6317126814632\n",
      "episode: 54 reward: -4.535299319594524\n",
      "episode: 55 reward: -133.70892423024802\n",
      "episode: 56 reward: -134.8732103854694\n",
      "episode: 57 reward: -5.315182694344295\n",
      "episode: 58 reward: -265.04898120165\n",
      "episode: 59 reward: -124.99288470795233\n",
      "episode: 60 reward: -4.247632479535832\n",
      "episode: 61 reward: -3.68334723705883\n",
      "episode: 62 reward: -133.617727327027\n",
      "episode: 63 reward: -136.28353948776376\n",
      "episode: 64 reward: -5.056124136459314\n",
      "episode: 65 reward: -262.7844771770983\n",
      "episode: 66 reward: -251.52420165781922\n",
      "episode: 67 reward: -133.4014820950796\n",
      "episode: 68 reward: -7.0558924646711\n",
      "episode: 69 reward: -135.41150554590206\n",
      "episode: 70 reward: -131.8871841825757\n",
      "episode: 71 reward: -130.8724972571845\n",
      "episode: 72 reward: -367.7339135957503\n",
      "episode: 73 reward: -134.25198778254116\n",
      "episode: 74 reward: -133.86858295338342\n",
      "episode: 75 reward: -378.9443227440811\n",
      "episode: 76 reward: -3.5473336732949625\n",
      "episode: 77 reward: -261.5470895641183\n",
      "episode: 78 reward: -408.34135925288217\n",
      "episode: 79 reward: -257.6727990499033\n",
      "episode: 80 reward: -399.78682205537433\n",
      "episode: 81 reward: -266.08087229456055\n",
      "episode: 82 reward: -817.186490578741\n",
      "episode: 83 reward: -4.500140134501902\n",
      "episode: 84 reward: -508.65456581456573\n",
      "episode: 85 reward: -378.46002005145874\n",
      "episode: 86 reward: -137.76181809972095\n",
      "episode: 87 reward: -674.8280917415572\n",
      "episode: 88 reward: -128.65034230393303\n",
      "episode: 89 reward: -3.922315525193146\n",
      "episode: 90 reward: -131.00005239353024\n",
      "episode: 91 reward: -130.68974732718007\n",
      "episode: 92 reward: -135.21946982972375\n",
      "episode: 93 reward: -137.3667851983452\n",
      "episode: 94 reward: -136.9119001250973\n",
      "episode: 95 reward: -254.5371556381929\n",
      "episode: 96 reward: -374.827391591992\n",
      "episode: 97 reward: -523.9964989484117\n",
      "episode: 98 reward: -133.94200200894622\n",
      "episode: 99 reward: -133.74880434577523\n",
      "episode: 100 reward: -247.32247835568552\n",
      "episode: 101 reward: -138.75528548988993\n",
      "episode: 102 reward: -4.847096453940289\n",
      "episode: 103 reward: -136.62732481247133\n",
      "episode: 104 reward: -262.20300946977864\n",
      "episode: 105 reward: -6.5435854338994\n",
      "episode: 106 reward: -125.17361036750681\n",
      "episode: 107 reward: -690.5202921080676\n",
      "episode: 108 reward: -280.53617631459497\n",
      "episode: 109 reward: -135.40352441695322\n",
      "episode: 110 reward: -131.07617970631023\n",
      "episode: 111 reward: -247.0260554601557\n",
      "episode: 112 reward: -135.40673404514774\n",
      "episode: 113 reward: -395.03306256658476\n",
      "episode: 114 reward: -384.1784417792837\n",
      "episode: 115 reward: -128.4500742980931\n",
      "episode: 116 reward: -463.6977661877445\n",
      "episode: 117 reward: -130.94801971085445\n",
      "episode: 118 reward: -144.0228791279258\n",
      "episode: 119 reward: -667.2634492717342\n",
      "episode: 120 reward: -131.79948959004724\n",
      "episode: 121 reward: -138.03140142705894\n",
      "episode: 122 reward: -129.26779443720966\n",
      "episode: 123 reward: -3.2877798185337896\n",
      "episode: 124 reward: -134.72016283865193\n",
      "episode: 125 reward: -382.2159098741087\n",
      "episode: 126 reward: -264.6491917411121\n",
      "episode: 127 reward: -134.2254720027939\n",
      "episode: 128 reward: -424.8235005744391\n",
      "episode: 129 reward: -134.52619102883028\n",
      "episode: 130 reward: -537.7406839640856\n",
      "episode: 131 reward: -133.90654715605245\n",
      "episode: 132 reward: -132.20198118805123\n",
      "episode: 133 reward: -400.3589991495165\n",
      "episode: 134 reward: -130.12695949420717\n",
      "episode: 135 reward: -290.86810229081595\n",
      "episode: 136 reward: -394.9043391522139\n",
      "episode: 137 reward: -133.42125091255778\n",
      "episode: 138 reward: -134.96306459417266\n",
      "episode: 139 reward: -3.8499366797706336\n",
      "episode: 140 reward: -3.828788719469504\n",
      "episode: 141 reward: -5.554963437941836\n",
      "episode: 142 reward: -4.510403163975261\n",
      "episode: 143 reward: -325.97799775791754\n",
      "episode: 144 reward: -3.1174779530363375\n",
      "episode: 145 reward: -134.55262416681552\n",
      "episode: 146 reward: -350.45777263184095\n",
      "episode: 147 reward: -137.33235583532627\n",
      "episode: 148 reward: -452.0061280718382\n",
      "episode: 149 reward: -265.98673902850385\n",
      "episode: 150 reward: -284.8590382363739\n",
      "episode: 151 reward: -250.06981206461143\n",
      "episode: 152 reward: -129.50428228187013\n",
      "episode: 153 reward: -393.09302439930724\n",
      "episode: 154 reward: -5.075964808667517\n",
      "episode: 155 reward: -129.83816358490287\n",
      "episode: 156 reward: -266.1020126434327\n",
      "episode: 157 reward: -132.23463644630868\n",
      "episode: 158 reward: -779.5855091317233\n",
      "episode: 159 reward: -3.763971510946643\n",
      "episode: 160 reward: -132.67794144748086\n",
      "episode: 161 reward: -662.5587064643477\n",
      "episode: 162 reward: -135.2401324340408\n",
      "episode: 163 reward: -259.9633585943629\n",
      "episode: 164 reward: -6.232862086437321\n",
      "episode: 165 reward: -139.498411973157\n",
      "episode: 166 reward: -135.35070491390638\n",
      "episode: 167 reward: -135.1400077480551\n",
      "episode: 168 reward: -347.3664683729514\n",
      "episode: 169 reward: -427.1984854733556\n",
      "episode: 170 reward: -5.15672209428849\n",
      "episode: 171 reward: -525.916662268042\n",
      "episode: 172 reward: -133.7053511504196\n",
      "episode: 173 reward: -271.26784680564384\n",
      "episode: 174 reward: -124.85474506625023\n",
      "episode: 175 reward: -134.19873581079943\n",
      "episode: 176 reward: -255.83160338962983\n",
      "episode: 177 reward: -135.13400569542506\n",
      "episode: 178 reward: -4.960226836538054\n",
      "episode: 179 reward: -139.19809065222032\n",
      "episode: 180 reward: -140.05080094044732\n",
      "episode: 181 reward: -137.76647105767526\n",
      "episode: 182 reward: -403.1731636539886\n",
      "episode: 183 reward: -257.970427512537\n",
      "episode: 184 reward: -3.7473226459331066\n",
      "episode: 185 reward: -278.3098063643893\n",
      "episode: 186 reward: -255.99692458401518\n",
      "episode: 187 reward: -4.6365121508813445\n",
      "episode: 188 reward: -244.67627722290948\n",
      "episode: 189 reward: -131.21920785362062\n",
      "episode: 190 reward: -777.3698354491825\n",
      "episode: 191 reward: -132.07220706141683\n",
      "episode: 192 reward: -392.09434598281683\n",
      "episode: 193 reward: -136.06354238422503\n",
      "episode: 194 reward: -377.4409927865957\n",
      "episode: 195 reward: -132.18253486880235\n",
      "episode: 196 reward: -129.15162595976702\n",
      "episode: 197 reward: -396.5254064840202\n",
      "episode: 198 reward: -3.610361833207753\n",
      "episode: 199 reward: -245.53736015092704\n",
      "episode: 200 reward: -270.99181854480565\n",
      "episode: 201 reward: -247.4231450110685\n",
      "episode: 202 reward: -131.59894474370887\n",
      "episode: 203 reward: -144.7898370619998\n",
      "episode: 204 reward: -926.5588068852352\n",
      "episode: 205 reward: -133.39727923189105\n",
      "episode: 206 reward: -131.93566436017008\n",
      "episode: 207 reward: -6.40529176710689\n",
      "episode: 208 reward: -257.08448208556194\n",
      "episode: 209 reward: -130.92098423630432\n",
      "episode: 210 reward: -262.2927047192545\n",
      "episode: 211 reward: -6.859901180492491\n",
      "episode: 212 reward: -262.70877767928914\n",
      "episode: 213 reward: -134.56588203218894\n",
      "episode: 214 reward: -135.22465193371625\n",
      "episode: 215 reward: -137.9657247788344\n",
      "episode: 216 reward: -135.13425433384725\n",
      "episode: 217 reward: -132.3215993693809\n",
      "episode: 218 reward: -400.611961792729\n",
      "episode: 219 reward: -401.91908212383294\n",
      "episode: 220 reward: -282.5082305011229\n",
      "episode: 221 reward: -135.42191465289923\n",
      "episode: 222 reward: -399.7881535647735\n",
      "episode: 223 reward: -131.06522770318847\n",
      "episode: 224 reward: -130.7681491912167\n",
      "episode: 225 reward: -135.31477016876133\n",
      "episode: 226 reward: -3.914901001828447\n",
      "episode: 227 reward: -134.5129393394648\n",
      "episode: 228 reward: -376.1469783238271\n",
      "episode: 229 reward: -133.09045533066046\n",
      "episode: 230 reward: -383.2750315233141\n",
      "episode: 231 reward: -263.71240275232276\n",
      "episode: 232 reward: -500.0083919266878\n",
      "episode: 233 reward: -135.22531187168758\n",
      "episode: 234 reward: -135.17818433537522\n",
      "episode: 235 reward: -395.9834332194123\n",
      "episode: 236 reward: -126.08778928679216\n",
      "episode: 237 reward: -413.7495701300203\n",
      "episode: 238 reward: -131.37116502717876\n",
      "episode: 239 reward: -121.6506938627967\n",
      "episode: 240 reward: -653.7053929625495\n",
      "episode: 241 reward: -254.87183145095838\n",
      "episode: 242 reward: -129.71331746419523\n",
      "episode: 243 reward: -265.9795936355916\n",
      "episode: 244 reward: -400.65989274385277\n",
      "episode: 245 reward: -251.82522565834446\n",
      "episode: 246 reward: -3.95924871368981\n",
      "episode: 247 reward: -312.7505665224348\n",
      "episode: 248 reward: -135.5875093701436\n",
      "episode: 249 reward: -441.6053043293015\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "max_expert_num = 50000\n",
    "num_steps = 0\n",
    "expert_traj = []\n",
    "\n",
    "for i_episode in count():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        expert_traj.append(np.hstack([state, action]))\n",
    "        num_steps += 1\n",
    "    \n",
    "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
    "    \n",
    "    if num_steps >= max_expert_num:\n",
    "        break\n",
    "        \n",
    "expert_traj = np.stack(expert_traj)\n",
    "print()\n",
    "print(expert_traj.shape)\n",
    "print()\n",
    "np.save(\"expert_traj.npy\", expert_traj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 467.89781800000003,
   "position": {
    "height": "40px",
    "left": "827.273px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
