{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://notify-api.line.me/api/notify\"\n",
    "token = '88RzP9jGYYEusPQKqpdWpELln97VxOah7ZIab2MyV1R'\n",
    "headers = {\"Authorization\" : \"Bearer \"+ token}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "from common.figure8_env import gen_figure8_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 10\n",
    "HORIZON = 1500\n",
    "render = False\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gen_figure8_env(sim_number=0, HORIZON=HORIZON, render=render)\n",
    "        return env\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gen_figure8_env(HORIZON=HORIZON, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save(num_epoch, rewards, image_path):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('epoch %s. reward: %s' % (num_epoch, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.savefig(image_path)\n",
    "    plt.show()\n",
    "\n",
    "def send_line(headers, message, image_path):\n",
    "    # send to line\n",
    "    payload = {\"message\" :  message}\n",
    "    files = {\"imageFile\": open(image_path, \"rb\")}\n",
    "    r = requests.post(url ,headers = headers ,params=payload, files=files)\n",
    "\n",
    "    \n",
    "def test_env(vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = HORIZON * 10\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = 1000\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "image_path = './result/ppo2.png'\n",
    "model_path = './result/ppo2.pt'\n",
    "test_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 26/100 [29:49<1:24:52, 68.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False False False False False False False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 27/100 [30:04<1:21:18, 66.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False  True False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "state = envs.reset()\n",
    "early_stop = False\n",
    "\n",
    "for num_epoch in trange(max_epochs):\n",
    "    if early_stop:\n",
    "        break\n",
    "    \n",
    "    state = envs.reset()\n",
    "\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "        \n",
    "        if sum(done) > 0:\n",
    "            print(done)\n",
    "            break\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "    if num_epoch % 5 == 0:\n",
    "        test_reward = np.mean([test_env() for _ in range(3)])\n",
    "        test_rewards.append(test_reward)\n",
    "        plot_and_save(num_epoch, test_rewards, image_path)\n",
    "        send_line(headers, 'epoch: {}'.format(num_epoch), image_path)\n",
    "        if test_reward > threshold_reward: early_stop = True\n",
    "       \n",
    "    num_epoch += 1\n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCritic(\n",
       "  (critic): Sequential(\n",
       "    (0): Linear(in_features=28, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (actor): Sequential(\n",
       "    (0): Linear(in_features=28, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Saving trajectories for GAIL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: -133.5056485070341\n",
      "episode: 1 reward: -3.3737309166625002\n",
      "episode: 2 reward: -135.0328820133956\n",
      "episode: 3 reward: -131.27964142064513\n",
      "episode: 4 reward: -125.12845453838382\n",
      "episode: 5 reward: -4.247933460422459\n",
      "episode: 6 reward: -395.59297834503883\n",
      "episode: 7 reward: -253.25736991568547\n",
      "episode: 8 reward: -135.50603026103278\n",
      "episode: 9 reward: -132.72095459732952\n",
      "episode: 10 reward: -133.89608385869212\n",
      "episode: 11 reward: -4.5990508813314035\n",
      "episode: 12 reward: -134.44470210766775\n",
      "episode: 13 reward: -801.7661346371387\n",
      "episode: 14 reward: -131.97725229377644\n",
      "episode: 15 reward: -266.76940521674015\n",
      "episode: 16 reward: -247.5062278004002\n",
      "episode: 17 reward: -4.914595620774103\n",
      "episode: 18 reward: -138.7990887577753\n",
      "episode: 19 reward: -268.3754189751262\n",
      "episode: 20 reward: -363.28764882256417\n",
      "episode: 21 reward: -128.15870842354997\n",
      "episode: 22 reward: -134.94598918501788\n",
      "episode: 23 reward: -309.9577786212293\n",
      "episode: 24 reward: -131.91670030817002\n",
      "episode: 25 reward: -134.65823444568952\n",
      "episode: 26 reward: -134.5615349098279\n",
      "episode: 27 reward: -273.5740578550409\n",
      "episode: 28 reward: -265.05553942459926\n",
      "episode: 29 reward: -258.0591054576666\n",
      "episode: 30 reward: -128.91060595426686\n",
      "episode: 31 reward: -656.2461074160591\n",
      "episode: 32 reward: -136.84071690580248\n",
      "episode: 33 reward: -259.2365200533221\n",
      "episode: 34 reward: -132.68644155022494\n",
      "episode: 35 reward: -260.66364797902054\n",
      "episode: 36 reward: -128.8211009270027\n",
      "episode: 37 reward: -384.53615237759317\n",
      "episode: 38 reward: -4.612904346743044\n",
      "episode: 39 reward: -401.1162060114804\n",
      "episode: 40 reward: -126.25334578262932\n",
      "episode: 41 reward: -3.845934927726255\n",
      "episode: 42 reward: -132.44253012402612\n",
      "episode: 43 reward: -134.1267203432647\n",
      "episode: 44 reward: -128.56866661753938\n",
      "episode: 45 reward: -4.97856955649956\n",
      "episode: 46 reward: -392.498679426522\n",
      "episode: 47 reward: -4.756869243844947\n",
      "episode: 48 reward: -4.59189846851519\n",
      "episode: 49 reward: -4.7496626929539225\n",
      "episode: 50 reward: -131.08999767991665\n",
      "episode: 51 reward: -138.17235302513578\n",
      "episode: 52 reward: -3.751761058079555\n",
      "episode: 53 reward: -260.6317126814632\n",
      "episode: 54 reward: -4.535299319594524\n",
      "episode: 55 reward: -133.70892423024802\n",
      "episode: 56 reward: -134.8732103854694\n",
      "episode: 57 reward: -5.315182694344295\n",
      "episode: 58 reward: -265.04898120165\n",
      "episode: 59 reward: -124.99288470795233\n",
      "episode: 60 reward: -4.247632479535832\n",
      "episode: 61 reward: -3.68334723705883\n",
      "episode: 62 reward: -133.617727327027\n",
      "episode: 63 reward: -136.28353948776376\n",
      "episode: 64 reward: -5.056124136459314\n",
      "episode: 65 reward: -262.7844771770983\n",
      "episode: 66 reward: -251.52420165781922\n",
      "episode: 67 reward: -133.4014820950796\n",
      "episode: 68 reward: -7.0558924646711\n",
      "episode: 69 reward: -135.41150554590206\n",
      "episode: 70 reward: -131.8871841825757\n",
      "episode: 71 reward: -130.8724972571845\n",
      "episode: 72 reward: -367.7339135957503\n",
      "episode: 73 reward: -134.25198778254116\n",
      "episode: 74 reward: -133.86858295338342\n",
      "episode: 75 reward: -378.9443227440811\n",
      "episode: 76 reward: -3.5473336732949625\n",
      "episode: 77 reward: -261.5470895641183\n",
      "episode: 78 reward: -408.34135925288217\n",
      "episode: 79 reward: -257.6727990499033\n",
      "episode: 80 reward: -399.78682205537433\n",
      "episode: 81 reward: -266.08087229456055\n",
      "episode: 82 reward: -817.186490578741\n",
      "episode: 83 reward: -4.500140134501902\n",
      "episode: 84 reward: -508.65456581456573\n",
      "episode: 85 reward: -378.46002005145874\n",
      "episode: 86 reward: -137.76181809972095\n",
      "episode: 87 reward: -674.8280917415572\n",
      "episode: 88 reward: -128.65034230393303\n",
      "episode: 89 reward: -3.922315525193146\n",
      "episode: 90 reward: -131.00005239353024\n",
      "episode: 91 reward: -130.68974732718007\n",
      "episode: 92 reward: -135.21946982972375\n",
      "episode: 93 reward: -137.3667851983452\n",
      "episode: 94 reward: -136.9119001250973\n",
      "episode: 95 reward: -254.5371556381929\n",
      "episode: 96 reward: -374.827391591992\n",
      "episode: 97 reward: -523.9964989484117\n",
      "episode: 98 reward: -133.94200200894622\n",
      "episode: 99 reward: -133.74880434577523\n",
      "episode: 100 reward: -247.32247835568552\n",
      "episode: 101 reward: -138.75528548988993\n",
      "episode: 102 reward: -4.847096453940289\n",
      "episode: 103 reward: -136.62732481247133\n",
      "episode: 104 reward: -262.20300946977864\n",
      "episode: 105 reward: -6.5435854338994\n",
      "episode: 106 reward: -125.17361036750681\n",
      "episode: 107 reward: -690.5202921080676\n",
      "episode: 108 reward: -280.53617631459497\n",
      "episode: 109 reward: -135.40352441695322\n",
      "episode: 110 reward: -131.07617970631023\n",
      "episode: 111 reward: -247.0260554601557\n",
      "episode: 112 reward: -135.40673404514774\n",
      "episode: 113 reward: -395.03306256658476\n",
      "episode: 114 reward: -384.1784417792837\n",
      "episode: 115 reward: -128.4500742980931\n",
      "episode: 116 reward: -463.6977661877445\n",
      "episode: 117 reward: -130.94801971085445\n",
      "episode: 118 reward: -144.0228791279258\n",
      "episode: 119 reward: -667.2634492717342\n",
      "episode: 120 reward: -131.79948959004724\n",
      "episode: 121 reward: -138.03140142705894\n",
      "episode: 122 reward: -129.26779443720966\n",
      "episode: 123 reward: -3.2877798185337896\n",
      "episode: 124 reward: -134.72016283865193\n",
      "episode: 125 reward: -382.2159098741087\n",
      "episode: 126 reward: -264.6491917411121\n",
      "episode: 127 reward: -134.2254720027939\n",
      "episode: 128 reward: -424.8235005744391\n",
      "episode: 129 reward: -134.52619102883028\n",
      "episode: 130 reward: -537.7406839640856\n",
      "episode: 131 reward: -133.90654715605245\n",
      "episode: 132 reward: -132.20198118805123\n",
      "episode: 133 reward: -400.3589991495165\n",
      "episode: 134 reward: -130.12695949420717\n",
      "episode: 135 reward: -290.86810229081595\n",
      "episode: 136 reward: -394.9043391522139\n",
      "episode: 137 reward: -133.42125091255778\n",
      "episode: 138 reward: -134.96306459417266\n",
      "episode: 139 reward: -3.8499366797706336\n",
      "episode: 140 reward: -3.828788719469504\n",
      "episode: 141 reward: -5.554963437941836\n",
      "episode: 142 reward: -4.510403163975261\n",
      "episode: 143 reward: -325.97799775791754\n",
      "episode: 144 reward: -3.1174779530363375\n",
      "episode: 145 reward: -134.55262416681552\n",
      "episode: 146 reward: -350.45777263184095\n",
      "episode: 147 reward: -137.33235583532627\n",
      "episode: 148 reward: -452.0061280718382\n",
      "episode: 149 reward: -265.98673902850385\n",
      "episode: 150 reward: -284.8590382363739\n",
      "episode: 151 reward: -250.06981206461143\n",
      "episode: 152 reward: -129.50428228187013\n",
      "episode: 153 reward: -393.09302439930724\n",
      "episode: 154 reward: -5.075964808667517\n",
      "episode: 155 reward: -129.83816358490287\n",
      "episode: 156 reward: -266.1020126434327\n",
      "episode: 157 reward: -132.23463644630868\n",
      "episode: 158 reward: -779.5855091317233\n",
      "episode: 159 reward: -3.763971510946643\n",
      "episode: 160 reward: -132.67794144748086\n",
      "episode: 161 reward: -662.5587064643477\n",
      "episode: 162 reward: -135.2401324340408\n",
      "episode: 163 reward: -259.9633585943629\n",
      "episode: 164 reward: -6.232862086437321\n",
      "episode: 165 reward: -139.498411973157\n",
      "episode: 166 reward: -135.35070491390638\n",
      "episode: 167 reward: -135.1400077480551\n",
      "episode: 168 reward: -347.3664683729514\n",
      "episode: 169 reward: -427.1984854733556\n",
      "episode: 170 reward: -5.15672209428849\n",
      "episode: 171 reward: -525.916662268042\n",
      "episode: 172 reward: -133.7053511504196\n",
      "episode: 173 reward: -271.26784680564384\n",
      "episode: 174 reward: -124.85474506625023\n",
      "episode: 175 reward: -134.19873581079943\n",
      "episode: 176 reward: -255.83160338962983\n",
      "episode: 177 reward: -135.13400569542506\n",
      "episode: 178 reward: -4.960226836538054\n",
      "episode: 179 reward: -139.19809065222032\n",
      "episode: 180 reward: -140.05080094044732\n",
      "episode: 181 reward: -137.76647105767526\n",
      "episode: 182 reward: -403.1731636539886\n",
      "episode: 183 reward: -257.970427512537\n",
      "episode: 184 reward: -3.7473226459331066\n",
      "episode: 185 reward: -278.3098063643893\n",
      "episode: 186 reward: -255.99692458401518\n",
      "episode: 187 reward: -4.6365121508813445\n",
      "episode: 188 reward: -244.67627722290948\n",
      "episode: 189 reward: -131.21920785362062\n",
      "episode: 190 reward: -777.3698354491825\n",
      "episode: 191 reward: -132.07220706141683\n",
      "episode: 192 reward: -392.09434598281683\n",
      "episode: 193 reward: -136.06354238422503\n",
      "episode: 194 reward: -377.4409927865957\n",
      "episode: 195 reward: -132.18253486880235\n",
      "episode: 196 reward: -129.15162595976702\n",
      "episode: 197 reward: -396.5254064840202\n",
      "episode: 198 reward: -3.610361833207753\n",
      "episode: 199 reward: -245.53736015092704\n",
      "episode: 200 reward: -270.99181854480565\n",
      "episode: 201 reward: -247.4231450110685\n",
      "episode: 202 reward: -131.59894474370887\n",
      "episode: 203 reward: -144.7898370619998\n",
      "episode: 204 reward: -926.5588068852352\n",
      "episode: 205 reward: -133.39727923189105\n",
      "episode: 206 reward: -131.93566436017008\n",
      "episode: 207 reward: -6.40529176710689\n",
      "episode: 208 reward: -257.08448208556194\n",
      "episode: 209 reward: -130.92098423630432\n",
      "episode: 210 reward: -262.2927047192545\n",
      "episode: 211 reward: -6.859901180492491\n",
      "episode: 212 reward: -262.70877767928914\n",
      "episode: 213 reward: -134.56588203218894\n",
      "episode: 214 reward: -135.22465193371625\n",
      "episode: 215 reward: -137.9657247788344\n",
      "episode: 216 reward: -135.13425433384725\n",
      "episode: 217 reward: -132.3215993693809\n",
      "episode: 218 reward: -400.611961792729\n",
      "episode: 219 reward: -401.91908212383294\n",
      "episode: 220 reward: -282.5082305011229\n",
      "episode: 221 reward: -135.42191465289923\n",
      "episode: 222 reward: -399.7881535647735\n",
      "episode: 223 reward: -131.06522770318847\n",
      "episode: 224 reward: -130.7681491912167\n",
      "episode: 225 reward: -135.31477016876133\n",
      "episode: 226 reward: -3.914901001828447\n",
      "episode: 227 reward: -134.5129393394648\n",
      "episode: 228 reward: -376.1469783238271\n",
      "episode: 229 reward: -133.09045533066046\n",
      "episode: 230 reward: -383.2750315233141\n",
      "episode: 231 reward: -263.71240275232276\n",
      "episode: 232 reward: -500.0083919266878\n",
      "episode: 233 reward: -135.22531187168758\n",
      "episode: 234 reward: -135.17818433537522\n",
      "episode: 235 reward: -395.9834332194123\n",
      "episode: 236 reward: -126.08778928679216\n",
      "episode: 237 reward: -413.7495701300203\n",
      "episode: 238 reward: -131.37116502717876\n",
      "episode: 239 reward: -121.6506938627967\n",
      "episode: 240 reward: -653.7053929625495\n",
      "episode: 241 reward: -254.87183145095838\n",
      "episode: 242 reward: -129.71331746419523\n",
      "episode: 243 reward: -265.9795936355916\n",
      "episode: 244 reward: -400.65989274385277\n",
      "episode: 245 reward: -251.82522565834446\n",
      "episode: 246 reward: -3.95924871368981\n",
      "episode: 247 reward: -312.7505665224348\n",
      "episode: 248 reward: -135.5875093701436\n",
      "episode: 249 reward: -441.6053043293015\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "max_expert_num = 50000\n",
    "num_steps = 0\n",
    "expert_traj = []\n",
    "\n",
    "for i_episode in count():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        expert_traj.append(np.hstack([state, action]))\n",
    "        num_steps += 1\n",
    "    \n",
    "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
    "    \n",
    "    if num_steps >= max_expert_num:\n",
    "        break\n",
    "        \n",
    "expert_traj = np.stack(expert_traj)\n",
    "print()\n",
    "print(expert_traj.shape)\n",
    "print()\n",
    "np.save(\"expert_traj.npy\", expert_traj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 467.89781800000003,
   "position": {
    "height": "489.716px",
    "left": "1387.27px",
    "right": "20px",
    "top": "170px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
