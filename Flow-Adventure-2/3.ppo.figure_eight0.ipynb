{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://notify-api.line.me/api/notify\"\n",
    "token = '88RzP9jGYYEusPQKqpdWpELln97VxOah7ZIab2MyV1R'\n",
    "headers = {\"Authorization\" : \"Bearer \"+ token}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "from common.figure8_env import gen_figure8_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 10\n",
    "HORIZON = 1500\n",
    "render = False\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gen_figure8_env(sim_number=0, HORIZON=HORIZON, render=render)\n",
    "        return env\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gen_figure8_env(HORIZON=HORIZON, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save(num_epoch, rewards, image_path):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('epoch %s. reward: %s' % (num_epoch, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.savefig(image_path)\n",
    "    plt.show()\n",
    "\n",
    "def send_line(headers, message, image_path):\n",
    "    # send to line\n",
    "    payload = {\"message\" :  message}\n",
    "    files = {\"imageFile\": open(image_path, \"rb\")}\n",
    "    r = requests.post(url ,headers = headers ,params=payload, files=files)\n",
    "\n",
    "    \n",
    "def test_env(vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = HORIZON * 2\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = 1000\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "image_path = './result/ppo_figureeight1.png'\n",
    "model_path = './result/ppo_figureeight1.pt'\n",
    "test_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d3487e6823cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0madvantage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mppo_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppo_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-7e55721e6f86>\u001b[0m in \u001b[0;36mppo_update\u001b[0;34m(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/flow/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/flow/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state = envs.reset()\n",
    "early_stop = False\n",
    "\n",
    "for num_epoch in trange(max_epochs):\n",
    "    if early_stop:\n",
    "        break\n",
    "    \n",
    "    state = envs.reset()\n",
    "\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "        \n",
    "        if sum(done) > 0:\n",
    "            print(\"done occured at num_steps: {}\".format(num_steps))\n",
    "            print(done)\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "    if num_epoch % 5 == 0:\n",
    "        test_reward = np.mean([test_env() for _ in range(3)])\n",
    "        test_rewards.append(test_reward)\n",
    "        plot_and_save(num_epoch, test_rewards, image_path)\n",
    "        send_line(headers, 'epoch: {}'.format(num_epoch), image_path)\n",
    "        if test_reward > threshold_reward: early_stop = True\n",
    "       \n",
    "    num_epoch += 1\n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCritic(\n",
       "  (critic): Sequential(\n",
       "    (0): Linear(in_features=28, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (actor): Sequential(\n",
       "    (0): Linear(in_features=28, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444.6199543020788"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1500):\n",
    "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    dist, _ = model(state)\n",
    "    # next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "    next_state, reward, done, _ = env.step(1)\n",
    "    state = next_state\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433.3293343103748\n"
     ]
    }
   ],
   "source": [
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Saving trajectories for GAIL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: -133.5056485070341\n",
      "episode: 1 reward: -3.3737309166625002\n",
      "episode: 2 reward: -135.0328820133956\n",
      "episode: 3 reward: -131.27964142064513\n",
      "episode: 4 reward: -125.12845453838382\n",
      "episode: 5 reward: -4.247933460422459\n",
      "episode: 6 reward: -395.59297834503883\n",
      "episode: 7 reward: -253.25736991568547\n",
      "episode: 8 reward: -135.50603026103278\n",
      "episode: 9 reward: -132.72095459732952\n",
      "episode: 10 reward: -133.89608385869212\n",
      "episode: 11 reward: -4.5990508813314035\n",
      "episode: 12 reward: -134.44470210766775\n",
      "episode: 13 reward: -801.7661346371387\n",
      "episode: 14 reward: -131.97725229377644\n",
      "episode: 15 reward: -266.76940521674015\n",
      "episode: 16 reward: -247.5062278004002\n",
      "episode: 17 reward: -4.914595620774103\n",
      "episode: 18 reward: -138.7990887577753\n",
      "episode: 19 reward: -268.3754189751262\n",
      "episode: 20 reward: -363.28764882256417\n",
      "episode: 21 reward: -128.15870842354997\n",
      "episode: 22 reward: -134.94598918501788\n",
      "episode: 23 reward: -309.9577786212293\n",
      "episode: 24 reward: -131.91670030817002\n",
      "episode: 25 reward: -134.65823444568952\n",
      "episode: 26 reward: -134.5615349098279\n",
      "episode: 27 reward: -273.5740578550409\n",
      "episode: 28 reward: -265.05553942459926\n",
      "episode: 29 reward: -258.0591054576666\n",
      "episode: 30 reward: -128.91060595426686\n",
      "episode: 31 reward: -656.2461074160591\n",
      "episode: 32 reward: -136.84071690580248\n",
      "episode: 33 reward: -259.2365200533221\n",
      "episode: 34 reward: -132.68644155022494\n",
      "episode: 35 reward: -260.66364797902054\n",
      "episode: 36 reward: -128.8211009270027\n",
      "episode: 37 reward: -384.53615237759317\n",
      "episode: 38 reward: -4.612904346743044\n",
      "episode: 39 reward: -401.1162060114804\n",
      "episode: 40 reward: -126.25334578262932\n",
      "episode: 41 reward: -3.845934927726255\n",
      "episode: 42 reward: -132.44253012402612\n",
      "episode: 43 reward: -134.1267203432647\n",
      "episode: 44 reward: -128.56866661753938\n",
      "episode: 45 reward: -4.97856955649956\n",
      "episode: 46 reward: -392.498679426522\n",
      "episode: 47 reward: -4.756869243844947\n",
      "episode: 48 reward: -4.59189846851519\n",
      "episode: 49 reward: -4.7496626929539225\n",
      "episode: 50 reward: -131.08999767991665\n",
      "episode: 51 reward: -138.17235302513578\n",
      "episode: 52 reward: -3.751761058079555\n",
      "episode: 53 reward: -260.6317126814632\n",
      "episode: 54 reward: -4.535299319594524\n",
      "episode: 55 reward: -133.70892423024802\n",
      "episode: 56 reward: -134.8732103854694\n",
      "episode: 57 reward: -5.315182694344295\n",
      "episode: 58 reward: -265.04898120165\n",
      "episode: 59 reward: -124.99288470795233\n",
      "episode: 60 reward: -4.247632479535832\n",
      "episode: 61 reward: -3.68334723705883\n",
      "episode: 62 reward: -133.617727327027\n",
      "episode: 63 reward: -136.28353948776376\n",
      "episode: 64 reward: -5.056124136459314\n",
      "episode: 65 reward: -262.7844771770983\n",
      "episode: 66 reward: -251.52420165781922\n",
      "episode: 67 reward: -133.4014820950796\n",
      "episode: 68 reward: -7.0558924646711\n",
      "episode: 69 reward: -135.41150554590206\n",
      "episode: 70 reward: -131.8871841825757\n",
      "episode: 71 reward: -130.8724972571845\n",
      "episode: 72 reward: -367.7339135957503\n",
      "episode: 73 reward: -134.25198778254116\n",
      "episode: 74 reward: -133.86858295338342\n",
      "episode: 75 reward: -378.9443227440811\n",
      "episode: 76 reward: -3.5473336732949625\n",
      "episode: 77 reward: -261.5470895641183\n",
      "episode: 78 reward: -408.34135925288217\n",
      "episode: 79 reward: -257.6727990499033\n",
      "episode: 80 reward: -399.78682205537433\n",
      "episode: 81 reward: -266.08087229456055\n",
      "episode: 82 reward: -817.186490578741\n",
      "episode: 83 reward: -4.500140134501902\n",
      "episode: 84 reward: -508.65456581456573\n",
      "episode: 85 reward: -378.46002005145874\n",
      "episode: 86 reward: -137.76181809972095\n",
      "episode: 87 reward: -674.8280917415572\n",
      "episode: 88 reward: -128.65034230393303\n",
      "episode: 89 reward: -3.922315525193146\n",
      "episode: 90 reward: -131.00005239353024\n",
      "episode: 91 reward: -130.68974732718007\n",
      "episode: 92 reward: -135.21946982972375\n",
      "episode: 93 reward: -137.3667851983452\n",
      "episode: 94 reward: -136.9119001250973\n",
      "episode: 95 reward: -254.5371556381929\n",
      "episode: 96 reward: -374.827391591992\n",
      "episode: 97 reward: -523.9964989484117\n",
      "episode: 98 reward: -133.94200200894622\n",
      "episode: 99 reward: -133.74880434577523\n",
      "episode: 100 reward: -247.32247835568552\n",
      "episode: 101 reward: -138.75528548988993\n",
      "episode: 102 reward: -4.847096453940289\n",
      "episode: 103 reward: -136.62732481247133\n",
      "episode: 104 reward: -262.20300946977864\n",
      "episode: 105 reward: -6.5435854338994\n",
      "episode: 106 reward: -125.17361036750681\n",
      "episode: 107 reward: -690.5202921080676\n",
      "episode: 108 reward: -280.53617631459497\n",
      "episode: 109 reward: -135.40352441695322\n",
      "episode: 110 reward: -131.07617970631023\n",
      "episode: 111 reward: -247.0260554601557\n",
      "episode: 112 reward: -135.40673404514774\n",
      "episode: 113 reward: -395.03306256658476\n",
      "episode: 114 reward: -384.1784417792837\n",
      "episode: 115 reward: -128.4500742980931\n",
      "episode: 116 reward: -463.6977661877445\n",
      "episode: 117 reward: -130.94801971085445\n",
      "episode: 118 reward: -144.0228791279258\n",
      "episode: 119 reward: -667.2634492717342\n",
      "episode: 120 reward: -131.79948959004724\n",
      "episode: 121 reward: -138.03140142705894\n",
      "episode: 122 reward: -129.26779443720966\n",
      "episode: 123 reward: -3.2877798185337896\n",
      "episode: 124 reward: -134.72016283865193\n",
      "episode: 125 reward: -382.2159098741087\n",
      "episode: 126 reward: -264.6491917411121\n",
      "episode: 127 reward: -134.2254720027939\n",
      "episode: 128 reward: -424.8235005744391\n",
      "episode: 129 reward: -134.52619102883028\n",
      "episode: 130 reward: -537.7406839640856\n",
      "episode: 131 reward: -133.90654715605245\n",
      "episode: 132 reward: -132.20198118805123\n",
      "episode: 133 reward: -400.3589991495165\n",
      "episode: 134 reward: -130.12695949420717\n",
      "episode: 135 reward: -290.86810229081595\n",
      "episode: 136 reward: -394.9043391522139\n",
      "episode: 137 reward: -133.42125091255778\n",
      "episode: 138 reward: -134.96306459417266\n",
      "episode: 139 reward: -3.8499366797706336\n",
      "episode: 140 reward: -3.828788719469504\n",
      "episode: 141 reward: -5.554963437941836\n",
      "episode: 142 reward: -4.510403163975261\n",
      "episode: 143 reward: -325.97799775791754\n",
      "episode: 144 reward: -3.1174779530363375\n",
      "episode: 145 reward: -134.55262416681552\n",
      "episode: 146 reward: -350.45777263184095\n",
      "episode: 147 reward: -137.33235583532627\n",
      "episode: 148 reward: -452.0061280718382\n",
      "episode: 149 reward: -265.98673902850385\n",
      "episode: 150 reward: -284.8590382363739\n",
      "episode: 151 reward: -250.06981206461143\n",
      "episode: 152 reward: -129.50428228187013\n",
      "episode: 153 reward: -393.09302439930724\n",
      "episode: 154 reward: -5.075964808667517\n",
      "episode: 155 reward: -129.83816358490287\n",
      "episode: 156 reward: -266.1020126434327\n",
      "episode: 157 reward: -132.23463644630868\n",
      "episode: 158 reward: -779.5855091317233\n",
      "episode: 159 reward: -3.763971510946643\n",
      "episode: 160 reward: -132.67794144748086\n",
      "episode: 161 reward: -662.5587064643477\n",
      "episode: 162 reward: -135.2401324340408\n",
      "episode: 163 reward: -259.9633585943629\n",
      "episode: 164 reward: -6.232862086437321\n",
      "episode: 165 reward: -139.498411973157\n",
      "episode: 166 reward: -135.35070491390638\n",
      "episode: 167 reward: -135.1400077480551\n",
      "episode: 168 reward: -347.3664683729514\n",
      "episode: 169 reward: -427.1984854733556\n",
      "episode: 170 reward: -5.15672209428849\n",
      "episode: 171 reward: -525.916662268042\n",
      "episode: 172 reward: -133.7053511504196\n",
      "episode: 173 reward: -271.26784680564384\n",
      "episode: 174 reward: -124.85474506625023\n",
      "episode: 175 reward: -134.19873581079943\n",
      "episode: 176 reward: -255.83160338962983\n",
      "episode: 177 reward: -135.13400569542506\n",
      "episode: 178 reward: -4.960226836538054\n",
      "episode: 179 reward: -139.19809065222032\n",
      "episode: 180 reward: -140.05080094044732\n",
      "episode: 181 reward: -137.76647105767526\n",
      "episode: 182 reward: -403.1731636539886\n",
      "episode: 183 reward: -257.970427512537\n",
      "episode: 184 reward: -3.7473226459331066\n",
      "episode: 185 reward: -278.3098063643893\n",
      "episode: 186 reward: -255.99692458401518\n",
      "episode: 187 reward: -4.6365121508813445\n",
      "episode: 188 reward: -244.67627722290948\n",
      "episode: 189 reward: -131.21920785362062\n",
      "episode: 190 reward: -777.3698354491825\n",
      "episode: 191 reward: -132.07220706141683\n",
      "episode: 192 reward: -392.09434598281683\n",
      "episode: 193 reward: -136.06354238422503\n",
      "episode: 194 reward: -377.4409927865957\n",
      "episode: 195 reward: -132.18253486880235\n",
      "episode: 196 reward: -129.15162595976702\n",
      "episode: 197 reward: -396.5254064840202\n",
      "episode: 198 reward: -3.610361833207753\n",
      "episode: 199 reward: -245.53736015092704\n",
      "episode: 200 reward: -270.99181854480565\n",
      "episode: 201 reward: -247.4231450110685\n",
      "episode: 202 reward: -131.59894474370887\n",
      "episode: 203 reward: -144.7898370619998\n",
      "episode: 204 reward: -926.5588068852352\n",
      "episode: 205 reward: -133.39727923189105\n",
      "episode: 206 reward: -131.93566436017008\n",
      "episode: 207 reward: -6.40529176710689\n",
      "episode: 208 reward: -257.08448208556194\n",
      "episode: 209 reward: -130.92098423630432\n",
      "episode: 210 reward: -262.2927047192545\n",
      "episode: 211 reward: -6.859901180492491\n",
      "episode: 212 reward: -262.70877767928914\n",
      "episode: 213 reward: -134.56588203218894\n",
      "episode: 214 reward: -135.22465193371625\n",
      "episode: 215 reward: -137.9657247788344\n",
      "episode: 216 reward: -135.13425433384725\n",
      "episode: 217 reward: -132.3215993693809\n",
      "episode: 218 reward: -400.611961792729\n",
      "episode: 219 reward: -401.91908212383294\n",
      "episode: 220 reward: -282.5082305011229\n",
      "episode: 221 reward: -135.42191465289923\n",
      "episode: 222 reward: -399.7881535647735\n",
      "episode: 223 reward: -131.06522770318847\n",
      "episode: 224 reward: -130.7681491912167\n",
      "episode: 225 reward: -135.31477016876133\n",
      "episode: 226 reward: -3.914901001828447\n",
      "episode: 227 reward: -134.5129393394648\n",
      "episode: 228 reward: -376.1469783238271\n",
      "episode: 229 reward: -133.09045533066046\n",
      "episode: 230 reward: -383.2750315233141\n",
      "episode: 231 reward: -263.71240275232276\n",
      "episode: 232 reward: -500.0083919266878\n",
      "episode: 233 reward: -135.22531187168758\n",
      "episode: 234 reward: -135.17818433537522\n",
      "episode: 235 reward: -395.9834332194123\n",
      "episode: 236 reward: -126.08778928679216\n",
      "episode: 237 reward: -413.7495701300203\n",
      "episode: 238 reward: -131.37116502717876\n",
      "episode: 239 reward: -121.6506938627967\n",
      "episode: 240 reward: -653.7053929625495\n",
      "episode: 241 reward: -254.87183145095838\n",
      "episode: 242 reward: -129.71331746419523\n",
      "episode: 243 reward: -265.9795936355916\n",
      "episode: 244 reward: -400.65989274385277\n",
      "episode: 245 reward: -251.82522565834446\n",
      "episode: 246 reward: -3.95924871368981\n",
      "episode: 247 reward: -312.7505665224348\n",
      "episode: 248 reward: -135.5875093701436\n",
      "episode: 249 reward: -441.6053043293015\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "max_expert_num = 50000\n",
    "num_steps = 0\n",
    "expert_traj = []\n",
    "\n",
    "for i_episode in count():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        expert_traj.append(np.hstack([state, action]))\n",
    "        num_steps += 1\n",
    "    \n",
    "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
    "    \n",
    "    if num_steps >= max_expert_num:\n",
    "        break\n",
    "        \n",
    "expert_traj = np.stack(expert_traj)\n",
    "print()\n",
    "print(expert_traj.shape)\n",
    "print()\n",
    "np.save(\"expert_traj.npy\", expert_traj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "50"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 467.89781800000003,
   "position": {
    "height": "489.716px",
    "left": "1387.27px",
    "right": "20px",
    "top": "170px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
